1.title:"title":"Swarm Verification"
1.abstract:"abstract":"Reportedly, supercomputer designer Seymour Cray once said that he would sooner use two strong oxen to plow afield than a thousand chickens. Although this is undoubtedly wise when it comes to plowing afield, it is not so clear for other types of tasks. Model checking problems are of the proverbial \"search the needle in a haystack\" type. Such problems can often be parallelized easily. Alas, none of the usual divide and conquer methods can be used to parallelize the working of a model checker. Given that it has become easier than ever to gain access to large numbers of computers to perform even routine tasks it is becoming more and more attractive to find alternate ways to use these resources to speed up model checking tasks. This paper describes one such method, called swarm verification."
1.url:http://dx.doi.org/10.1109/ASE.2008.9
1.opinion:exclude

2.title:"title":"Reflections on, and Predictions for, Support Systems for the Development of Programs"
2.abstract:"abstract":"My first attempt to build a \"formal development support system\" was in IBM in the 1970s; more public is the mural system we built in Manchester; the Rodin (EU) project developed a set of open source tools that are now being used in the (EU) DEPLOY project. These attempts give me some perspective from which to predict what sort of tool will finally make the difference to software developers that CAD systems have made in hardware design."
2.url:http://dx.doi.org/10.1109/ASE.2008.10
2.opinion:exclude

3.title:"title":"Increasing Test Granularity by Aggregating Unit Tests"
3.abstract:"abstract":"Unit tests are focused, efficient, and there are many techniques to support their automatic generation. Coarser granularity tests, however, are necessary to validate the behavior of larger software components, and are also likely to be more robust in the presence of program changes. This paper investigates whether coarser granularity tests can be automatically generated by aggregating unit tests. We leverage our Differential Unit Test (DUT) framework to represent unit tests, define a space of potential aggregations of those unit tests, and implement a strategy to traverse that space to generate Aggregated DUTs (A- DUTs) that validate the effects of multiple method calls on a (set of) receiver object(s). An empirical study of A-DUTs on two applications shows their tradeoffs with DUTs and their potential to increase the number of versions for which tests remain usable relative to method level tests."
3.url:http://dx.doi.org/10.1109/ASE.2008.11
3.opinion:exclude

4.title:"title":"Random Test Run Length and Effectiveness"
4.abstract:"abstract":"A poorly understood but important factor in random testing is the selection of a maximum length for test runs. Given a limited time for testing, it is seldom clear whether executing a small number of long runs or a large number of short runs maximizes utility. It is generally expected that longer runs are more likely to expose failures - which is certainly true with respect to runs shorter than the shortest failing trace. However, longer runs produce longer failing traces, requiring more effort from humans in debugging or more resources for automated minimization. In testing with feedback, increasing ranges for parameters may also cause the probability of failure to decrease in longer runs. We show that the choice of test length dramatically impacts the effectiveness of random testing, and that the patterns observed in simple models and predicted by analysis are useful in understanding effects observed in a large scale case study of a JPL flight software system."
4.url:http://dx.doi.org/10.1109/ASE.2008.12
4.opinion:exclude

5.title:"title":"Program Analysis with Dynamic Precision Adjustment"
5.abstract:"abstract":"We present and evaluate a framework and tool for combining multiple program analyses which allows the dynamic (on-line) adjustment of the precision of each analysis depending on the accumulated results. For example, the explicit tracking of the values of a variable may be switched off in favor of a predicate abstraction when and where the number of different variable values that have been encountered has exceeded a specified threshold. The method is evaluated on verifying the SSH client/server software and shows significant gains compared with predicate abstraction-based model checking."
5.url:http://dx.doi.org/10.1109/ASE.2008.13
5.opinion:exclude

6.title:"title":"IR-Based Traceability Recovery Processes: An Empirical Comparison of \"One-Shot\" and Incremental Processes"
6.abstract:"abstract":"We present the results of a controlled experiment aiming at analysing the role played by the approach adopted during an IR-based traceability recovery process. In particular, we compare the tracing performances achieved by subjects using the \"one-shot\" process, where the full ranked list of candidate links is proposed, and the incremental process, where a similarity threshold is used to cut the ranked list and the links are classified step-by-step. The analysis of the achieved results shows that, in general, the incremental process improves the tracing accuracy and reduces the effort to analyse the proposed links."
6.url:http://dx.doi.org/10.1109/ASE.2008.14
6.opinion:exclude

7.title:"title":"Enabling Automated Traceability Maintenance by Recognizing Development Activities Applied to Models"
7.abstract:"abstract":"For anything but the simplest of software systems, the ease and costs associated with change management can become critical to the success of a project. Establishing traceability initially can demand questionable effort, but sustaining this traceability as changes occur can be a neglected matter altogether. Without conscious effort, traceability relations become increasingly inaccurate and irrelevant as the artifacts they associate evolve. Based upon the observation that there are finite types of development activity that appear to impact traceability when software development proceeds through the construction and refinement of UML models, we have developed an approach to automate traceability maintenance in such contexts. Within this paper, we describe the technical details behind the recognition of these development activities, a task upon which our automated approach depends, and we discuss how we have validated this aspect of the work to date."
7.url:http://dx.doi.org/10.1109/ASE.2008.15
7.opinion:exclude

8.title:"title":"Incremental Latent Semantic Indexing for Automatic Traceability Link Evolution Management"
8.abstract:"abstract":"Maintaining traceability links among software artifacts is particularly important for many software engineering tasks. Even though automatic traceability link recovery tools are successful in identifying the semantic connections among software artifacts produced during software development, no existing traceability link management approach can effectively and automatically deal with software evolution. We propose a technique to automatically manage traceability link evolution and update the links in evolving software. Our novel technique, called incremental latent semantic indexing (iLSI), allows for the fast and low-cost LSI computation for the update of traceability links by analyzing the changes to software artifacts and by reusing the result from the previous LSI computation before the changes. We present our iLSI technique, and describe a complete automatic traceability link evolution management tool, TLEM, that is capable of interactively and quickly updating traceability links in the presence of evolving software artifacts. We report on our empirical evaluation with various experimental studies to assess the performance and usefulness of our approach."
8.url:http://dx.doi.org/10.1109/ASE.2008.16
8.opinion:exclude

9.title:"title":"Automated Verification of Multi-Agent Programs"
9.abstract:"abstract":"In this paper, we show that the flexible model-checking of multi-agent systems, implemented using agent-oriented programming languages, is viable thus paving the way for the construction of verifiably correct applications of autonomous agents and multi-agent systems. Model checking experiments were carried out on AJPF (agent JPF), our extension of Java PathFinder that incorporates the agent infrastructure layer, our unifying framework for agent programming languages. In our approach, properties are specified in a temporal language extended with (shallow) agent-related modalities. The framework then allows the verification of programs written in a variety of agent programming languages, thus removing the need for individual languages to implement their own verification framework. It even allows the verification of multi-agent systems comprised of agents developed in a variety of different (agent) programming languages. As an example, we also provide model checking results for the verification of a multi-agent system implementing a well-known task sharing protocol."
9.url:http://dx.doi.org/10.1109/ASE.2008.17
9.opinion:exclude

10.title:"title":"Validating Real Time Specifications using Real Time Event Queue Modeling"
10.abstract:"abstract":"Interrupt-driven real time control software is difficult to design and validate. It does not line up well with traditional state-based, timed-transition specification formalisms, due to the complexity of timers and the pending interrupt queue. The present work takes a new approach to the problem of modeling and tool-supported reasoning about such systems based on infinite-state modeling of the temporal event queue. This approach, RTEQ, can be used in any formalism or tool set supporting function rich modeling. The present paper describes the approach, explores its expressive power and semantics, and describes a significant industrial case study applying it to the design of a novel network medium access controller for wireless communications."
10.url:http://dx.doi.org/10.1109/ASE.2008.18
10.opinion:exclude

11.title:"title":"Automatic Inference of Frame Axioms Using Static Analysis"
11.abstract:"abstract":"Many approaches to software verification are currently semi-automatic: a human must provide key logical insights - e.g., loop invariants, class invariants, and frame axioms that limit the scope of changes that must be analyzed. This paper describes a technique for automatically inferring frame axioms of procedures and loops using static analysis. The technique builds on a pointer analysis that generates limited information about all data structures in the heap. Our technique uses that information to over-approximate a potentially unbounded set of memory locations modified by each procedure/loop; this over- approximation is a candidate frame axiom. We have tested this approach on the buffer-overflow benchmarks from ASE 2007. With manually provided specifications and invariants/axioms, our tool could verify/falsify 226 of the 289 benchmarks. With our automatically inferred frame axioms, the tool could verify/falsify 203 of the 289, demonstrating the effectiveness of our approach."
11.url:http://dx.doi.org/10.1109/ASE.2008.19
11.opinion:exclude

12.title:"title":"Generating and Evaluating Choices for Fixing Inconsistencies in UML Design Models"
12.abstract:"abstract":"Our objective is to provide automated support for assisting designers in fixing inconsistencies in UML models. We have previously developed techniques for efficiently detecting inconsistencies in such models and identifying where changes need to occur in order to fix problems detected by these means. This paper extends previous work by describing a technique for automatically generating a set of concrete changes for fixing inconsistencies and providing information about the impact of each change on all consistency rules. The approach is integrated with the design tool IBM Rational Rose . We demonstrate the computational scalability and usability of the approach through the empirical evaluation of 39 UML models of sizes up to 120,000 elements."
12.url:http://dx.doi.org/10.1109/ASE.2008.20
12.opinion:exclude

13.title:"title":"Mining Scenario-Based Triggers and Effects"
13.abstract:"abstract":"We present and investigate the problem of mining scenario-based triggers and effects from execution traces, in the framework of Damm and Harel's live sequence charts (LSC); a visual, modal, scenario-based, inter-object language. Given a 'trigger scenario', we extract LSCs whose pre-chart is equivalent to the given trigger; dually, given an 'effect scenario', we extract LSCs whose main-chart is equivalent to the given effect. Our algorithms use data mining methods to provide significant sound and complete results modulo user-defined thresholds. Both the input trigger and effect scenarios, and the resulting candidate modal scenarios, are represented and visualized using a UML2- compliant variant of LSC. Thus, existing modeling tools can be used both to specify the input for the miner and to exploit its output. Experiments performed with several applications show promising results."
13.url:http://dx.doi.org/10.1109/ASE.2008.21
13.opinion:exclude

14.title:"title":"Refining Real-Time System Specifications through Bounded Model- and Satisfiability-Checking"
14.abstract:"abstract":"In bounded model checking (BMC) a system is modeled with a finite automaton and various desired properties with temporal logic formulae. Property verification is achieved by translation into boolean logic and the application of SAT-solvers. bounded satisfiability checking (BSC) adopts a similar approach, but both the system and the properties are modeled with temporal logic formulae, without an underlying operational model. Hence, BSC supports a higher-level, descriptive approach to system specification and analysis. We compare the performance of BMC and BSC over a set of case studies, using the Zot tool to translate automata and temporal logic formulae into boolean logic. We also propose a method to check whether an operational model is a correct implementation (refinement) of a temporal logic model, and assess its effectiveness on the same set of case studies. Our experimental results show the feasibility of BSC and refinement checking, with modest performance loss w.r.t. BMC."
14.url:http://dx.doi.org/10.1109/ASE.2008.22
14.opinion:exclude

15.title:"title":"Evaluating Models for Model-Based Debugging"
15.abstract:"abstract":"Developing model-based automatic debugging strategies has been an active research area for several years, with the aim of locating defects in a program by utilising fully automated generation of a model of the program from its source code. We provide an overview of current techniques in model-based debugging and assess strengths and weaknesses of the individual approaches. An empirical comparison is presented that investigates the relative accuracy of different models on a set of test programs and fault assumptions, showing that our abstract interpretation based model provides high accuracy at significantly less computational effort than slightly more accurate techniques. We compare a range of model-based debugging techniques with other state-of-the-art automated debugging approaches and outline possible future developments in automatic debugging using model-based reasoning as the central unifying component in a comprehensive framework."
15.url:http://dx.doi.org/10.1109/ASE.2008.23
15.opinion:exclude

16.title:"title":"Error Reporting Logic"
16.abstract:"abstract":"When a system fails to meet its specification, it can be difficult to find the source of the error and determine how to fix it. In this paper, we introduce error reporting logic (ERL), an algorithm and tool that produces succinct explanations for why a target system violates a specification expressed in first order predicate logic. ERL analyzes the specification to determine which parts contributed to the failure, and it displays an error message specific to those parts. Additionally, ERL uses a heuristic to determine which object in the target system is responsible for the error. Results from a small user study suggest that the combination of a more focused error message and a responsible object for the error helps users to find the failure in the system more effectively. The study also yielded insights into how the users find and fix errors that may guide future research."
16.url:http://dx.doi.org/10.1109/ASE.2008.24
16.opinion:exclude

17.title:"title":"Efficient Monitoring of Parametric Context-Free Patterns"
17.abstract:"abstract":"Recent developments in runtime verification and monitoring show that parametric regular and temporal logic specifications can be efficiently monitored against large programs. However, these logics reduce to ordinary finite automata, limiting their expressivity. For example, neither can specify structured properties that refer to the call stack of the program. While context-free grammars (CFGs) are expressive and well-understood, existing techniques for monitoring CFGs generate large runtime overhead in real-life applications. This paper shows, for the first time, that monitoring parametric CFGs is practical (with overhead on the order of 10% or lower for average cases, several times faster than the state-of-the-art). We present a monitor synthesis algorithm for CFGs based on an LR(1) parsing algorithm, modified with stack cloning to account for good prefix matching. In addition, a logic-independent mechanism is introduced to support matching against the suffixes of execution traces."
17.url:http://dx.doi.org/10.1109/ASE.2008.25
17.opinion:exclude

18.title:"title":"A Framework for Dynamic Service Discovery"
18.abstract:"abstract":"Service discovery has been recognised as an important activity for service-based systems. In this paper we describe a framework for dynamic service discovery that supports the identification of service during the execution time of service-based systems. In the framework, services are identified based on structural, behavioural, quality, and contextual characteristics of a system represented in query languages. The framework supports both pull and push modes of query execution. In the approach, a service is identified based on the computation of distances between a query and a candidate service. A prototype tool has been implemented in order to illustrate and evaluate the framework. The paper also describes the results of a set of experiments that we have conducted to evaluate the work."
18.url:http://dx.doi.org/10.1109/ASE.2008.26
18.opinion:exclude

19.title:"title":"A Methodology and Framework for Creating Domain-Specific Development Infrastructures"
19.abstract:"abstract":"Domain-specific architectures, middleware platforms, and analysis techniques leverage domain knowledge to help engineers build systems more effectively. An integrated set of these elements is called a domain-specific development infrastructure (DSDI). DSDIs are commonly created in a costly, ad-hoc fashion because current model-driven engineering (MDE) technologies lack sufficient mechanisms for capturing the semantics of domain concepts. In this paper, we propose a methodology for incorporating semantics within MDE frameworks to simplify and automate DSDI integration. We also present and evaluate a framework, called XTEAM, that implements our approach, resulting in structured processes and enforceable guidelines for DSDI integration. We have applied our approach to several DSDIs, and report on the benefits accrued."
19.url:http://dx.doi.org/10.1109/ASE.2008.27
19.opinion:exclude

20.title:"title":"Connecting Programming Environments to Support Ad-Hoc Collaboration"
20.abstract:"abstract":"Physical proximity supports various forms of ad-hoc collaboration among developers such as opportunistic task adaptation and helping co-developers when they are stuck. Connecting the input/output flows of stand-alone programming environments of distributed developers offers the potential to support such collaboration among them. Such a connection has several components including communication sessions, awareness of others' availability and the state of the objects on which they are working, and control channels allowing users to import edits of and share code with others and be notified when a team member has moved away from a program element of interest. It is possible to develop a collaboration-centered design that combines a variety of collaboration streams into a usable and useful user-interface, and implement the design using existing programming environment, communication, and compiler technologies."
20.url:http://dx.doi.org/10.1109/ASE.2008.28
20.opinion:exclude

21.title:"title":"Reducing False Positives by Combining Abstract Interpretation and Bounded Model Checking"
21.abstract:"abstract":"Fully automatic source code analysis tools based on abstract interpretation have become an integral part of the embedded software development process in many companies. And although these tools are of great help in identifying residual errors, they still possess a major drawback: analyzing industrial code comes at the cost of many spurious errors that must be investigated manually. The need for efficient development cycles prohibits extensive manual reviews, however. To overcome this problem, the combination of different software verification techniques has been suggested in the literature. Following this direction, we present a novel approach combining abstract interpretation and source code bounded model checking, where the model checker is used to reduce the number of false error reports. We apply our methodology to source code from the automotive industry written in C, and show that the number of spurious errors emitted by an abstract interpretation product can be reduced considerably."
21.url:http://dx.doi.org/10.1109/ASE.2008.29
21.opinion:exclude

22.title:"title":"Unit Testing of Flash Memory Device Driver through a SAT-Based Model Checker"
22.abstract:"abstract":"Flash memory has become virtually indispensable in most mobile devices. In order for mobile devices to operate successfully, it is essential that the flash memory be controlled correctly through the device driver software. However, as is typical for embedded software, conventional testing methods often fail to detect hidden flaws in the complex device driver software. This deficiency incurs significant development and operation overheads to the manufacturers. Model checking techniques have been proposed to compensate for the weaknesses of conventional testing methods through exhaustive analyses. These techniques, however, require significant manual efforts to create an abstract target model and, thus, are not widely applied in industry. In this project, we applied a model checking technique based on a Boolean satisfiability (SAT) solver. One advantage of SAT-based model checking is that a target C code can be analyzed directly without an abstract model, thereby enabling automated and bit-level accurate verification. In this project, we have applied CBMC, a SAT-based software model checker, to the unit testing of the Samsung OneNANDtrade device driver. Through this project, we detected several bugs that had not been discovered previously."
22.url:http://dx.doi.org/10.1109/ASE.2008.30
22.opinion:exclude

23.title:"title":"Effort Estimation in Capturing Architectural Knowledge"
23.abstract:"abstract":"Capturing and using design rationale is becoming a hot topic for software architects, as architectural design decisions are now considered first class entities that should be recorded and documented explicitly. Capturing such architecture knowledge has been underestimated for several years as architects have been only focused on documenting their architectures and neglecting the rationale that led to them. The importance of recording design rationale becomes enormous for maintenance and evolution activities, as design decisions can be replayed in order to avoid highly cost architecture recovery processes. Hence, in this work we describe how architecture design decisions can be captured and documented with specific tool support. We also provide effort estimation in capturing such knowledge and we compare this with architecture modeling efforts in order to analyze the viability of knowledge capturing strategies."
23.url:http://dx.doi.org/10.1109/ASE.2008.31
23.opinion:exclude

24.title:"title":"Test-Suite Augmentation for Evolving Software"
24.abstract:"abstract":"One activity performed by developers during regression testing is test-suite augmentation, which consists of assessing the adequacy of a test suite after a program is modified and identifying new or modified behaviors that are not adequately exercised by the existing test suite and, thus, require additional test cases. In previous work, we proposed MATRIX, a technique for test-suite augmentation based on dependence analysis and partial symbolic execution. In this paper, we present the next step of our work, where we (I) improve the effectiveness of our technique by identifying all relevant change-propagation paths, (2) extend the technique to handle multiple and more complex changes, (3) introduce the first tool that fully implements the technique, and (4) present an empirical evaluation performed on real software. Our results show that our technique is practical and more effective than existing test-suite augmentation approaches in identifying test cases with high fault-detection capabilities."
24.url:http://dx.doi.org/10.1109/ASE.2008.32
24.opinion:exclude

25.title:"title":"Reducing the Cost of Path Property Monitoring Through Sampling"
25.abstract:"abstract":"Run-time monitoring can provide important insights about a program's behavior and, for simple properties, it can be done efficiently. Monitoring properties describing sequences of program states and events, however, can result in significant run-time overhead. In this paper we present a novel approach to reducing the cost of run-time monitoring of path properties. Properties are composed to form a single integrated property that is then systematically decomposed into a set of properties that encode necessary conditions for property violations. The resulting set of properties forms a lattice whose structure is exploited to select a sample of properties that can lower monitoring cost, while preserving violation detection power relative to the original properties. Preliminary studies for a widely used Java API reveal that our approach produces a rich, structured set of properties that enables control of monitoring overhead, while detecting more violations than alternative techniques."
25.url:http://dx.doi.org/10.1109/ASE.2008.33
25.opinion:exclude

26.title:"title":"Query-Aware Test Generation Using a Relational Constraint Solver"
26.abstract:"abstract":"We present a novel approach for black-box testing of database management systems (DBMS) using the Alloy tool-set. Given a database schema and an SQL query as inputs, our approach first formulates Alloy models for both inputs, and then using the Alloy Analyzer, it generates (1) input data to populate test databases, and (2) the expected result of executing the given query on the generated data. The Alloy Analyzer results form a complete test suite (input/oracle) for verifying the execution result of a DBMS query processor. By incorporating both the schema and the query during the analysis, our approach performs query-aware data generation where executing the query on the generated data produces meaningful non-empty results. We developed a prototype tool, ADUSA, and used it to evaluate our approach. Experimental results show the ability of our approach to detect bugs in both open-source as well as commercial database management systems."
26.url:http://dx.doi.org/10.1109/ASE.2008.34
26.opinion:exclude

27.title:"title":"Inferring Finite-State Models with Temporal Constraints"
27.abstract:"abstract":"Finite state machine-based abstractions of software behaviour are popular because they can be used as the basis for a wide range of (semi-) automated verification and validation techniques. These can however rarely be applied in practice, because the specifications are rarely kept up- to-date or even generated in the first place. Several techniques to reverse-engineer these specifications have been proposed, but they are rarely used in practice because their input requirements (i.e. the number of execution traces) are often very high if they are to produce an accurate result. An insufficient set of traces usually results in a state machine that is either too general, or incomplete. Temporal logic formulae can often be used to concisely express constraints on system behaviour that might otherwise require thousands of execution traces to identify. This paper describes an extension of an existing state machine inference technique that accounts for temporal logic formulae, and encourages the addition of new formulae as the inference process converges on a solution. The implementation of this process is openly available, and some preliminary results are provided."
27.url:http://dx.doi.org/10.1109/ASE.2008.35
27.opinion:exclude

28.title:"title":"Type-Checking Software Product Lines - A Formal Approach"
28.abstract:"abstract":"A software product line (SPL) is an efficient means to generate a family of program variants for a domain from a single code base. However, because of the potentially high number of possible program variants, it is difficult to test all variants and ensure properties like type-safety for the entire SPL. While first steps to type-check an entire SPL have been taken, they are informal and incomplete. In this paper, we extend the Featherweight Java (FJ) calculus with feature annotations to be used for SPLs. By extending FJ's type system, we guarantee that - given a well-typed SPL - all possible program variants are well- typed as well. We show how results from this formalization reflect and help implementing our own language-independent SPL tool CIDE."
28.url:http://dx.doi.org/10.1109/ASE.2008.36
28.opinion:exclude

29.title:"title":"Using Simulation to Investigate Requirements Prioritization Strategies"
29.abstract:"abstract":"Agile and traditional plan-based approaches to software system development both agree that prioritizing requirements is an essential activity. They differ in basic strategy - when to prioritize, to what degree, and how to guide implementation. As with many software engineering methods, verifying the benefit of following a particular approach is a challenge. Industry and student/classroom based experimental studies are generally impractical to use for large numbers of controlled experiments and benefits are difficult to measure directly. We use simulation to validate the fundamental, yet typically intangible benefits of requirements prioritization strategies. Our simulation is directly based on detailed empirical studies of agile and plan-based requirements management studies. Our simulation shows, as many have claimed, that an agile strategy excels when requirements are highly volatile, whereas a plan-based strategy excels when requirements are stable, and that there exist mixed strategies that are better than either for typical development efforts."
29.url:http://dx.doi.org/10.1109/ASE.2008.37
29.opinion:exclude

30.title:"title":"Automated Aspect Recommendation through Clustering-Based Fan-in Analysis"
30.abstract:"abstract":"Identifying code implementing a crosscutting concern (CCC) automatically can benefit the maintainability and evolvability of the application. Although many approaches have been proposed to identify potential aspects, a lot of manual work is typically required before these candidates can be converted into refactorable aspects. In this paper, we propose a new aspect mining approach, called clustering-based fan-in analysis (CBFA), to recommend aspect candidates in the form of method clusters, instead of single methods. CBFA uses a new lexical based clustering approach to identify method clusters and rank the clusters using a new ranking metric called cluster fan- in. Experiments on Linux and JHotDraw show that CBFA can provide accurate recommendations while improving aspect mining coverage significantly compared to other state-of-the-art mining approaches."
30.url:http://dx.doi.org/10.1109/ASE.2008.38
30.opinion:exclude

31.title:"title":"Predictive Typestate Checking of Multithreaded Java Programs"
31.abstract:"abstract":"Writing correct multithreaded programs is difficult. Existing tools for finding bugs in multithreaded programs primarily focus on finding generic concurrency problems such as data races, atomicity violations, and deadlocks. However, these generic bugs may sometimes be benign and may not help to catch other functional errors in multithreaded programs. In this paper, we focus on a high-level programming error, called typestate error, which happens when a program does not follow the correct usage protocol of an object. We present a novel technique that finds typestate errors in multithreaded programs by looking at a successful execution. An appealing aspect of our technique is that it not only finds typestate errors that occur during a program execution, but also many other typestate errors that could have occurred in a different execution. We have implemented this technique in a prototype tool for Java and have experimented it with a number of real-world Java programs."
31.url:http://dx.doi.org/10.1109/ASE.2008.39
31.opinion:exclude

32.title:"title":"Improving Structural Testing of Object-Oriented Programs via Integrating Evolutionary Testing and Symbolic Execution"
32.abstract:"abstract":"Achieving high structural coverage such as branch coverage in object-oriented programs is an important and yet challenging goal due to two main challenges. First, some branches involve complex program logics and generating tests to cover them requires deep knowledge of the program structure and semantics. Second, covering some branches requires special method sequences to lead the receiver object or non-primitive arguments to specific desirable states. Previous work has developed the symbolic execution technique and the evolutionary testing technique to address these two challenges, respectively. However, neither technique was designed to address both challenges at the same time. To address the respective weaknesses of these two previous techniques, we propose a novel framework called Evacon that integrates evolutionary testing (used to search for desirable method sequences) and symbolic execution (used to generate desirable method arguments). We have implemented our framework and applied it to test 13 classes previously used in evaluating white-box test generation tools. The experimental results show that the tests generated using our framework can achieve higher branch coverage than the ones generated by evolutionary testing, symbolic execution, or random testing within the same amount of time."
32.url:http://dx.doi.org/10.1109/ASE.2008.40
32.opinion:exclude

33.title:"title":"Automatic Debugging of Concurrent Programs through Active Sampling of Low Dimensional Random Projections"
33.abstract:"abstract":"Concurrent computer programs are fast becoming prevalent in many critical applications. Unfortunately, these programs are especially difficult to test and debug. Recently, it has been suggested that injecting random timing noise into many points within a program can assist in eliciting bugs within the program. Upon eliciting the bug, it is necessary to identify a minimal set of points that indicate the source of the bug to the programmer. In this paper, we pose this problem as an active feature selection problem. We propose an algorithm called the iterative group sampling algorithm that iteratively samples a lower dimensional projection of the program space and identifies candidate relevant points. We analyze the convergence properties of this algorithm. We test the proposed algorithm on several real-world programs and show its superior performance. Finally, we show the algorithms' performance on a large concurrent program."
33.url:http://dx.doi.org/10.1109/ASE.2008.41
33.opinion:exclude

34.title:"title":"How Program History Can Improve Code Completion"
34.abstract:"abstract":"Code completion is a widely used productivity tool. It takes away the burden of remembering and typing the exact names of methods or classes: As a developer starts typing a name, it provides a progressively refined list of candidates matching the name. However, the candidate list always comes in alphabetic order, i.e., the environment is only second-guessing the name based on pattern matching. Finding the correct candidate can be cumbersome or slower than typing the full name. We present an approach to improve code completion with program history. We define a benchmark measuring the accuracy and usefulness of a code completion engine. Further, we use the change history data to also improve the results offered by code completion tools. Finally, we propose an alternative interface for completion tools."
34.url:http://dx.doi.org/10.1109/ASE.2008.42
34.opinion:exclude

35.title:"title":"SpotWeb: Detecting Framework Hotspots and Coldspots via Mining Open Source Code on the Web"
35.abstract:"abstract":"Software developers often face challenges in reusing open source frameworks due to several factors such as the framework complexity and lack of proper documentation. In this paper, we propose a code-search-engine-based approach that detects hotspots in a given framework by mining code examples gathered from open source repositories available on the Web; these hotspots are API classes and methods that are frequently reused. Hotspots can serve as starting points for developers in understanding and reusing the given framework. Our approach also detects coldspots, which are API classes and methods that are rarely used. Coldspots serve as caveats for developers as there can be difficulties in finding relevant code examples and are generally less exercised compared to hotspots. We developed a tool, called SpotWeb, for frameworks or libraries written in Java and used our tool to detect hotspots and coldspots of eight widely used open source frameworks. We show the utility of our detected hotspots by comparing these hotspots with the API classes reused by a real application and compare our results with the results of a previous related approach."
35.url:http://dx.doi.org/10.1109/ASE.2008.43
35.opinion:exclude

36.title:"title":"Generic Patch Inference"
36.abstract:"abstract":"A key issue in maintaining Linux device drivers is the need to update drivers in response to evolutions in Linux internal libraries. Currently, there is little tool support for performing and documenting such changes. In this paper we present a tool, spdiff, that identifies common changes made in a set of pairs of files and their updated versions, and extracts a generic patch performing those changes. Library developers can use our tool to extract a generic patch based on the result of manually updating a few typical driver files, and then apply this generic patch to other drivers. Driver developers can use it to extract an abstract representation of the set of changes that others have made. Our experiments on recent changes in Linux show that the inferred generic patches are more concise than the corresponding patches found in commits to the Linux source tree while being safe with respect to the changes performed in the provided pairs of driver files."
36.url:http://dx.doi.org/10.1109/ASE.2008.44
36.opinion:exclude

37.title:"title":"Configuration Lifting: Verification meets Software Configuration"
37.abstract:"abstract":"Configurable software is ubiquitous, and the term software product line (SPL) has been coined for it lately. It remains a challenge, however, how such software can be verified over all variants. Enumerating all variants and analyzing them individually is inefficient, as knowledge cannot be shared between analysis runs. Instead of enumeration we present a new technique called lifting that converts all variants into a meta-program, and thus facilitates the configuration-aware application of verification techniques like static analysis, model checking and deduction-based approaches. As a side-effect, lifting provides a technique for checking software feature models, which describe software variants, for consistency. We demonstrate the feasibility of our approach by checking configuration dependent hazards for the highly configurable Linux kernel which possesses several thousand of configurable features. Using our techniques, two novel bugs in the kernel configuration system were found."
37.url:http://dx.doi.org/10.1109/ASE.2008.45
37.opinion:exclude

38.title:"title":"Product Line Tools are Product Lines Too: Lessons Learned from Developing a Tool Suite"
38.abstract:"abstract":"Tool developers are facing high expectations regarding the capabilities and usability of software engineering tools. Users expect tools which are tailored to their specific needs and integrated in their environment. This increases the complexity of tools and makes their customization more difficult, although numerous mechanisms supporting adaptability and extensibility are available. In this experience paper we report on the lessons we have learned when developing a tool suite for product line engineering. Our experiences suggest that software engineering tools should be designed as product lines."
38.url:http://dx.doi.org/10.1109/ASE.2008.46
38.opinion:exclude

39.title:"title":"A Specification Language for Static Analysis of Student Exercises"
39.abstract:"abstract":"In this paper we use formal software engineering techniques to support one of the most difficult steps in software engineering: learning to use a programming language. In order to support numerous exercises of a large number of students some automatic support for checking the solutions and providing hints for the occurrence of erroneous code fragments is highly desirable to help students doing their work. For such support a specification language to describe search patterns for typical solution patterns in student exercises has been created. This language especially supports the structured description of complex search patterns. The specified patterns are transformed into graph rules, which are applied to student solutions in our testing support environment. The approach extends the work we presented in [5]. In addition to the application areas described here the search for a class of structures can be used in other areas like the check for design patterns or looking for code fragments where refactorings can be applied."
39.url:http://dx.doi.org/10.1109/ASE.2008.47
39.opinion:exclude

40.title:"title":"A Case Study on the Automatic Composition of Network Application Mashups"
40.abstract:"abstract":"MaxMash is a tool that can compose select features of networked application and generate the source code for application mashups that can integrate those features. This paper presents a case study that demonstrates how MaxMash is used to combine the Jabber chatting protocol and the Microsoft Maps Web application. The composed mashup is able to answer direction queries via a chat client."
40.url:http://dx.doi.org/10.1109/ASE.2008.48
40.opinion:exclude

41.title:"title":"Predicting Effectiveness of Automatic Testing Tools"
41.abstract:"abstract":"Automatic white-box test generation is a challenging problem. Many existing tools rely on complex code analyses and heuristics. As a result, structural features of an input program may impact tool effectiveness in ways that tool users and designers may not expect or understand. We develop a technique that uses structural program metrics to predict the test coverage achieved by three automatic test generation tools. We use coverage and structural metrics extracted from 11 software projects to train several decision tree classifiers. Our experiments show that these classifiers can predict high or low coverage with success rates of 82% to 94%."
41.url:http://dx.doi.org/10.1109/ASE.2008.49
41.opinion:exclude

42.title:"title":"Distributed Constraints Maintenance in Collaborative UML Modeling Environments"
42.abstract:"abstract":"Constraints maintenance plays an important role in keeping the integrity and validity of models in UML software modeling. Constraints maintenance capabilities are reasonably adequate in UML modeling applications, but few work has been done to address the distributed constraints maintenance issue in collaborative UML modeling environments. In this paper, we propose a novel solution to the issue, which can retain the effects of all concurrent modeling operations even though they may cause constraints violations. We further contribute a distributed constraints maintenance framework, in which the solution is encapsulated as a generic engine to be mounted in a variety of single-user UML modeling applications for supporting distributed collaborative UML modeling and distributed constraints maintenance."
42.url:http://dx.doi.org/10.1109/ASE.2008.50
42.opinion:exclude

43.title:"title":"Software Cost Estimation using Fuzzy Decision Trees"
43.abstract:"abstract":"This paper addresses the issue of software cost estimation through fuzzy decision trees, aiming at acquiring accurate and reliable effort estimates for project resource allocation and control. Two algorithms, namely CHAID and CART, are applied on empirical software cost data recorded in the ISBSG repository. Approximately 1000 project data records are selected for analysis and experimentation, with fuzzy decision trees instances being generated and evaluated based on prediction accuracy. The set of association rules extracted is used for providing mean effort value ranges. The experimental results suggest that the proposed approach may provide accurate cost predictions in terms of effort. In addition, there is strong evidence that the fuzzy transformation of cost drivers contribute to enhancing the estimation process."
43.url:http://dx.doi.org/10.1109/ASE.2008.51
43.opinion:exclude

44.title:"title":"Rhizome: A Feature Modeling and Generation Platform"
44.abstract:"abstract":"Rhizome is an end-to-end feature modeling and code generation platform that includes a feature modeling language (FeatureML), a template language (MarkerML) and a template-based code generator. A software designer creates feature models using FeatureML by selecting and defining design choices. These design choices can be automatically associated with code templates and interpreted as parameter values for code generation. The code generator then replaces markers embedded in the code templates with dynamically generated code blocks to produce source code."
44.url:http://dx.doi.org/10.1109/ASE.2008.52
44.opinion:exclude

45.title:"title":"Combining the Analysis of Spatial Layout and Text to Support Design Exploration"
45.abstract:"abstract":"The design exploration (DE) approach allows a large number of probable end users to communicate with software developers by creating mockups of user interfaces and augmenting the partial designs (e.g. windows and widgets) produced with textual descriptions and explanations. Software developers collect and analyze these annotated partial designs to gain an understanding of the user, their activity, and the domain of concern. This paper presents the DE analyzer and its techniques for automatically analyzing the collection of annotated partial designs. The DE Analyzer uses a combination of textual and spatial layout analysis algorithms to assist software developers' navigation and exploration of the information collected using the DE Builder."
45.url:http://dx.doi.org/10.1109/ASE.2008.53
45.opinion:exclude

46.title:"title":"Using Cluster Analysis to Improve the Design of Component Interfaces"
46.abstract:"abstract":"For large software systems, interface structure has an important impact on their maintainability and build performance. For example, for complex systems written in C, recompilation due to a change in one central header file can run into hours. In this paper, we explore how automated cluster analysis can be used to refactor interfaces, in order to reduce the number of dependencies and to improve encapsulation, thus improving build performance and maintainability. We implemented our approach in a tool called \"Interface Regroup Wizard\", which we applied to several interfaces of a large industrial embedded system. From this, we not only learned that automated cluster analysis works surprisingly well to improve the design of interfaces, but also which of the refactoring steps are best done manually by an architect."
46.url:http://dx.doi.org/10.1109/ASE.2008.54
46.opinion:exclude

47.title:"title":"Augmenting Counterexample-Guided Abstraction Refinement with Proof Templates"
47.abstract:"abstract":"Existing software model checkers based on predicate abstraction and refinement typically perform poorly at verifying the absence of buffer overflows, with analyses depending on the sizes of the arrays checked. We observe that many of these analyses can be made efficient by providing proof templates for common array traversal idioms idioms, which guide the model checker towards proofs that are independent of array size. We have integrated this technique into our software model checker, PtYasm, and have evaluated our approach on a set of testcases derived from the Verisec suite, demonstrating that our technique enables verification of the safety of array accesses independently of array size."
47.url:http://dx.doi.org/10.1109/ASE.2008.55
47.opinion:exclude

48.title:"title":"An Assume Guarantee Verification Methodology for Aspect-Oriented Programming"
48.abstract:"abstract":"We propose a modular verification methodology for aspect oriented programs. Aspects are the new modularization units to encapsulate crosscutting concerns and have powerful features whose effects can drastically change software behavior. Having such an impact on behavior requires an automated verification support. In this work we introduce a technique that separately answers two questions: \"does the aspect have the provisioned effect?\" and \"does the base program satisfy the assumptions of the aspect?\" To answer these questions modularly, we propose using design contracts and state machines as aspect interfaces. An aspect interface both closes the environment of the aspect and specifies its assumptions on any base code. We show that our approach can be used in verifying AspectJ programs modularly and checking compatibility for aspect reuse."
48.url:http://dx.doi.org/10.1109/ASE.2008.56
48.opinion:exclude

49.title:"title":"Living with the Law: Can Automation give us Moore with Less?"
49.abstract:"abstract":"Multi-core programming presents developers with a dramatic paradigm shift. Whereas sequential programming largely allowed the decoupling of source from underlying architecture, it is now impossible to develop new patterns and abstractions in isolation from issues of modern hardware utilization. Synchronization and coordination are now manifested at all levels of the software stack, and developers currently lack the essential tools to even partially automate reasoning techniques and system configuration management. As a first stage to addressing this problem, this paper proposes a framework for a tool suite designed to partially automate the acquisition and management of static system visualization in a feedback loop with dynamic execution properties. This model enables developers to find a best fit system configuration, potentially reconciling resource contention and utilization tensions that are critical to multi-core platforms. The application of a prototype of this suite, Deja View, demonstrates how tool support can aid reasoning about causally related sets of changes across system artifacts."
49.url:http://dx.doi.org/10.1109/ASE.2008.57
49.opinion:exclude

50.title:"title":"VCR: Virtual Capture and Replay for Performance Testing"
50.abstract:"abstract":"This paper proposes a novel approach to performance testing, called virtual capture and replay (VCR), that couples capture and replay techniques with the checkpointing capabilities provided by the latest virtualization technologies. VCR enables software performance testers to automatically take a snapshot of a running system when certain critical conditions are verified, and to later replay the scenario that led to those conditions. Several in-depth analyses can be separately carried out in the laboratory just by rewinding the captured scenario and replaying it using different probes and analysis tools."
50.url:http://dx.doi.org/10.1109/ASE.2008.58
50.opinion:exclude

51.title:"title":"A Case for Automatic Exception Handling"
51.abstract:"abstract":"Exception handling mechanisms have been around for more than 30 years. Nevertheless, modern exceptions systems are not very different from the early models. Programming languages designers often neglect the exception mechanism and look at it more like an add-on for their language instead of central part. As a consequence, software quality suffers as programmers feel that the task of writing good error handling code is too complex, unattractive and inefficient. We propose a new model that automates the handling of exceptions by the runtime platform. This model frees the programmer from having to write exception handling code and, at the same time, successfully increases the resilience of programs to abnormal situations."
51.url:http://dx.doi.org/10.1109/ASE.2008.59
51.opinion:exclude

52.title:"title":"DiffGen: Automated Regression Unit-Test Generation"
52.abstract:"abstract":"Software programs continue to evolve throughout their lifetime. Maintenance of such evolving programs, including regression testing, is one of the most expensive activities in software development. We present an approach and its implementation called DiffGen for automated regression unit-test generation and checking for Java programs. Given two versions of a Java class, our approach instruments the code by adding new branches such that if these branches can be covered by a test generation tool, behavioral differences between the two class versions are exposed. DiffGen then uses a coverage-based test generation tool to generate test inputs for covering the added branches to expose behavioral differences. We have evaluated DiffGen on finding behavioral differences between 21 classes and their versions. Experimental results show that our approach can effectively expose many behavioral differences that cannot be exposed by state-of-the-art techniques."
52.url:http://dx.doi.org/10.1109/ASE.2008.60
52.opinion:exclude

53.title:"title":"An Automated Test Code Generation Method for Web Applications using Activity Oriented Approach"
53.abstract:"abstract":"Automated tests are important for Web applications as they grow more complex day by day. Web application testing frameworks have emerged to help satisfy this need. However, used without a model that is designed for system evolution and realization, maintaining test code becomes cumbersome and inefficient. This paper describes an activity oriented approach to engineer automated tests for Web applications with reference to a Web application developed for grant funding agencies. In this approach, the developer defines a domain model to represent application state, and uses a test activity graph comprised of self-validating user interactions to verify application correctness. We have implemented a test code generator called iTester using activity-oriented approach."
53.url:http://dx.doi.org/10.1109/ASE.2008.61
53.opinion:exclude

54.title:"title":"The Consistency of Web Conversations"
54.abstract:"abstract":"We describe BPELCheck, a tool for statically analyzing interactions of composite Web services implemented in BPEL. Our algorithm is compositional, and checks each process interacting with an abstraction of its peers, without constructing the product state space. Interactions between pairs of peer processes are modeled using conversation automata which encode the set of valid message exchange sequences between the two processes. A process is consistent if each possible conversation leaves its peer automata in a state labeled as consistent and the overall execution satisfies a user-specified predicate on the automata states. We have implemented BPELCheck in the Enterprise Service Pack of the NetBeans development environment. Our tool handles the major syntactic constructs of BPEL, including sequential and parallel composition, exception handling, flows, and Boolean state variables. We have used BPELCheck to check conversational consistency for a set of BPEL processes, including an industrial example."
54.url:http://dx.doi.org/10.1109/ASE.2008.62
54.opinion:exclude

55.title:"title":"Testing Peers' Volatility"
55.abstract:"abstract":"Peer-to-peer (P2P) is becoming a key technology for software development, but still lacks integrated solutions to build trust in the final software, in terms of correctness and security. Testing such systems is difficult because of the high numbers of nodes which can be volatile. In this paper, we present a framework for testing volatility of P2P systems. The framework is based on the individual control of peers, allowing test cases to precisely control the volatility of peers during execution. We validated our framework through implementation and experimentation on two open-source P2P systems. Through experimentation, we analyze the behavior of both systems on different conditions of volatility and show how the framework is able to detect implementation problems."
55.url:http://dx.doi.org/10.1109/ASE.2008.63
55.opinion:exclude

56.title:"title":"Automated Continuous Integration of Component-Based Software: An Industrial Experience"
56.abstract:"abstract":"When a software product is composed of dozens of or even hundreds of components with complicated dependency relationship among each other, one component's change can affect lots of other components' behavior. Sometimes, therefore, the stabilization job with multiple updated components drives the people who are responsible for integration and release the software into an integration hell. To avoid this kind of integration hell, we established integration procedure which encourages the developers frequent and small releases. We also created an automated integration system which continuously runs integration process in an incremental way so as to create and maintain an up-to-minute reasonably stable version of the product release candidate. In this paper, we introduce our integration procedure and automated integration system for a software product with hundreds of components, and a few lessons learned from the implementing and applying the system as well."
56.url:http://dx.doi.org/10.1109/ASE.2008.64
56.opinion:exclude

57.title:"title":"A Two-Step Approach for Modelling Flexibility in Software Processes"
57.abstract:"abstract":"In this paper, we present a two-step approach for modelling controlled flexibility in software processes: (1) senior process engineers express which, where and how changes can be applied onto process elements, (2) other process participants can easily identify which changes they are allowed to perform, and act accordingly. To support this, we propose a flexibility meta-model and modelling language, and a software process modelling tool called FlexEPFC. Finally, we compare FlexEPFC with three other prominent process-aware tools: SPADE, JIL/Juliette and Jazz."
57.url:http://dx.doi.org/10.1109/ASE.2008.65
57.opinion:exclude

58.title:"title":"A Generic Approach for Class Model Normalization"
58.abstract:"abstract":"Designing and maintaining a huge class model is a very complex task. When an object oriented software or model grows, duplicated elements start to appear, decreasing the readability and the maintainability. In this paper, we present an approach, implemented in a tool and validated by a case study, that helps software architects designing and improving their class models, discarding redundancy and adding relevant abstractions. Since many different languages allow to express class models, this approach has been made generic i.e. capable of dealing with any language described by a meta-model."
58.url:http://dx.doi.org/10.1109/ASE.2008.66
58.opinion:exclude

59.title:"title":"XE (eXtreme Editor) - Bridging the Aspect-Oriented Programming Usability Gap"
59.abstract:"abstract":"In spite of the modularization benefits supported by the Aspect-Oriented programming paradigm, different usability issues have hindered its adoption. The decoupling between aspect definitions and base code, and the compile-time weaving mechanism adopted by different AOP languages, require developers to manage the consistency between base code and aspect code themselves. These mechanisms create opportunities for errors related to aspect weaving invisibility and non-local control characteristics of AOP languages. This paper describes XE (Extreme Editor), an IDE that supports developers in managing these issues in the functional aspect-oriented programming domain."
59.url:http://dx.doi.org/10.1109/ASE.2008.67
59.opinion:exclude

60.title:"title":"Rapid: Identifying Bug Signatures to Support Debugging Activities"
60.abstract:"abstract":"Most existing fault-localization techniques focus on identifying and reporting single statements that may contain a fault. Even in cases where a fault involves a single statement, it is generally hard to understand the fault by looking at that statement in isolation. Faults typically manifest themselves in a specific context, and knowing that context is necessary to diagnose and correct the fault. In this paper, we present a novel fault-localization technique that identifies sequences of statements that lead to a failure. The technique works by analyzing partial execution traces corresponding to failing executions and identifying common segments in these traces, incrementally. Our approach provides developers a context that is likely to result in a more directed approach to fault understanding and a lower overall cost for debugging."
60.url:http://dx.doi.org/10.1109/ASE.2008.68
60.opinion:exclude

61.title:"title":"Heuristics for Scalable Dynamic Test Generation"
61.abstract:"abstract":"Recently there has been great success in using symbolic execution to automatically generate test inputs for small software systems. A primary challenge in scaling such approaches to larger programs is the combinatorial explosion of the path space. It is likely that sophisticated strategies for searching this path space are needed to generate inputs that effectively test large programs (by, e.g., achieving significant branch coverage). We present several such heuristic search strategies, including a novel strategy guided by the control flow graph of the program under test. We have implemented these strategies in CREST, our open source concolic testing tool for C, and evaluated them on two widely-used software tools, grep 2.2 (15 K lines of code) and Vim 5.7 (150 K lines). On these benchmarks, the presented heuristics achieve significantly greater branch coverage on the same testing budget than concolic testing with a traditional depth-first search strategy."
61.url:http://dx.doi.org/10.1109/ASE.2008.69
61.opinion:exclude

62.title:"title":"Managing Models through Macromodeling"
62.abstract:"abstract":"Software development involves the creation and use of many related models yet there are few tools that address the issue of how to work with and manage such collections of models, or \"multimodels.\" We propose a formal multimodeling framework that allows specialized model relationship types to be defined on existing types of models and provides a new type of model with a formal semantics called a macromodel. Macromodels can be used to enhance multimodel development, comprehension, consistency management and evolution. A preliminary evaluation of the framework is done using a detailed example from the telecommunications domain."
62.url:http://dx.doi.org/10.1109/ASE.2008.70
62.opinion:exclude

63.title:"title":"Cleman: Comprehensive Clone Group Evolution Management"
63.abstract:"abstract":"Recent research results have shown more benefits of the management of code clones, rather than detecting and removing them. However, existing management approaches for code clone group evolution are still ad hoc, unsatisfactory, and limited. In this paper, we introduce a novel method for comprehensive code clone group management in evolving software. The core of our method is Cleman, an algorithmic framework that allows for a systematic construction of efficient and accurate clone group management tools. Clone group management is rigorously formulated by a formal model, which provides the foundation for Cleman framework. We use Cleman framework to build a clone group management tool that is able to detect high-quality clone groups and efficiently manage them when the software evolves. We also conduct an empirical evaluation on real-world systems to show the flexibility of Cleman framework and the efficiency, completeness, and incremental updatability of our tool."
63.url:http://dx.doi.org/10.1109/ASE.2008.71
63.opinion:exclude

64.title:"title":"Composition of Qualitative Adaptation Policies"
64.abstract:"abstract":"In a highly dynamic environment, software systems requires a capacity of self-adaptation to fit the environment and the user needs evolution, which increases the software architecture complexity. Despite most current execution platforms include some facilities for handling dynamic adaptation, current design methodologies do not address this issue. One of the requirement for such a design process is to describe adaptation policies in a composable and qualitative fashion in order to cope with complexity. This paper introduces an approach for describing adaptation policies in a qualitative way while keeping the compositionality of adaptation policies. The basic example of a Web server is used to illustrate how to specify and to compose two adaptations policies which handle respectively the use of a cache and the deployment of new data sources."
64.url:http://dx.doi.org/10.1109/ASE.2008.72
64.opinion:exclude

65.title:"title":"A System for Supporting Development of Large Scaled Rich Internet Applications"
65.abstract:"abstract":"Rich Internet Application (RIA) has been proposed in order to solve the problems of current web applications. The user experience of current web applications is not comparable to desktop applications. RIA provides sophisticated interfaces for representing complex processes and data. Therefore, it requires collaboration between designers who design the interface and animation of an application and developers who implement business logics. In the development process of an application, the change of design is usually happened and it requires not only designer's work but also developer's work.Therefore, it costs a lot in large scaled applications. This paper provides a system which can provide complete separation of designer's and developer's work in source code level in order to reduce development costs for RIAs. In addition, we introduce this system into a practical system and evaluate its utility."
65.url:http://dx.doi.org/10.1109/ASE.2008.73
65.opinion:exclude

66.title:"title":"Discovering Patterns of Change Types"
66.abstract:"abstract":"The reasons why software is changed are manyfold; new features are added, bugs have to be fixed, or the consistency of coding rules has to be re-established. Since there are many types of of source code changes we want to explore whether they appear frequently together in time and whether they describe specific development activities. We describe a semi-automated approach to discover patterns of such change types using agglomerative hierarchical clustering. We extracted source code changes of one commercial and two open-source software systems and applied the clustering. We found that change type patterns do describe development activities and affect the control flow, the exception flow, or change the API."
66.url:http://dx.doi.org/10.1109/ASE.2008.74
66.opinion:exclude

67.title:"title":"Model-Driven Development of Mobile Personal Health Care Applications"
67.abstract:"abstract":"Personal health care applications on mobile devices allow patients to enhance their health via reminders, monitoring and feedback to health care providers. Engineering such applications is challenging with a need for health care plan meta-models, per-patient instantiation of care plans, and development and deployment of supporting Web and mobile device applications. We describe a novel prototype environment for visually modelling health care plans and automated plan and mobile device application code generation."
67.url:http://dx.doi.org/10.1109/ASE.2008.75
67.opinion:exclude

68.title:"title":"Enforcing Structural Regularities in Source Code using IntensiVE"
68.abstract:"abstract":"The design and implementation of a software system is often governed by many different coding conventions, design patterns, architectural design rules, and other so-called structural regularities. To prevent a deterioration of the system's source code, it is important that these regularities are verified and enforced in subsequent evolutions of the system. The Intensional Views Environment (IntensiVE), which is the subject of this demonstration, is a tool suite for documenting such structural regularities in (object-oriented) software systems and verifying their consistency in later versions of those systems."
68.url:http://dx.doi.org/10.1109/ASE.2008.76
68.opinion:exclude

69.title:"title":"QuARS Express - A Tool Demonstration"
69.abstract:"abstract":"Requirements analysis is an important phase in a software project. Automatic evaluation of natural language (NL) requirements documents has been proposed as a means to improve the quality of the system under development. QuARS Express is an automatic analyzer of natural language (NL) requirements able to manage complex and structured requirement documents containing metadata, and to produce an analysis report rich of information that points out linguistic defects and indications about the writing style of NL requirements."
69.url:http://dx.doi.org/10.1109/ASE.2008.77
69.opinion:exclude

70.title:"title":"MTSA: The Modal Transition System Analyser"
70.abstract:"abstract":"Modal transition systems (MTS) are operational models that distinguish between required and proscribed behaviour of the system to be and behaviour which it is not yet known whether the system should exhibit. MTS, in contrast with traditional behaviour models, support reasoning about the intended system behaviour in the presence of incomplete knowledge. In this paper, we present MTSA a tool that supports the construction, analysis and elaboration of Modal Transition Systems (MTS)."
70.url:http://dx.doi.org/10.1109/ASE.2008.78
70.opinion:exclude

71.title:"title":"MaramaEML: An Integrated Multi-View Business Process Modelling Environment with Tree-Overlays, Zoomable Interfaces and Code Generation"
71.abstract:"abstract":"MaramaEML is an integrated support tool for the enterprise modelling language (EML) built using the Eclipse based Marama framework. It provides support for multiple visual notations including: the business process modelling notation (BPMN); the EML tree- based, multi-layer hierarchical service representation; fisheye zooming capabilities; automatic BPEL code generation; and inter-notation mapping."
71.url:http://dx.doi.org/10.1109/ASE.2008.79
71.opinion:exclude

72.title:"title":"PtYasm: Software Model Checking with Proof Templates"
72.abstract:"abstract":"We describe PTYASM, an enhanced version of the YASM software model checker which uses proof templates. These templates associate correctness arguments with common programming idioms, thus enabling efficient verification. We have used PTYASM to verify the safety of array accesses in programs derived from the Verisec suite. PTYASM is able to verify this property in the majority of testcases, while existing software model checkers fail to do so due to loop unrolling."
72.url:http://dx.doi.org/10.1109/ASE.2008.80
72.opinion:exclude

73.title:"title":"Semi-Automating Pragmatic Reuse Tasks"
73.abstract:"abstract":"Developers undertaking a pragmatic reuse task must collect and reason about information that is spread throughout the source code of a system before they can understand the scope of their task. Once they have this understanding, they can undertake the manual process of actually reusing the source code within their system. We have created a tool environment to help developers plan and perform pragmatic reuse tasks enabling them to reason about and perform larger reuse tasks than they generally feel comfortable attempting otherwise."
73.url:http://dx.doi.org/10.1109/ASE.2008.81
73.opinion:exclude

74.title:"title":"DUALLY: A framework for Architectural Languages and Tools Interoperability"
74.abstract:"abstract":"Nowadays different notations for architectural modeling have been proposed, each one focussing on a specific application domain, analysis type, or modeling environment. No effective interoperability is possible to date. DUALLY is an automated framework that aims to offer an answer to this need allowing architectural languages and tools interoperability. DUALLY has been implemented as an Eclipse plugin and it is based on model transformation techniques. This demonstration paper shows DUALLY by applying its approach to two outstanding architectural description languages."
74.url:http://dx.doi.org/10.1109/ASE.2008.82
74.opinion:exclude

75.title:"title":"Automated Mapping from Goal Models to Self-Adaptive Systems"
75.abstract:"abstract":"Self-adaptive systems should autonomously adapt at run time to changes in their operational environment, guided by the goals assigned by their stakeholders. We present a tool that supports goal-oriented modelling and generation of code for goal-directed, self-adaptive systems, supporting Tropos4AS, an extension of the software engineering methodology Tropos."
75.url:http://dx.doi.org/10.1109/ASE.2008.83
75.opinion:exclude

76.title:"title":"ADDSS: Architecture Design Decision Support System Tool"
76.abstract:"abstract":"This paper describes the ADDSS tool which enables capturing and documenting architectural design decisions in order to avoid knowledge vaporization."
76.url:http://dx.doi.org/10.1109/ASE.2008.84
76.opinion:exclude

77.title:"title":"APPAREIL: A Tool for Building Automated Program Translators Using Annotated Grammars"
77.abstract:"abstract":"Operations languages are used to write spacecraft operations procedures. The APPAREIL tool automates the process of generating program translators between operations languages, from a specification of their language grammar annotated with extra information. From these annotated grammars the tool automatically produces a partial translator that covers most of the translation. This translator needs to be augmented manually with specific transformations, to deal with the more complicated cases. To get more confidence on the correctness of the translation, the tool offers a control-flow equivalence verification module."
77.url:http://dx.doi.org/10.1109/ASE.2008.85
77.opinion:exclude

78.title:"title":"Rapid Model-Driven Prototyping and Verification of High-Integrity Real-Time Systems"
78.abstract:"abstract":"Model-driven technologies offer a most attractive framework for rapid, iterative software development cycles by facilitating a productive merge of high-level modeling with automated model transformation and verification. In the high- integrity application domain it is of paramount importance to assure semantics preservation across each view of the system produced as part of the development process, most notably the design model, the analysis model, the executable. In this short paper we present a toolset that assures the preservation of properties of interest across all stages of an MDE-geared development."
78.url:http://dx.doi.org/10.1109/ASE.2008.86
78.opinion:exclude

79.title:"title":"Save-IDE: An Integrated Development Environment for Building Predictable Component-Based Embedded Systems"
79.abstract:"abstract":"In this paper we present an Integrated Development Environment Save-IDE, a toolset that embraces several tools: a tool for designing component-based systems and components, modeling and predicting certain run-time properties, such as timing properties, and transforming the components to real-time execution elements. Save-IDE is specialized for the domain of dependable embedded systems, which in addition to standard design tools requires tool support for analysis and verification of particular properties of such systems."
79.url:http://dx.doi.org/10.1109/ASE.2008.87
79.opinion:exclude

80.title:"title":"The Clem Toolkit"
80.abstract:"abstract":"In this demonstration session, we present a toolkit we have designed around a model-driven language fLEJ. This relies on formal methods to ease the development of applications in an efficient and reusable way. Formal methods have been advocated as a means of increasing the reliability of systems, especially those which are safety or business critical. It is still difficult to develop automatic specification and verification tools due to limitations like state explosion, undecidability, etc. To face these problems, we provide LE with a constructive semantic that allows the modular compilation of programs into software and hardware targets (C code, VHDL code, FPGA synthesis, Verification tools). Moreover, we also provide software to design, compile and verify LE programs. Our approach is pertinent according to the two main requirements of critical realistic applications: the modular compilation allows us to deal with large systems, the model-driven approach provides us with formal validation."
80.url:http://dx.doi.org/10.1109/ASE.2008.88
80.opinion:exclude

81.title:"title":"Tool Support for Parametric Analysis of Large Software Simulation Systems"
81.abstract:"abstract":"The analysis of large and complex parameterized software systems, e.g., systems simulation in aerospace, is very complicated and time-consuming due to the large parameter space, and the complex, highly coupled nonlinear nature of the different system components. Thus, such systems are generally validated only in regions local to anticipated operating points rather than through characterization of the entire feasible operational envelope of the system. We have addressed the factors deterring such an analysis with a tool to support envelope assessment: we utilize a combination of advanced Monte Carlo generation with n-factor combinatorial parameter variations to limit the number of cases, but still explore important interactions in the parameter space in a systematic fashion. Additional test-cases, automatically generated from models (e.g., UML, Simulink, Stateflow) improve the coverage. The distributed test runs of the software system produce vast amounts of data, making manual analysis impossible. Our tool automatically analyzes the generated data through a combination of unsupervised Bayesian clustering techniques (AutoBayes) and supervised learning of critical parameter ranges using the treatment learner TAR3. The tool has been developed around the Trick simulation environment, which is widely used within NASA. We will present this tool with a GN&amp;C (Guidance, Navigation and Control) simulation of a small satellite system."
81.url:http://dx.doi.org/10.1109/ASE.2008.89
81.opinion:exclude

82.title:"title":"ReqsCov: A Tool for Measuring Test-Adequacy over Requirements"
82.abstract:"abstract":"When creating test cases for software, a common approach is to create tests that exercise requirements. Determining the adequacy of test cases, however, is generally done through inspection or indirectly by measuring structural coverage of an executable artifact (such as source code or a software model). We present ReqsCov, a tool to directly measure requirements coverage provided by test cases. ReqsCov allows users to measure Linear Temporal Logic requirements coverage using three increasingly rigorous requirements coverage metrics: naive coverage, antecedent coverage, and Unique First Cause coverage. By measuring requirements coverage, users are given insight into the quality of test suites beyond what is available when solely using structural coverage metrics over an implementation."
82.url:http://dx.doi.org/10.1109/ASE.2008.90
82.opinion:exclude

83.title:"title":"AspectM: UML-Based Extensible AOM Language"
83.abstract:"abstract":"AspectM, a UML-based aspect-oriented modeling (AOM) language, provides not only basic modeling constructs but also an extension mechanism called metamodel access protocol (MMAP) that allows a modeler to extend the AspectM metamodel. MMAP enables a modeler to construct domain-specific AOM languages at relatively low cost. In this paper, we show the overview of an AspectM support tool consisting of a reflective model editor and a verifying model weaver."
83.url:http://dx.doi.org/10.1109/ASE.2008.91
83.opinion:exclude

84.title:"title":"Tools for Traceability in Secure Software Development"
84.abstract:"abstract":"For secure and dependable software system development, one must ensure that security requirements are truly traceable to design and implementation, and the traceability links can be updated accordingly to changed entities. To address this, we present a suite of security requirements analysis and traceability assurance tools and demonstrate how they are effectively integrated."
84.url:http://dx.doi.org/10.1109/ASE.2008.92
84.opinion:exclude

85.title:"title":"Unifying Analysis Tools with Rosetta"
85.abstract:"abstract":"The Rosetta system specification language will require a variety of analysis capabilities to assist system designers. The language's generality prohibits the development of a single analysis tool. It is proposed, instead, to leverage the existing analysis tools and create an analysis environment unified around the Rosetta language. A semi-automated tool, the Rosetta Nexus, will generated tool-specific analysis models and a correspondence with the original Rosetta specifications."
85.url:http://dx.doi.org/10.1109/ASE.2008.93
85.opinion:exclude

86.title:"title":"Model-Driven Development of Mobile Applications"
86.abstract:"abstract":"This research aims to simplify the creation of applications for mobile platforms by developing a high-level and platform independent model of an application, and automatically transforming this high-level model to platform specific code. The research method is a combination of the model-driven development (MDD) approach in software development and application of techniques in the field of human-computer interaction (HCI) particularly on user- centered system design. This research involves developing a graphical modeling language which is specific to mobile applications and coming up with a generic algorithm for the conversion of this graphical model into code. The main focus of the research however, is on the design of the graphical model, and the interaction techniques which will allow non-expert people to create specialized mobile applications with case."
86.url:http://dx.doi.org/10.1109/ASE.2008.94
86.opinion:exclude

87.title:"title":"Automated Web Performance Analysis"
87.abstract:"abstract":"Performance is a key feature in many systems nowadays. There are several tools on the market that ensure and test for adequate performance. They, can be divided into simulation tools and monitoring tools. But only a few automatise and combine both approaches. This paper describes a system capable of automatically creating a web performance simulation and conducting trend analysis of the system under test (SUT). To achieve this the system requires input information, like Monitoring Points and Static-Information about the SUT. The system monitors and analyses the SUT and based on this information generates a simulation model of the system. The simulation model is refinded stepwise e.g. by adding or removing connections between the model components or adjusting the parameters until the aimed accuracy is achieved. With the help of the simulation model a prediction module creates an analysis of the SUT, and thereby can give as much information about the current state of the system and potential trends as possible. This predictive information can be used for pro-active server tuning or other performance optimisations. The focus of my PhD thesis is on the adjustment and prediction part of the system described here. For all other parts, already existing tools and techniques will be used where possible. This initial paper outlines the complete system."
87.url:http://dx.doi.org/10.1109/ASE.2008.95
87.opinion:exclude

88.title:"title":"Automatic Test Generation for LUSTRE/SCADE Programs"
88.abstract:"abstract":"Lustre is a declarative, data-flow language, which is devoted to the specification of synchronous and real-time applications. It ensures efficient code generation and provides formal specification and verification facilities. A graphical tool dedicated to the development of critical embedded systems and often used by industries and professionals is SCADE (Safety Critical Application Development Environment). SCADE is a graphical environment based on the LUSTRE language and it allows the hierarchical definition of the system components and the automatic code generation. This research work is partially concerned with Lutess, a testing environment which automatically transforms formal specifications into test data generators."
88.url:http://dx.doi.org/10.1109/ASE.2008.96
88.opinion:exclude

89.title:"title":"Feature Interaction Detection in the Automotive Domain"
89.abstract:"abstract":"The main goal of our research is to develop techniques for the detection of feature interactions for embedded systems in the automotive domain. Automotive systems are cyber-physical systems (CPS), which are composed of a \"cyber\" part that consists of software components running on digital hardware and a \"physical\" part that is the mechanical processes monitored by the software. Feature interactions in automotive systems arise from the activation of two or more features sending requests to the mechanical processes that create contradictory physical forces, possibly at distinct times, that cause unsafe behaviours. An example is having simultaneous requests to apply the brakes and the throttle. While both actions may be \"correct\" according to the intended behaviour of each feature, their interaction is undesired and potentially dangerous. To deal with the feature interaction problem, we propose to perform analysis at design time by using formal methods to detect all possible interactions."
89.url:http://dx.doi.org/10.1109/ASE.2008.97
89.opinion:exclude

90.title:"title":"Towards Good Enough Testing: A Cognitive-Oriented Approach Applied to Infotainment Systems"
90.abstract:"abstract":"This contribution outlines a cognitive-oriented approach to construct test systems that can \"partially \" imitate several cognitive paradigms of skilled human testers. For example, learning, reasoning, optimization, etc. Hence, a reasonable portion of the workload done by a human tester would be shifted to the test system itself. This consequently leads to a substantial reduction in the development time and cost; yet the test efficiency is not sacrificed."
90.url:http://dx.doi.org/10.1109/ASE.2008.98
90.opinion:exclude

91.title:"title":"ARAMIS 2008: The First Int. Workshop on Automated engineeRing of Autonomic and run-tiMe evolvIng Systems"
91.abstract:"abstract":"Provides notice of upcoming conference events of interest to practitioners and researchers."
91.url:http://dx.doi.org/10.1109/ASE.2008.99
91.opinion:exclude

92.title:"title":"First International Workshop on Social Software Engineering and Applications (SoSEA 2008)"
92.abstract:"abstract":"Provides notice of upcoming conference events of interest to practitioners and researchers."
92.url:http://dx.doi.org/10.1109/ASE.2008.100
92.opinion:exclude

93.title:"title":"4th International ERCIM Workshop on Software Evolution and Evolvability (Evol'08)"
93.abstract:"abstract":"Provides notice of upcoming conference events of interest to practitioners and researchers."
93.url:http://dx.doi.org/10.1109/ASE.2008.101
93.opinion:exclude

