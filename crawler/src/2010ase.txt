1.title:Keynote address: toward compositional software engineering
1.abstract:Large-scale software development, for the longest time, has relied heavily on centralized, process-centric approaches, such as CCMI. There are three trends that make applying a traditional approach increasingly infeasible, i.e. the increasing adoption of software product lines, global software development and software ecosystem. Although agile software development methods have removed much of the inefficiencies in small and medium-scale software development, addressing the inefficiencies in large scale software development requires a more compositional approach. The presentation introduces the differences between intra-team and inter-team coordination and presents an architecture-centric approach to large-scale software development that heavily relies on automated tool support.
1.url:http://doi.acm.org/10.1145/1858996.1858997
1.opinion:exclude

2.title:Analyzing security architectures
2.abstract:We present a semi-automated approach, SECORIA, for analyzing a security runtime architecture for security and for conformance to an object-oriented implementation. Type-checkable annotations describe architectural intent within the code, enabling a static analysis to extract a hierarchical object graph that soundly reflects all runtime objects and runtime relations between them. In addition, the annotations can describe modular, code-level policies. A separate analysis establishes traceability between the extracted object graph and a target architecture documented in an architecture description language. Finally, architectural types, properties, and logic predicates describe global constraints on the target architecture, which will also hold in the implementation. We validate the SECORIA approach by analyzing a 3,000-line pedagogical Java implementation and a runtime architecture designed by a security expert.
2.url:http://doi.acm.org/10.1145/1858996.1859001
2.opinion:exclude

3.title:VikiBuilder: end-user specification and generation of visual wikis
3.abstract:With the need to make sense out of large and constantly growing information spaces, tools to support information management are becoming increasingly valuable. In prior work we proposed the "Visual Wiki" concept to describe and implement web-based information management applications. By focusing on the integration of two promising approaches, visualizations and collaboration tools, our Visual Wiki work explored synergies and demonstrated the value of the concept. Building on this, we introduce "VikiBuilder", a Visual Wiki meta-tool, which provides end-user supported modeling and automatic generation of Visual Wiki instances. We describe the design and implementation of the VikiBuilder including its architecture, a domain specific visual language for modeling Visual Wikis, and automatic generation of those. To demonstrate the utility of the tool, we have used it to construct a variety of different Visual Wikis. We describe the construction of Visual Wikis and discuss the strengths and weaknesses of our meta-tool approach.
3.url:http://doi.acm.org/10.1145/1858996.1859002
3.opinion:exclude

4.title:Software design sketching with calico
4.abstract:Despite the availability of a host of software design notations and associated tools, software developers are known to frequently turn to the whiteboard when faced with a specific design problem. There, they typically engage in an informal form of software design that relies heavily on sketching. However, whereas whiteboards afford flexibility and fluidity, they at the same time limit a designer in only being able to draw and erase content. This paper presents Calico, a novel software design tool that leverages electronic whiteboards to enhance the design experience with explicit support for the creative, exploratory aspects of design. Specifically, Calico offers a grid, scraps, and a palette together with gesture-based input to address several natural behaviors exhibited by software designers, including frequent shifts in focus, use of low-detail models, and use of a mix of notations. To evaluate Calico, we performed a laboratory experiment involving eight pairs of graduate students and collected and analyzed six corporate design sessions that employed Calico. Results are promising and indicate the benefits of Calico, while they at the same time highlight several ways in which it can be enhanced.
4.url:http://doi.acm.org/10.1145/1858996.1859003
4.opinion:exclude

5.title:Automatically documenting program changes
5.abstract:Source code modifications are often documented with log messages. Such messages are a key component of software maintenance: they can help developers validate changes, locate and triage defects, and understand modifications. However, this documentation can be burdensome to create and can be incomplete or inaccurate. We present an automatic technique for synthesizing succinct human-readable documentation for arbitrary program differences. Our algorithm is based on a combination of symbolic execution and a novel approach to code summarization. The documentation it produces describes the effect of a change on the runtime behavior of a program, including the conditions under which program behavior changes and what the new behavior is. We compare our documentation to 250 human-written log messages from 5 popular open source projects. Employing a human study, we find that our generated documentation is suitable for supplementing or replacing 89% of existing log messages that directly describe a code change.
5.url:http://doi.acm.org/10.1145/1858996.1859005
5.opinion:accept

6.title:Towards automatically generating summary comments for Java methods
6.abstract:Studies have shown that good comments can help programmers quickly understand what a method does, aiding program comprehension and software maintenance. Unfortunately, few software projects adequately comment the code. One way to overcome the lack of human-written summary comments, and guard against obsolete comments, is to automatically generate them. In this paper, we present a novel technique to automatically generate descriptive summary comments for Java methods. Given the signature and body of a method, our automatic comment generator identifies the content for the summary and generates natural language text that summarizes the method's overall actions. According to programmers who judged our generated comments, the summaries are accurate, do not miss important content, and are reasonably concise.
6.url:http://doi.acm.org/10.1145/1858996.1859006
6.opinion:exclude

7.title:Automatic detection of nocuous coordination ambiguities in natural language requirements
7.abstract:Natural language is prevalent in requirements documents. However, ambiguity is an intrinsic phenomenon of natural language, and is therefore present in all such documents. Ambiguity occurs when a sentence can be interpreted differently by different readers. In this paper, we describe an automated approach for characterizing and detecting so-called nocuous ambiguities, which carry a high risk of misunderstanding among different readers. Given a natural language requirements document, sentences that contain specific types of ambiguity are first extracted automatically from the text. A machine learning algorithm is then used to determine whether an ambiguous sentence is nocuous or innocuous, based on a set of heuristics that draw on human judgments, which we collected as training data. We implemented a prototype tool for Nocuous Ambiguity Identification (NAI), in order to illustrate and evaluate our approach. The tool focuses on coordination ambiguity. We report on the results of a set of experiments to assess the performance and usefulness of the approach.
7.url:http://doi.acm.org/10.1145/1858996.1859007
7.opinion:exclude

8.title:Flexible and scalable consistency checking on product line variability models
8.abstract:The complexity of product line variability models makes it hard to maintain their consistency over time regardless of the modeling approach used. Engineers thus need support for detecting and resolving inconsistencies. We describe experiences of applying a tool-supported approach for incremental consistency checking on variability models. Our approach significantly improves the overall performance and scalability compared to batch-oriented techniques and allows providing immediate feedback to modelers. It is extensible as new consistency constraints can easily be added. Furthermore, the approach is flexible as it is not limited to variability models and it also checks the consistency of the models with the underlying code base of the product line. We report the results of a thorough evaluation based on real-world product line models and discuss lessons learned.
8.url:http://doi.acm.org/10.1145/1858996.1859009
8.opinion:exclude

9.title:Variability modeling in the real: a perspective from the operating systems domain
9.abstract:Variability models represent the common and variable features of products in a product line. Several variability modeling languages have been proposed in academia and industry; however, little is known about the practical use of such languages. We study and compare the constructs, semantics, usage and tools of two variability modeling languages, Kconfig and CDL. We provide empirical evidence for the real-world use of the concepts known from variability modeling research. Since variability models provide basis for automated tools (feature dependency checkers and product configurators), we believe that our findings will be of interest to variability modeling language and tool designers.
9.url:http://doi.acm.org/10.1145/1858996.1859010
9.opinion:exclude

10.title:RESISTing reliability degradation through proactive reconfiguration
10.abstract:Situated software systems are an emerging class of systems that are predominantly pervasive, embedded, and mobile. They are marked with a high degree of unpredictability and dynamism in the execution context. At the same time, such systems often need to satisfy strict reliability requirements. Most current software reliability analysis approaches are not suitable for situated software systems. We propose an approach geared to such systems, which continuously furnishes refined reliability predictions at runtime by incorporating various sources of information. The reliability predictions are leveraged to proactively place the software in the optimal configuration with respect to changing conditions. Our approach considers two representative architectural reconfiguration decisions that impact the system's reliability: reallocation of components to processes and changing the architectural style. We have realized the approach as part of a framework intended for mission-critical settings, called REsilient SItuated SofTware system (RESIST), and evaluated it using a mobile emergency response system.
10.url:http://doi.acm.org/10.1145/1858996.1859011
10.opinion:exclude

11.title:Automatic construction of an effective training set for prioritizing static analysis warnings
11.abstract:In order to improve ineffective warning prioritization of static analysis tools, various approaches have been proposed to compute a ranking score for each warning. In these approaches, an effective training set is vital in exploring which factors impact the ranking score and how. While manual approaches to build a training set can achieve high effectiveness but suffer from low efficiency (i.e., high cost), existing automatic approaches suffer from low effectiveness. In this paper, we propose an automatic approach for constructing an effective training set. In our approach, we select three categories of impact factors as input attributes of the training set, and propose a new heuristic for identifying actionable warnings to automatically label the training set. Our empirical evaluations show that the precision of the top 22 warnings for Lucene, 20 for ANT, and 6 for Spring can achieve 100% with the help of our constructed training set.
11.url:http://doi.acm.org/10.1145/1858996.1859013
11.opinion:exclude

12.title:An automated approach for finding variable-constant pairing bugs
12.abstract:Named constants are used heavily in operating systems code, both as internal flags and in interactions with devices. Decision making within an operating system thus critically depends on the correct usage of these values. Nevertheless, compilers for the languages typically used in implementing operating systems provide little support for checking the usage of named constants. This affects correctness, when a constant is used in a context where its value is meaningless, and software maintenance, when a constant has the right value for its usage context but the wrong name. We propose a hybrid program-analysis and data-mining based approach to identify the uses of named constants and to identify anomalies in these uses. We have applied our approach to a recent version of the Linux kernel and have found a number of bugs affecting both correctness and software maintenance. Many of these bugs have been validated by the Linux developers.
12.url:http://doi.acm.org/10.1145/1858996.1859014
12.opinion:exclude

13.title:Deviance from perfection is a better criterion than closeness to evil when identifying risky code
13.abstract:We propose an approach for the automatic detection of potential design defects in code. The detection is based on the notion that the more code deviates from good practices, the more likely it is bad. Taking inspiration from artificial immune systems, we generated a set of detectors that characterize different ways that a code can diverge from good practices. We then used these detectors to measure how far code in assessed systems deviates from normality. We evaluated our approach by finding potential defects in two open-source systems (Xerces-J and Gantt). We used the library JHotDraw as the code base representing good design/programming practices. In both systems, we found that 90% of the riskiest classes were defects, a precision far superiour to state of the art rule-based approaches.
13.url:http://doi.acm.org/10.1145/1858996.1859015
13.opinion:exclude

14.title:Seamlessly integrated, but loosely coupled: building user interfaces from heterogeneous components
14.abstract:User interface development is a time and resource consuming task. Thus, reusing existing UI components is a desirable approach for rapid UI development. To keep UIs maintainable, those components should be loosely coupled. Composing UIs of heterogeneous components developed with different technologies, on the other hand, is a non-trivial task not supported well by currently existing integration frameworks, and there is only little progress in automatizing the integration step. In this paper, we introduce a framework for UI integration which is capable of handling heterogeneous UI components. It facilitates events annotated with RDF and ontologies for assembling user interfaces from loosely coupled components. With that framework, UIs can be composed semi-automatically, based on logic event processing rules.
14.url:http://doi.acm.org/10.1145/1858996.1859017
14.opinion:exclude

15.title:Tool support for quality-driven development of software architectures
15.abstract:In this paper, we present a prototype tool that supports the systematic development of software architectures driven by quality requirements using architectural tactics. The tool allows one to configure architectural tactics based on quality requirements and compose the configured tactics to produce an initial architecture for the system. We demonstrate the tool for developing an architecture for a resource profiling system in the web environment and validate the results using a set of metrics.
15.url:http://doi.acm.org/10.1145/1858996.1859018
15.opinion:exclude

16.title:MiTV: multiple-implementation testing of user-input validators for web applications
16.abstract:User-input validators play an essential role in guarding a web application against application-level attacks. Hence, the security of the web application can be compromised by defective validators. To detect defects in validators, testing is one of the most commonly used methodologies. Testing can be performed by manually writing test inputs and oracles, but this manual process is often labor-intensive and ineffective. On the other hand, automated test generators cannot generate test oracles in the absence of specifications, which are often not available in practice. To address this issue in testing validators, we propose a novel approach, called MiTV, that applies Multiple-implementation Testing for Validators, i.e., comparin gthe behavior of a validator under test with other validators of the same type. These other validators of the same type can be collected from either open or proprietary source code repositories. To show the effectiveness of MiTV, we applied MiTV on 53 different validators (of 6 common types) for web applications. Our results show that MiTV detected real defects in 70% of the validators.
16.url:http://doi.acm.org/10.1145/1858996.1859019
16.opinion:exclude

17.title:Model comparison with GenericDiff
17.abstract:This paper proposes GenericDiff, a general framework for model comparison. The main idea is to separate the specification of domain-specific model properties and syntax from the general graph matching process and to use composite numeric vectors and pairup graph to encode the domain-specific properties and syntax so that they can be uniformly exploited in the general matching process. Our initial evaluation demonstrates that it is easy to deploy GenericDiff in a new application domain and GenericDiff is able to produce an accurate comparison reports for diverse types of models.
17.url:http://doi.acm.org/10.1145/1858996.1859020
17.opinion:exclude

18.title:Eliminating products to test in a software product line
18.abstract:A Software Product Line (SPL) is a family of programs where each program is defined by a unique combination of features. Developing a set of programs with commonalities and variabilities in this way can significantly reduce both the time and cost of software development. However, as the number of programs may be exponential in the number of features, testing an SPL, the phase to which the majority of software development is dedicated, becomes especially challenging [12]. Indeed, scale is the biggest challenge in testing or checking the properties of programs in a product line. Even a product line with just 10 optional features has over a thousand (210) distinct programs. As an example of a situation where every program must be considered, suppose that every program of an SPL outputs a String that each feature might modify.
18.url:http://doi.acm.org/10.1145/1858996.1859021
18.opinion:exclude

19.title:A visual interactive debugger based on symbolic execution
19.abstract:We present the concepts, usage, and prototypic implementation of a new kind of visual debugging tool based on symbolic execution of Java source code called visual symbolic state debugger. It allows to start debugging of source code at any code location without the need to write a fixture as well as to visualize all possible symbolic execution paths and all symbolic states up to a finite depth. A code-based test generation facility is integrated.
19.url:http://doi.acm.org/10.1145/1858996.1859022
19.opinion:exclude

20.title:Model-driven reverse engineering of legacy graphical user interfaces
20.abstract:Businesses are more and more modernizing the legacy systems they developed with Rapid Application Development (RAD), so that they can benefit from the new platforms and technologies. In these systems, the Graphical User Interface (GUI) layout is implicitly given by the position of the GUI elements (i.e. coordinates). However, taking advantage of current features of GUI technologies often requires an explicit, high-level layout model. We propose a Model-Driven Engineering process to perform reverse engineering of RAD-built GUIs, which is focused on discovering the implicit layout, and produces a GUI model where the layout is explicit. Based on the information we obtain, other reengineering activities can be performed, for example, to adapt the GUI for mobile device screens.
20.url:http://doi.acm.org/10.1145/1858996.1859023
20.opinion:exclude

21.title:A two-step technique for extract class refactoring
21.abstract:We propose a novel approach supporting the Extract Class refactoring. The proposed approach analyzes the (structural and semantic) similarity of the methods in a class in order to identify chains of strongly related methods. The identified method chains are used to define new classes with higher cohesion than the original class. A preliminary evaluation reveals that the approach is able to identify meaningful refactoring operations.
21.url:http://doi.acm.org/10.1145/1858996.1859024
21.opinion:exclude

22.title:Detecting user-visible failures in AJAX web applications by analyzing users' interaction behaviors
22.abstract:Web applications can suffer from poor reliability, and AJAX technology makes Web sites even more error-prone. Failures of a Web application, particularly user-visible failures, impact users' satisfaction and may drive users away from using the Web site. Conventional testing techniques are inadequate for improving AJAX applications' reliability, and application providers commonly rely on fast failure detection, which is challenging. In this paper, we present a novel technique for automatically detecting user-visible failures in AJAX applications. Our technique trains a Bayesian model to analyze users' interaction behaviors to infer whether such user responses are related to user-visible failures. We implemented our technique in a tool called SIRANA. We performed a case study using a commercial AJAX application with seeded bugs, and collected users' interaction data during 14 one-hour sessions. We evaluated our technique using SIRANA applied to the collected data. The results demonstrate the effectiveness of our technique: It not only detected all seeded bugs, but also detected four real, previously-unknown bugs.
22.url:http://doi.acm.org/10.1145/1858996.1859025
22.opinion:exclude

23.title:Architectural style as an independent variable
23.abstract:A key idea in modern software engineering is that we can and should make architectural style choices separately from choices about many other system properties. There is a fundamental separation of concerns implicit in this idea: given an application model that expresses system properties independently of architectural style, we can choose a compatible architectural style and then map the application model to one or more architectural models (architectures) in this style. The problem is that we do not have a formal account of this separation of concerns, or the associated architectural maps, sufficient to enable automated synthesis of architectures from application models and architecture style specifications. The contribution of this work is such an account and a demonstration that it enables automated formal derivation of style-specific architectures.
23.url:http://doi.acm.org/10.1145/1858996.1859026
23.opinion:exclude

24.title:Text mining in supporting software systems risk assurance
24.abstract:Insufficient risk analysis often leads to software system design defects and system failures. Assurance of software risk documents aims to increase the confidence that identified risks are complete, specific, and correct. Yet assurance methods rely heavily on manual analysis that requires significant knowledge of historical projects and subjective, perhaps biased judgment from domain experts. To address the issue, we have developed RARGen, a text mining-based approach based on well-established methods aiming to automatically create and maintain risk repositories to identify usable risk association rules (RARs) from a corpus of risk analysis documents. RARs are risks that have frequently occurred in historical projects. We evaluate RARGen on 20 publicly available e-service projects. Our evaluation results show that RARGen can effectively reason about RARs, increase confidence and cost-effectiveness of risk assurance, and support difficult-to-perform activities such as assuring complete-risk identification.
24.url:http://doi.acm.org/10.1145/1858996.1859027
24.opinion:exclude

25.title:JCCD: a flexible and extensible API for implementing custom code clone detectors
25.abstract:Code clone detection is an enabling technology for plenty of applications, each having different requirements for a code clone detector. In the tool demonstration we present JCCD, a code clone detection API, which is based on a pipeline model. By combining and parameterizing predefined API components as well as by adding new components, the pipeline model does not only facilitate to build new custom code clone detectors, but also to parallelize the detection process.
25.url:http://doi.acm.org/10.1145/1858996.1859029
25.opinion:exclude

26.title:PeerUnit: a framework for testing peer-to-peer systems
26.abstract:Testing distributed systems is challenging. Peer-to-peer (P2P) systems are composed of a high number of concurrent nodes distributed across the network. The nodes are also highly volatile (i.e., free to join and leave the system at any time). In this kind of system, a great deal of control should be carried out by the test harness, including: volatility of nodes, test case deployment and coordination. In this demonstration we present the PeerUnit framework for testing P2P systems. The original characteristic of this framework is the individual control of nodes, allowing test cases to precisely control their volatility during execution. We validated this framework through implementation and experimentation on two popular open-source P2P systems.
26.url:http://doi.acm.org/10.1145/1858996.1859030
26.opinion:exclude

27.title:Reconfigurable run-time support for distributed service component architectures
27.abstract:SCA (Service Component Architecture) is an OASIS standard for describing service-oriented middleware architectures. In particular, SCA promotes a disciplined way for designing distributed architectures based on a component model and an Architecture Description Language (ADL). However, SCA does not cover the deployment and the run-time management of SCA applications. In this paper, we therefore describe the FraSCAti platform, which provides run-time support, deployment capabilities, and run-time management for SCA. Compared to state-of-the-art platforms, FraSCAti brings a dynamic reflective support to SCA and enables both introspecting and reconfiguring service-oriented architectures at run-time. To achieve this capability, the components are completed by a dedicated container, which is automatically generated by the platform. Furthermore, FraSCAti is a highly configurable platform that can be easily customized by finely selecting the features and functionalities which need to be included. In this way, the platform can be adapted to different application needs and middleware environments.
27.url:http://doi.acm.org/10.1145/1858996.1859031
27.opinion:exclude

28.title:MoDisco: a generic and extensible framework for model driven reverse engineering
28.abstract:Nowadays, almost all companies, independently of their size and type of activity, are facing the problematic of having to manage, maintain or even replace their legacy systems. Many times, the first problem they need to solve is to really understand what are the functionalities, architecture, data, etc of all these often huge legacy applications. As a consequence, reverse engineering still remains a major challenge for software engineering today. This paper introduces MoDisco, a generic and extensible open source reverse engineering solution. MoDisco intensively uses MDE principles and techniques to improve existing approaches for reverse engineering.
28.url:http://doi.acm.org/10.1145/1858996.1859032
28.opinion:exclude

29.title:Tool support for continuous maintenance of state machine models in program code
29.abstract:Software under development is considered by developers at different levels of abstraction, often with formal model specifications that describe actual program code. However, there are semantic barriers between high-level specifications and the resulting programs. In this contribution we introduce a set of tools that maintain multiple abstraction levels in appropriate program code patterns throughout the development process, including run time and monitoring. This makes the program code the only notation necessary for expressing different abstraction levels and improves maintenance of high-level specifications, synchronization of different specifications, and design recovery.
29.url:http://doi.acm.org/10.1145/1858996.1859033
29.opinion:exclude

30.title:Reac2o: a runtime for enterprise system models
30.abstract:Information technology is playing a more and more critical role in many large organizations and enterprise software systems have become increasingly integrated to better support the goals of these organizations. As a consequence, it is a major engineering challenge to assure quality of a software system that is to be deployed in an enterprise environment containing many thousands of interconnected systems. To address this challenge, we propose the use of a scalable emulation environment enabling real-time system testing in large-scale settings. In this work, we discuss the main concepts of our approach, present Reac2o, a prototype implementation of such an emulation environment, and illustrate its applicability in the context of identity management.
30.url:http://doi.acm.org/10.1145/1858996.1859034
30.opinion:exclude

31.title:Symbolic PathFinder: symbolic execution of Java bytecode
31.abstract:Symbolic Pathfinder (SPF) combines symbolic execution with model checking and constraint solving for automated test case generation and error detection in Java programs with unspecified inputs. In this tool, programs are executed on symbolic inputs representing multiple concrete inputs. Values of variables are represented as constraints generated from the analysis of Java bytecode. The constraints are solved using off-the shelf solvers to generate test inputs guaranteed to achieve complex coverage criteria. SPF has been used successfully at NASA, in academia, and in industry.
31.url:http://doi.acm.org/10.1145/1858996.1859035
31.opinion:exclude

32.title:Enumeration refactoring: a tool for automatically converting Java constants to enumerated types
32.abstract:Java 5 introduces several new features that offer significant improvements over older Java technology. We consider the new enum construct, which provides language support for enumerated types. Before Java 5, programmers needed to employ various patterns to compensate for the absence of enumerated types in Java. Unfortunately, these compensation patterns lack several highly desirable properties of the enum construct, most notably, type-safety. We demonstrate an automated refactoring tool called Convert Constants to Enum for transforming legacy Java code to use the new enumeration construct. An interprocedural type inferencing algorithm that tracks the flow of enumerated values drives the approach, and the tool is implemented as a seamless extension to existing refactoring support in Eclipse. The resulting code is semantically equivalent to the original, increasingly type-safe, easier to comprehend, less complex, and supports separate compilation.
32.url:http://doi.acm.org/10.1145/1858996.1859036
32.opinion:exclude

33.title:Keynote address: the actual implementation will be derived from the formal specification -- KBSA, 1983
33.abstract:The 1983 KBSA (Knowledge-Based Software Assistant) report led to the founding of the KBSE (Knowledge-Based Software Engineering conference series. The KBSE conference then changed into the ASE conference in 1997. And this year marks the 25th anniversary of the conference series. As an author of the KBSA report I was invited to talk here about the report and the beginnings of the conference. I will also discuss my own early research in program synthesis and logic programming, leading up to the KBSA report. We will visit some of the goals/milestones in the KBSA report. For example, the report calls for a formal requirements language, executable specifications, and a transformation language. The report also calls for a future development process in which "The transformation from requirements to specifications to implementations will be carried out with automated, knowledge-based assistance"; i.e., "the actual implementation will be derived from the formal specification", and "the [software] development and the proof of correctness will be co-derived". Then I will describe recent progress at Kestrel Institute, including a higher-order logic specification of a small operating system.
33.url:http://doi.acm.org/10.1145/1858996.1858998
33.opinion:exclude

34.title:Verification-driven slicing of UML/OCL models
34.abstract:Model defects are a significant concern in the Model-Driven Development (MDD) paradigm, as model transformations and code generation may propagate errors to other notations where they are harder to detect and trace. Formal verification techniques can check the correctness of a model, but their high computational complexity can limit their scalability. In this paper, we consider a specific static model (UML class diagrams annotated with unrestricted OCL constraints) and a specific property to verify (satisfiability, i.e., "is it possible to create objects without violating any constraint?"). Current approaches to this problem have an exponential worst-case runtime. We propose a technique to improve their scalability by partitioning the original model into submodels (slices) which can be verified independently and where irrelevant information has been abstracted. The definition of the slicing procedure ensures that the property under verification is preserved after partitioning.
34.url:http://doi.acm.org/10.1145/1858996.1859038
34.opinion:exclude

35.title:Automated support for repairing input-model faults
35.abstract:Model transforms are a class of applications that convert a model to another model or text. The inputs to such transforms are often large and complex; therefore, faults in the models that cause a transformation to generate incorrect output can be difficult to identify and fix. In previous work, we presented an approach that uses dynamic tainting to help locate input-model faults. In this paper, we present techniques to assist with repairing input-model faults. Our approach collects runtime information for the failing transformation, and computes repair actions that are targeted toward fixing the immediate cause of the failure. In many cases, these repair actions result in the generation of the correct output. In other cases, the initial fix can be incomplete, with the input model requiring further repairs. To address this, we present a pattern-analysis technique that identifies correct output fragments that are similar to the incorrect fragment and, based on the taint information associated with such fragments, computes additional repair actions. We present the results of empirical studies, conducted using real model transforms, which illustrate the applicability and effectiveness of our approach for repairing different types of faults.
35.url:http://doi.acm.org/10.1145/1858996.1859039
35.opinion:exclude

36.title:Security-driven model-based dynamic adaptation
36.abstract:Security is a key-challenge for software engineering, especially when considering access control and software evolutions. No satisfying solution exists for maintaining the alignment of access control policies with the business logic. Current implementations of access control rely on the separation between the policy and the application code. In practice, this separation is not so strict and some rules are hard-coded within the application, making the evolution of the policy difficult. We propose a new methodology for implementing security-driven applications. From a policy defined by a security expert, we generate an architectural model, reflecting the access control policy. We leverage the advances in the models@runtime domain to keep this model synchronized with the running system. When the policy is updated, the architectural model is updated, which in turn reconfigures the running system. As a proof of concept, we apply the approach to the development of a library management system.
36.url:http://doi.acm.org/10.1145/1858996.1859040
36.opinion:exclude

37.title:JRF-E: using model checking to give advice on eliminating memory model-related bugs
37.abstract:According to Java's relaxed memory model, programs that contain data races need not be sequentially consistent. Executions that are not sequentially consistent may exhibit surprising behavior such as operations on a thread occurring in a different order than indicated by the source code or different threads having inconsistent views of updates of shared variables. Java Racefinder (JRF) is an extension of Java Pathfinder (JPF), a model checker for Java bytecode. JRF precisely detects data races as defined by the memory model and can thus be used to verify sequential consistency. We describe an extension to JRF, JRF-Eliminator (JRF-E), that analyzes information collected during model checking, specifically counterexample traces and acquiring histories, and provides advice to the programmer on how to eliminate detected data races from a program. If data races have been eliminated, standard model checking and other verification techniques that implicitly assume sequential consistency can be soundly employed to verify additional properties.
37.url:http://doi.acm.org/10.1145/1858996.1859042
37.opinion:exclude

38.title:A bounded statistical approach for model checking of unbounded until properties
38.abstract:We study the problem of statistical model checking of probabilistic systems for PCTL unbounded until property PJoinp(Æ1UÆ2) (where Join |X| {<, d, >, e}) using the computation of P d 0(Æ1UÆ2). The approach is first proposed by Sen et al. in CAV'05 but their approach suffers from two drawbacks. Firstly, the computation of Pd0Æ1UÆ2) requires for its validity, a user-specified input parameter ´2 which the user is unlikely to correctly provide. Secondly, the validity of computation of Pd0Æ1UÆ2) is limited only to probabilistic models that do not contain loops. We present a new technique which addresses both problems described above. Essentially our technique transforms the hypothesis test for the unbounded until property in the original model into a new equivalent hypothesis test for bounded until property in our modified model. We empirically show the effectiveness of our technique and compare our results with those using the method proposed by Sen et al.
38.url:http://doi.acm.org/10.1145/1858996.1859043
38.opinion:exclude

39.title:Eliminating navigation errors in web applications via model checking and runtime enforcement of navigation state machines
39.abstract:The enforcement of navigation constraints in web applications is challenging and error prone due to the unrestricted use of navigation functions in web browsers. This often leads to navigation errors, producing cryptic messages and exposing information that can be exploited by malicious users. We propose a runtime enforcement mechanism that restricts the control flow of a web application to a state machine model specified by the developer, and use model checking to verify temporal properties on these state machines. Our experiments, performed on three real-world applications, show that 1) our runtime enforcement mechanism incurs negligible overhead under normal circumstances, and can even reduce server processing time in handling unexpected requests; 2) by combining runtime enforcement with model checking, navigation correctness can be efficiently guaranteed in large web applications.
39.url:http://doi.acm.org/10.1145/1858996.1859044
39.opinion:exclude

40.title:Towards mining replacement queries for hard-to-retrieve traces
40.abstract:Automated trace retrieval methods can significantly reduce the cost and effort needed to create and maintain requirements traces. However, the set of generated traces is generally quite imprecise and must be manually evaluated by analysts. In applied settings when the retrieval algorithm is unable to find the relevant links for a given query, a human user can improve the trace results by manually adding additional search terms and filtering out unhelpful ones. However, the effectiveness of this approach is largely dependent upon the knowledge of the user. In this paper we present an automated technique for replacing the original query with a new set of query terms. These query terms are learned through seeding a web-based search with the original query and then processing the results to identify a set of domain-specific terms. The query-mining algorithm was evaluated and fine-tuned using security regulations from the USA government's Health Insurance Privacy and Portability Act (HIPAA) traced against ten healthcare related requirements specifications.
40.url:http://doi.acm.org/10.1145/1858996.1859046
40.opinion:exclude

41.title:Tool support for essential use cases to better capture software requirements
41.abstract:Capturing software requirements from clients often leads to error prone and vague requirements documents. To surmount this issue, requirements engineers often choose to use UML models to capture their requirements. In this paper we discuss the use of Essential Use Cases (EUCs) as an alternative, user-centric representation which was developed to ease the process of capturing and describing requirements. However, EUCs are not commonly used in practice because, to our knowledge, no suitable tool support has been developed. In addition, requirements engineers face difficulties in finding the correct "essential" requirements (abstract interactions) in a time efficient manner. In order to overcome these problems, we have developed a prototype tool for automated tracing of abstract interactions. We describe the tool and compare the performance and correctness of the results provided by it to that of manual essential use case extraction efforts by a group of requirements engineers. The results of an end user study of the tool's usefulness and ease of use are also discussed.
41.url:http://doi.acm.org/10.1145/1858996.1859047
41.opinion:exclude

42.title:Timesheet assistant: mining and reporting developer effort
42.abstract:Timesheets are an important instrument used to track time spent by team members in a software project on the tasks assigned to them. In a typical project, developers fill timesheets manually on a periodic basis. This is often tedious, time consuming and error prone. Over or under reporting of time spent on tasks causes errors in billing development costs to customers and wrong estimation baselines for future work, which can have serious business consequences. In order to assist developers in filling their timesheets accurately, we present a tool called Timesheet Assistant (TA) that non-intrusively mines developer activities and uses statistical analysis on historical data to estimate the actual effort the developer may have spent on individual assigned tasks. TA further helps the developer or project manager by presenting the details of the activities along with effort data so that the effort may be seen in the context of the actual work performed. We report on an empirical study of TA in a software maintenance project at IBM that provides preliminary validation of its feasibility and usefulness. Some of the limitations of the TA approach and possible ways to address those are also discussed.
42.url:http://doi.acm.org/10.1145/1858996.1859049
42.opinion:exclude

43.title:An experience report on scaling tools for mining software repositories using MapReduce
43.abstract:The need for automated software engineering tools and techniques continues to grow as the size and complexity of studied systems and analysis techniques increase. Software engineering researchers often scale their analysis techniques using specialized one-off solutions, expensive infrastructures, or heuristic techniques (e.g., search-based approaches). However, such efforts are not reusable and are often costly to maintain. The need for scalable analysis is very prominent in the Mining Software Repositories (MSR) field, which specializes in the automated recovery and analysis of large data stored in software repositories. In this paper, we explore the scaling of automated software engineering analysis techniques by reusing scalable analysis platforms from the web field. We use three representative case studies from the MSR field to analyze the potential of the MapReduce platform to scale MSR tools with minimal effort. We document our experience such that other researchers could benefit from them. We find that many of the web field's guidelines for using the MapReduce platform need to be modified to better fit the characteristics of software engineering problems.
43.url:http://doi.acm.org/10.1145/1858996.1859050
43.opinion:exclude

44.title:iMashup: assisting end-user programming for the service-oriented web
44.abstract:The Web is currently moving towards a platform with rich services. A notable trend is that end-users create mashups composing services with short, iterative development life cycles as well as updating with evolving needs. However, the large number of services and the high complexity of composition constraints make manual composition extremely difficult. Addressing this issue, we have developed an approach to assisting the end-users to build mashups in a simple and fast fashion. A tag-based model provides end-users a quick and intuitive insight of services. End-users simply describe their desired goals with tags. Interacting with a service repository, our approach employs a planning approach to suggest services that end-users might want to involve in the final outputs, including some additional interesting or relevant ones to induce more potential composition opportunities. End-users are allowed to iteratively modify, adjust or refine their goals. We have implemented our approach with a tool called iMashup.
44.url:http://doi.acm.org/10.1145/1858996.1859052
44.opinion:exclude

45.title:MODA: automated test generation for database applications via mock objects
45.abstract:Software testing has been commonly used in assuring the quality of database applications. It is often prohibitively expensive to manually write quality tests for complex database applications. Automated test generation techniques, such as Dynamic Symbolic Execution (DSE), have been proposed to reduce human efforts in testing database applications. However, such techniques have two major limitations: (1) they assume that the database that the application under test interacts with is accessible, which may not always be true; and (2) they usually cannot create necessary database states as a part of the generated tests. To address the preceding limitations, we propose an approach that applies DSE to generate tests for a database application. Instead of using the actual database that the application interacts with, our approach produces and uses a mock database in test generation. A mock database mimics the behavior of an actual database by performing identical database operations on itself. We conducted two empirical evaluations on both a medical device and an open source software system to demonstrate that our approach can generate, without producing false warnings, tests with higher code coverage than conventional DSE-based techniques.
45.url:http://doi.acm.org/10.1145/1858996.1859053
45.opinion:exclude

46.title:Random unit-test generation with MUT-aware sequence recommendation
46.abstract:A key component of automated object-oriented unit-test generation is to find method-call sequences that generate desired inputs of a method under test (MUT). Previous work cannot find desired sequences effectively due to the large search space of possible sequences. To address this issue, we present a MUT-aware sequence recommendation approach called RecGen to improve the effectiveness of random object-oriented unit-test generation. Unlike existing random testing approaches that select sequences without considering how a MUT may use inputs generated from sequences, RecGen analyzes object fields accessed by a MUT and recommends a short sequence that mutates these fields. In addition, for MUTs whose test generation keeps failing, RecGen recommends a set of sequences to cover all the methods that mutate object fields accessed by the MUT. This technique further improves the chance of generating desired inputs. We have implemented RecGen and evaluated it on three libraries. Evaluation results show that RecGen improves code coverage over previous random testing tools.
46.url:http://doi.acm.org/10.1145/1858996.1859054
46.opinion:exclude

47.title:End-user oriented critic specification for domain-specific visual language tools
47.abstract:This paper presents a new approach to specifying critics for domain-specific visual language tools using a visual and template-based approach. In this paper we describe our approach for specifying critics for domain-specific visual language tools. This allows target end-user tool developers to design and implement critics efficiently in a natural manner. We describe a survey that we conducted to evaluate our new approach and the Cognitive Dimensions approach that was applied in the survey questionnaire design. Survey results are briefly discussed along with the issues raised by some respondents to improve our approach.
47.url:http://doi.acm.org/10.1145/1858996.1859055
47.opinion:exclude

48.title:Checking roundoff errors using counterexample-guided narrowing
48.abstract:This paper proposes a counterexample-guided narrowing approach, which mutually refines analyses and testing if (possibly spurious) counterexamples are found. A prototype tool CANAT for checking roundoff errors between floating point and fixed point numbers is reported with preliminary experiments.
48.url:http://doi.acm.org/10.1145/1858996.1859056
48.opinion:exclude

49.title:Realizing architecture frameworks through megamodelling techniques
49.abstract:Most practising software architects operate within an architecture framework which is a coordinated set of viewpoints, models and notations prescribed for them. Whereas architecture frameworks are defined to varying degrees of rigour and offer varying levels of tool support, they tend to be closed: constituent elements are defined in different non-standard ways, they are not re-usable, and the creation of other frameworks requires a complete rework. With the aim to manage this issue, this paper presents MEGAF, an infrastructure for realizing architecture frameworks, which can be used to create architecture descriptions. It builds upon the conceptual foundations of ISO/IEC 42010 for architecture description. MEGAF is realized through megamodeling techniques and is implemented via Eclipse plugins.
49.url:http://doi.acm.org/10.1145/1858996.1859057
49.opinion:exclude

50.title:Recovering inter-project dependencies in software ecosystems
50.abstract:In large software systems, knowing the dependencies between modules or components is critical to assess the impact of changes. To recover the dependencies, fact extractors analyze the system as a whole and build the dependency graph, parsing the system down to the statement level. At the level of software ecosystems, which are collections of software projects, the dependencies that need to be recovered reside not only within the individual systems, but also between the libraries, frameworks, and entire software systems that make up the complete ecosystem; scaling issues arise. In this paper we present and evaluate several variants of a lightweight and scalable approach to recover dependencies between the software projects of an ecosystem. We evaluate our recovery algorithms on the Squeak 3.10 Universe, an ecosystem containing more than 200 software projects.
50.url:http://doi.acm.org/10.1145/1858996.1859058
50.opinion:exclude

51.title:Automated program repair through the evolution of assembly code
51.abstract:A method is described for automatically repairing legacy software at the assembly code level using evolutionary computation. The technique is demonstrated on Java byte code and x86 assembly programs, showing how to find program variations that correct defects while retaining desired behavior. Test cases are used to demonstrate the defect and define required functionality. The paper explores advantages of assembly-level repair over earlier work at the source code level - the ability to repair programs written in many different languages; and the ability to repair bugs that were previously intractable. The paper reports experimental results showing reasonable performance of assembly language repair even on non-trivial programs
51.url:http://doi.acm.org/10.1145/1858996.1859059
51.opinion:exclude

52.title:Integrating model verification and self-adaptation
52.abstract:In software development, formal verification plays an important role in improving the quality and safety of products and processes. Model checking is a successful approach to verification, used both in academic research and industrial applications. One important improvement regarding utilization of model checking is the development of automated processes to evolve models according to information obtained from verification. In this paper, we propose a new framework that make use of artificial intelligence and machine learning to generate and evolve models from partial descriptions and examples created by the model checking process. This was implemented as a tool that is integrated with a model checker. Our work extends model checking to be applicable when initial description of a system is not available, through observation of actual behaviour of this system. The framework is capable of integrated verification and evolution of abstract models, but also of reengineering partial models of a system.
52.url:http://doi.acm.org/10.1145/1858996.1859060
52.opinion:exclude

53.title:When to use data from other projects for effort estimation
53.abstract:Collecting the data required for quality prediction within a development team is time-consuming and expensive. An alternative to make predictions using data that crosses from other projects or even other companies. We show that with/without relevancy filtering, imported data performs the same/worse (respectively) than using local data. Therefore, we recommend the use of relevancy filtering whenever generating estimates using data from another project.
53.url:http://doi.acm.org/10.1145/1858996.1859061
53.opinion:exclude

54.title:Kadre: domain-specific architectural recovery for scientific software systems
54.abstract:Scientists today conduct new research via software-based experimentation and validation in a host of disciplines. Scientific software represents a significant investment due to its complexity and longevity yet there is little reuse of scientific software beyond small libraries which increases development and maintenance costs. To alleviate this disconnect, we have developed KADRE, a domain-specific architecture recovery approach and toolset to aid automatic and accurate identification of workflow components in existing scientific software. KADRE improves upon state of the art general cluster techniques, helping to promote component-based reuse within the domain.
54.url:http://doi.acm.org/10.1145/1858996.1859062
54.opinion:exclude

55.title:Automated SQL query generation for systematic testing of database engines
55.abstract:We present a novel approach for generating syntactically and semantically correct SQL queries as inputs for testing relational databases. We leverage the SAT-based Alloy tool-set to reduce the problem of generating valid SQL queries into a SAT problem. Our approach translates SQL query constraints into Alloy models, which enable it to generate valid queries that cannot be automatically generated using conventional grammar-based generators. Given a database schema, our new approach combined with our previous work on ADUSA, automatically generates (1) syntactically and semantically valid SQL queries for testing, (2) input data to populate test databases, and (3) expected result of executing the given query on the generated data. Experimental results show that not only can we automatically generate valid queries which detect bugs in database engines, but also we are able to combine this work with our previous work on ADUSA to automatically generate input queries and tables as well as expected query execution outputs to enable automated testing of database engines.
55.url:http://doi.acm.org/10.1145/1858996.1859063
55.opinion:exclude

56.title:Tag and prune: a pragmatic approach to software product line implementation
56.abstract:To realise variability at the code level, product line methods classically advocate usage of inheritance, components, frameworks, aspects or generative techniques. However, these might require unaffordable paradigm shifts for the developers if the software was not thought at the outset as a product line. Furthermore, these techniques can be conflicting with a company's coding practices or external regulations. These concerns were the motivation for the industry-university collaboration described in this paper where we develop a minimally intrusive coding technique based on tags. It is supported by a toolchain and is now in use in the partner company for the development of flight grade satellite communication software libraries.
56.url:http://doi.acm.org/10.1145/1858996.1859064
56.opinion:exclude

57.title:Reducing estimation uncertainty with continuous assessment: tracking the "cone of uncertainty"
57.abstract:Accurate software cost and schedule estimations are essential especially for large software projects. However, once the required efforts have been estimated, little is done to recalibrate and reduce the uncertainty of the initial estimates. To address this problem, we have developed and used a framework to continuously monitor the software project progress and readjust the estimated effort utilizing the Constructive Cost Model II (COCOMO II) and the Unified CodeCount Tool developed by the University of Southern California (USC). As a software project progresses, we gain more information about the project itself, which can then be used to assess and re-estimate the effort required to complete the project. With more accurate estimations and less uncertainties, the quality and goal of project outcome can be assured within the available resources. The paper thus also provides and analyzes empirical data on how projects evolve within the familiar software "cone of uncertainty".
57.url:http://doi.acm.org/10.1145/1858996.1859065
57.opinion:exclude

58.title:Using transitive changesets to support feature location
58.abstract:In this paper, we present a new construct, called Transitive Changeset, that can be used for feature location. Transitive Changesets are created by extending changesets from revision control systems with additional information. A changeset temporally associate changes and conceptual descriptions provided in a commit transaction. By following transitive relationships from these changesets, we can create a Transitive Changeset that relates concepts in the problem domain to a list of program elements that enclose changes made in the transaction and other relevant program elements. We have implemented a prototype Eclipse plug-in, Kayley, to create Transitive Changesets.
58.url:http://doi.acm.org/10.1145/1858996.1859066
58.opinion:exclude

59.title:RuMoR: monitoring and recovery for BPEL applications
59.abstract:We describe a RUntime MOnitoring and Recovery framework (RuMoR) for BPEL applications. Our tool checks for behavioral conformance with respect to a set of user-specified properties. When runtime violations are discovered, RuMoR automatically proposes and ranks recovery plans which users can then select for execution. These plans are generated using an adaptation of a SAT-based planning technique.
59.url:http://doi.acm.org/10.1145/1858996.1859068
59.opinion:exclude

60.title:Model/analyzer: a tool for detecting, visualizing and fixing design errors in UML
60.abstract:Integrated development environments are widely used in industry and support software engineers with instant error feedback about their work. Modeling tools often react to changes at a coarse level of granularity that make reasoning about errors inefficient and late. Furthermore, there is often a lack of appropriate visualizations of model errors and information on how to fix them. This paper presents the Model/Analyzer tool, an eclipse-based plug-in for the IBM Rational Software Modeler (RSM). The tool lets software engineers define arbitrary design rules and provides instant feedback on their validity in context of a model. Design errors are then visualized together with the information on what parts of the model contributed to them and how to fix them. The tool is fully automated and currently supports OCL and Java as languages for defining the design rules; and UML as the modeling language. The main benefit for the software engineer is the tool's incremental nature if providing instant feedback for many kinds of design errors even for large models.
60.url:http://doi.acm.org/10.1145/1858996.1859069
60.opinion:exclude

61.title:CoGenTe: a tool for code generator testing
61.abstract:We present the CoGenTe tool for automated black-box testing of code generators. A code generator is a program that takes a model in a high-level modeling language as input, and outputs a program that captures the behaviour of the model. Thus, a code generator's input and output are complex objects having not just syntactic structure but execution semantics, too. Hence, traditional test generation methods that take only syntax into account are not effective in testing code generators. CoGenTe amends this by incorporating various coverage criteria over semantics. This enables it to generate test-cases with a higher potential of revealing subtle semantic errors in code generators. CoGenTe has uncovered such issues in widely used real-life code generators: (i) lexical analyzer generators Flex and JFlex, and (ii) The MathWorks' simulator/code generator for Stateflow.
61.url:http://doi.acm.org/10.1145/1858996.1859070
61.opinion:exclude

62.title:Impendulo: debugging the programmer
62.abstract:We describe the Impendulo tool for fine-grained analyses of programmer behavior. The initial design goal was to create a system to answer the following simple question: "What kind of mistakes do programmers make and how often do they make these mistakes?" However it quickly became apparent that the tool can be used to also analyze other fundamental software engineering questions, such as, how good are static analysis tools at finding real errors?, what is the fault finding capability of automated test generation tools?, what is the influence of a bad specification?, etc. We briefly describe the tool and some of the insights gained from using it.
62.url:http://doi.acm.org/10.1145/1858996.1859071
62.opinion:exclude

63.title:SpecDiff: debugging formal specifications
63.abstract:This paper presents our SpecDiff tool that exploits the model differencing technique for debugging and understanding evolving behaviors of formal specifications. SpecDiff has been integrated in the Process Analysis Toolkit (PAT), a framework for formal specification, verification and simulation. SpecDiff is able to assist in diagnosing system faults, understanding the impacts of specification optimization techniques, and revealing the system change patterns.
63.url:http://doi.acm.org/10.1145/1858996.1859072
63.opinion:exclude

64.title:Deriving behavior of multi-user processes from interactive requirements validation
64.abstract:In this tool demonstration we present an implementation for interactively validating requirements for multi-user software systems and the processes they support with end users. The tool combines the advantages of requirements animation and scenario synthesis to gather stakeholder feedback and create a common understanding amongst stakeholders. Additionally, the users' behavior during the simulation is captured and used to automatically derive new behavioral specifications.
64.url:http://doi.acm.org/10.1145/1858996.1859073
64.opinion:exclude

65.title:Tool support for code generation from a UMLsec property
65.abstract:This demo presents a tool to generate code from verified Role-Based Access Control properties defined using UMLsec. It can either generate Java code, or generate Java code for the UML model and AspectJ code for enforcing said RBA properties. Both approaches use the Java Authentication and Authorization Service (JAAS) to enforce access control.
65.url:http://doi.acm.org/10.1145/1858996.1859074
65.opinion:exclude

66.title:PlayGo: towards a comprehensive tool for scenario based programming
66.abstract:We present PlayGo, a comprehensive tool for scenario-based programming, built around the language of live sequence charts and the play-in/play-out approach [7], which includes a compiler into AspectJ code and means for debugging the execution. PlayGo is intended to be a full IDE that addresses major parts of the vision of Liberating Programming [3]. This paper presents the first version of PlayGo, which already includes several of the intended capabilities.
66.url:http://doi.acm.org/10.1145/1858996.1859075
66.opinion:exclude

67.title:REMES tool-chain: a set of integrated tools for behavioral modeling and analysis of embedded systems
67.abstract:In this paper, we present a tool-chain for the REMES language, which can be used for the construction and analysis of embedded system behavioral models. The tool-chain consists of the following tools: (i) a REMES editor for modeling behaviors of embedded components, (ii) a REMES simulator to test timing and resource behavior prior to formal analysis, and (iii) an automated transformation from REMES to Priced Timed Automata, needed for formal analysis.
67.url:http://doi.acm.org/10.1145/1858996.1859076
67.opinion:exclude

68.title:C2O: a tool for guided decision-making
68.abstract:Decision models are widely used in software engineering to describe and restrict decision-making (e.g., deriving a product from a product-line). Since decisions are typically interdependent, conflicts during decision-making are inevitably reached when invalid combinations of decisions are made. Unfortunately, the current state-of-the-art provides little support for dealing with such conflicts. On the one hand, some conflicts can be avoided by providing more freedom in which order decisions are made (i.e., most important decisions first). On the other hand, conflicts are unavoidable at times and living with conflicts may be preferable over forcing the user to fix them right away - particularly, because fixing conflicts becomes easier the more is known about an user's intentions. This paper introduces the C2O (Configurator 2.0) tool for guided decision-making. The tool allows the user to answer questions in an arbitrary order - with and without the presence of conflicts. While giving users those freedoms, it still supports and guides them by 1) rearranging the order of questions according to their potential to minimize user input, 2) providing guidance to avoid follow-on conflicts, and 3) supporting users in fixing conflicts at a later time.
68.url:http://doi.acm.org/10.1145/1858996.1859077
68.opinion:exclude

69.title:Keynote address: model engineering for model-driven engineering
69.abstract:The effectiveness of MDE relies on our ability to build high-quality models. This task is intrinsically difficult. We need to produce sufficiently complete, adequate, consistent, and well-structured models from incomplete, imprecise, and sparse material originating from multiple, often conflicting sources. The system we need to consider in the early stages comprises software and environment components including people and devices. Such models should integrate the intentional, structural, functional, and behavioral facets of the system being developed. Rigorous techniques are needed for model construction, analysis, and evolution. They should support early and incremental reasoning about partial models for a variety of purposes, including satisfaction arguments, property checks, animations, the evaluation of alternative options, the analysis of risks, threats and conflicts, and traceability management. The tension between technical precision and practical applicability calls for a suitable mix of heuristic, deductive, and inductive forms of reasoning on a suitable mix of declarative and operational models. Formal techniques should be deployed only when and where needed, and kept hidden wherever possible. The talk will provide a retrospective account of our research efforts and practical experience along this route, including recent progress in model engineering for safety-critical medical workfows. Problem-oriented abstractions, analyzable models, and constructive techniques are pervasive concerns.
69.url:http://doi.acm.org/10.1145/1858996.1858999
69.opinion:exclude

70.title:Search-carrying code
70.abstract:In this paper, we introduce a model-checking-based certification technique called search-carrying code (SCC). SCC is an adaptation of the principles of proof-carrying code, in which program certification is reduced to checking a provided safety proof. In SCC, program certification is an efficient re-examination of a program's state space. A code producer, who offers a program for use, provides a search script that encodes a search of the program's state space. A code consumer, who wants to certify that the program fits her needs, uses the search script to direct how a model checker searches the program's state space. Basic SCC achieves slight reductions in certification time, but it can be optimized in two important ways. (1) When a program comes from a trusted source, SCC certification can forgo authenticating the provided search script and instead optimize for speed of certification. (2) The search script can be partitioned into multiple partial certification tasks of roughly equal size, which can be performed in parallel. Using parallel model checking, we reduce the certification times by a factor of up to n, for n processors. When certifying a program from a trusted source, we reduce the certification times by a factor of up to 5n, for n processors.
70.url:http://doi.acm.org/10.1145/1858996.1859079
70.opinion:exclude

71.title:Solving string constraints lazily
71.abstract:Decision procedures have long been a fixture in program analysis, and reasoning about string constraints is a key element in many program analyses and testing frameworks. Recent work on string analysis has focused on providing decision procedures that model string operations. Separating string analysis from its client applications has important and familiar benefits: it enables the independent improvement of string analysis tools and it saves client effort. We present a constraint solving algorithm for equations over string variables. We focus on scalability with regard to the size of the input constraints. Our algorithm performs an explicit search for a satisfying assignment; the search space is constructed lazily based on an automata representation of the constraints. We evaluate our approach by comparing its performance with that of existing string decision procedures. Our prototype is, on average, several orders of magnitude faster than the fastest existing implementation
71.url:http://doi.acm.org/10.1145/1858996.1859080
71.opinion:exclude

72.title:Scenario-based and value-based specification mining: better together
72.abstract:Specification mining takes execution traces as input and extracts likely program invariants, which can be used for comprehension, verification, and evolution related tasks. In this work we integrate scenario-based specification mining, which uses data-mining algorithms to suggest ordering constraints in the form of live sequence charts, an inter-object, visual, modal, scenario-based specification language, with mining of value-based invariants, which detects likely invariants holding at specific program points. The key to the integration is a technique we call scenario-based slicing, running on top of the mining algorithms to distinguish the scenario-specific invariants from the general ones. The resulting suggested specifications are rich, consisting of modal scenarios annotated with scenario-specific value-based invariants, referring to event parameters and participating object properties. An evaluation of our work over a number of case studies shows promising results in extracting expressive specifications from real programs, which could not be extracted previously. The more expressive the mined specifications, the higher their potential to support program comprehension and testing.
72.url:http://doi.acm.org/10.1145/1858996.1859081
72.opinion:exclude

73.title:Test generation to expose changes in evolving programs
73.abstract:Software constantly undergoes changes throughout its life cycle, and thereby it evolves. As changes are introduced into a code base, we need to make sure that the effect of the changes is thoroughly tested. For this purpose, it is important to generate test cases that can stress the effect of a given change. In this paper, we propose an automatic test generation solution to this problem. Given a change c, we use dynamic symbolic execution to generate a test input t, which stresses the change. This is done by ensuring (i) the change c is executed by t, and (ii) the effect of c is observable in the output produced by the test t. To construct a change-reaching input, our technique uses distance in control-dependency graph to guide path exploration towards the change. Then, our technique identifies the common programming patterns that may prevent a given change from affecting the program's output. For each of these patterns we propose methods to tune the change-reaching input into an input that reaches the change and propagates the effect of the change to the output. Our experimental results show that our test generation technique is effective in generating change-exposing inputs for real-world programs.
73.url:http://doi.acm.org/10.1145/1858996.1859083
73.opinion:exclude

74.title:How did you specify your test suite
74.abstract:Although testing is central to debugging and software certification, there is no adequate language to specify test suites over source code. Such a language should be simple and concise in daily use, feature a precise semantics, and of course, it has to facilitate suitable engines to compute test suites and assess the coverage achieved by a test suite. This paper introduces the language FQL designed to fit these purposes. We achieve the necessary expressive power by a natural extension of regular expressions which matches test suites rather than individual executions. To evaluate the language, we show for a list of informal requirements how to express them in FQL. Moreover, we present a test case generation engine for C programs and perform practical experiments with the sample specifications.
74.url:http://doi.acm.org/10.1145/1858996.1859084
74.opinion:exclude

75.title:Dynamic and transparent analysis of commodity production systems
75.abstract:We propose a framework that provides a programming interface to perform complex dynamic system-level analyses of deployed production systems. By leveraging hardware support for virtualization available nowadays on all commodity machines, our framework is completely transparent to the system under analysis and it guarantees isolation of the analysis tools running on top of it. Thus, the internals of the kernel of the running system needs not to be modified and the whole platform runs unaware of the framework. Moreover, errors in the analysis tools do not affect the running system and the framework. This is accomplished by installing a minimalistic virtual machine monitor and migrating the system, as it runs, into a virtual machine. In order to demonstrate the potentials of our framework we developed an interactive kernel debugger, named HyperDbg. HyperDbg can be used to debug any critical kernel component, and even to single step the execution of exception and interrupt handlers.
75.url:http://doi.acm.org/10.1145/1858996.1859085
75.opinion:exclude

76.title:Apt-pbo: solving the software dependency problem using pseudo-boolean optimization
76.abstract:The installation of software packages depends on the correct resolution of dependencies and conflicts between packages. This problem is NP-complete and, as expected, is a hard task. Moreover, today's technology still does not address this problem in an acceptable way. This paper introduces a new approach to solving the software dependency problem in a Linux environment, devising a way for solving dependencies according to available packages and user preferences. This work introduces the "apt-pbo" tool, the first publicly available tool that solves dependencies in a complete and optimal way.
76.url:http://doi.acm.org/10.1145/1858996.1859087
76.opinion:exclude

77.title:A sentence-matching method for automatic license identification of source code files
77.abstract:The reuse of free and open source software (FOSS) components is becoming more prevalent. One of the major challenges in finding the right component is finding one that has a license that is e for its intended use. The license of a FOSS component is determined by the licenses of its source code files. In this paper, we describe the challenges of identifying the license under which source code is made available, and propose a sentence-based matching algorithm to automatically do it. We demonstrate the feasibility of our approach by implementing a tool named Ninka. We performed an evaluation that shows that Ninka outperforms other methods of license identification in precision and speed. We also performed an empirical study on 0.8 million source code files of Debian that highlight interesting facts about the manner in which licenses are used by FOSS
77.url:http://doi.acm.org/10.1145/1858996.1859088
77.opinion:exclude

78.title:Detection of recurring software vulnerabilities
78.abstract:Software security vulnerabilities are discovered on an almost daily basis and have caused substantial damage. Aiming at supporting early detection and resolution for them, we have conducted an empirical study on thousands of vulnerabilities and found that many of them are recurring due to software reuse. Based on the knowledge gained from the study, we developed SecureSync, an automatic tool to detect recurring software vulnerabilities on the systems that reuse source code or libraries. The core of SecureSync includes two techniques to represent and compute the similarity of vulnerable code across different systems. The evaluation for 60 vulnerabilities on 176 releases of 119 open-source software systems shows that SecureSync is able to detect recurring vulnerabilities with high accuracy and to identify 90 releases having potentially vulnerable code that are not reported or fixed yet, even in mature systems. A couple of cases were actually confirmed by their developers.
78.url:http://doi.acm.org/10.1145/1858996.1859089
78.opinion:exclude

79.title:Matching dependence-related queries in the system dependence graph
79.abstract:In software maintenance and evolution, it is common that developers want to apply a change to a number of similar places. Due to the size and complexity of the code base, it is challenging for developers to locate all the places that need the change. A main challenge in locating the places that need the change is that, these places share certain common dependence conditions but existing code searching techniques can hardly handle dependence relations satisfactorily. In this paper, we propose a technique that enables developers to make queries involving dependence conditions and textual conditions on the system dependence graph of the program. We carried out an empirical evaluation on four searching tasks taken from the development history of two real-world projects. The results of our evaluation indicate that, compared with code-clone detection, our technique is able to locate many required code elements that code-clone detection cannot locate, and compared with text search, our technique is able to effectively reduce false positives without losing any required code elements.
79.url:http://doi.acm.org/10.1145/1858996.1859091
79.opinion:exclude

80.title:PLASMA: a plan-based layered architecture for software model-driven adaptation
80.abstract:Modern software-intensive systems are expected to adapt, often while the system is executing, to changing requirements, failures, and new operational contexts. This paper describes an approach to dynamic system adaptation that utilizes plan-based and architecture-based mechanisms. Our approach utilizes an architecture description language (ADL) and a planning-as-model-checking technology to enable dynamic replanning. The ability to automatically generate adaptation plans based solely on ADL models and an application problem description simplifies the specification and use of adaptation mechanisms for system architects. The approach uses a three-layer architecture that, while similar to previous work, provides several significant improvements. We apply our approach within the context of a mobile robotics case study.
80.url:http://doi.acm.org/10.1145/1858996.1859092
80.opinion:exclude

81.title:A program differencing algorithm for verilog HDL
81.abstract:During code review tasks, comparing two versions of a hardware design description using existing program differencing tools such as diff is inherently limited because existing program differencing tools implicitly assume sequential execution semantics, while hardware description languages are designed to model concurrent computation. We designed a position-independent differencing algorithm to robustly handle language constructs whose relative orderings do not matter. This paper presents Vdiff, an instantiation of this position-independent differencing algorithm for Verilog HDL. To help programmers reason about the differences at a high-level, Vdiff outputs syntactic differences in terms of Verilog-specific change types. We evaluated Vdiff on two open source hardware design projects. The evaluation result shows that Vdiff is very accurate, with overall 96.8% precision and 97.3% recall when using manually classified differences as a basis of comparison.
81.url:http://doi.acm.org/10.1145/1858996.1859093
81.opinion:exclude

82.title:Error-avoiding adaptors for black-box software components
82.abstract:A lot of work has been done in the area of building component-based systems with correct-by-construction adaptors. This is accomplished by using preexisting specifications of the component behaviour. But what happens when known components get to interact with incompletely specified, black-box components, and errors occur? How can we avoid these errors without modifying existing/legacy components? We present a method to explore and control such systems. Our approach exploits information in correct and erroneous runs to build a controller that ensures our system will avoid observed errors. We consider the behavioural specifications for our known, legacy component as already documented and we infer partial behaviour information of the unknown component by studying its reactions to various interaction scenarios.
82.url:http://doi.acm.org/10.1145/1858996.1859095
82.opinion:exclude

83.title:Automated model grouping
83.abstract:A tremendous amount of software models has been created so far. This growing number of models adds to the fact that it gets more and more difficult to organise, structure, and reuse them. Thereby new software development projects cannot profit from existing knowledge. In our research we will study existing natural language processing techniques for their adaptability in the reuse of software models. We will research methods to group existing models according to their functionality.
83.url:http://doi.acm.org/10.1145/1858996.1859096
83.opinion:exclude

84.title:Automatic inference of abstract type behavior
84.abstract:Type hierarchies are an integral part of the object oriented software reuse machinery. Software flexibility can be increased through type inheritance which, if used in accordance with Liskov Substitution Principle (LSP) enables safe object substitution. Assuming that formal specifications are available for a set of subtypes, we present our early doctoral research on the automatic inference of an extended deterministic finite automaton that describes the legal usage of abstract supertypes and ensures the behavioral subtyping relation as defined by the Liskov Substitution Principle (LSP). We obtain the supertype interface automata by incrementally exploring the specification of the subtypes, unifying correlated subtype fields, simplifying predicates through quantification, and finally creating new model fields that we associate with the remaining predicates. The inferred automaton is simulated by the behavior of each subtype and can be used for safe hierarchy extension, verification of new hierarchy clients, and emphasis of LSP non-compliant methods.
84.url:http://doi.acm.org/10.1145/1858996.1859097
84.opinion:exclude

85.title:Extraction and visualization of traceability relationships between documents and source code
85.abstract:Traceability links between artifacts in a software system aid developers in comprehension, development, and effective management of the system. Traceability systems to date have been confronting the difficulties in retrieving relationships between artifacts with high quality and accuracy, and in visualizing extracted relationships in a natural and intuitive way. This research aims to combine several traceability recovery techniques to make up for each other's weaknesses to extract relationships between artifacts at a high-level accuracy and quality. Moreover, the recovered relationships are visualized in a hierarchical rich graphical tree that can be expanded and contracted to help users easily interact with these links and move easily between artifacts and their related artifacts and vice versa. Our preliminary evaluation demonstrated that integration of several traceability recovery techniques can improve the quality and accuracy of retrieved links.
85.url:http://doi.acm.org/10.1145/1858996.1859098
85.opinion:exclude

86.title:Model checking graph representation of precise boolean inter-procedural flow analysis
86.abstract:A new representation for inter-procedural analysis is presented. The representation only uses regular graph theory with guarded edges and variables for easy integration with model checkers but is limited to binary lattice inter-procedural analysis only. A simple inter-procedural problem is presented here with a description of the control flow graph associated. Construction of the graph automaton for model checking is described using graph rewriting rules applied on the control flow graph. Further research and possible extensions are reported.
86.url:http://doi.acm.org/10.1145/1858996.1859099
86.opinion:exclude

87.title:The influence of multiple artifacts on the effectiveness of software testing
87.abstract:The effectiveness of the software testing process is determined by artifacts used in testing, including the program, the set of tests, and the test oracle. However, in evaluating software testing techniques, including automated software testing techniques, the influence of these testing artifacts is often overlooked. In my upcoming dissertation, we intend to explore the interrelationship between these three testing artifacts, with the goal of establishing a solid scientific foundation for understanding how they interact. We plan to provide two contributions towards this goal. First, we propose a theoretical framework for discussing testing based on previous work in the theory of testing. Second, we intend to perform a rigorous empirical study controlling for program structure, test coverage criteria, and oracle selection in the domain of safety critical avionics software.
87.url:http://doi.acm.org/10.1145/1858996.1859100
87.opinion:exclude

