1.title:"Analysis of inconsistency in graph-based viewpoints: a category-theoretical approach"
1.abstract:"Eliciting the requirements for a proposed system typically involves different stakeholders with different expertise, responsibilities, and perspectives. Viewpoints-based approaches have been proposed as a way to manage incomplete and inconsistent models gathered from multiple sources. In this paper, we propose a category-theoretical framework for the analysis of fuzzy viewpoints. Informally, a fuzzy viewpoint is graph in which the elements of a lattice are used to specify the amount of knowledge available about the details of nodes and edges. By defining an appropriate notion of morphism between fuzzy viewpoints, we construct categories of fuzzy viewpoints and prove that these categories are (finitely) complete. We then show how colimits can be employed to merge the viewpoints and detect the inconsistencies that arise independent of any particular choice of viewpoint semantics. We illustrate an application of the framework through a case-study showing how fuzzy viewpoints can serve as a requirements elicitation tool in reactive systems."
1.url:http://dx.doi.org/10.1109/ASE.2003.1240290
1.opinion:exclude

2.title:"Deriving user interface requirements from densely interleaved scientific computing applications"
2.abstract:"Deriving user interface requirements is a key step in user interface generation and maintenance. For single purpose numeric routines, user interface requirements are relatively simple to derive. However, general numeric packages, which are solvers for entire classes of problems, are densely interleaved with strands shared and mixed among user options. This complexity forms a significant barrier to the derivation of user interface requirements and therefore to user interface generation and maintainance. Our methodology uses a graph representation to find potential user decision points implied by the control structure of the code. This graph is then iteratively refined to form a decision point diagram, a state machine representation of all possible user traversals through a user interface for the underlying code."
2.url:http://dx.doi.org/10.1109/ASE.2003.1240291
2.opinion:exclude

3.title:"Fault localization with nearest neighbor queries"
3.abstract:"We present a method for performing fault localization using similar program spectra. Our method assumes the existence of a faulty run and a larger number of correct runs. It then selects according to a distance criterion the correct run that most resembles the faulty run, compares the spectra corresponding to these two runs, and produces a report of \"suspicious\" parts of the program. Our method is widely applicable because it does not require any knowledge of the program input and no more information from the user than a classification of the runs as either \"correct\" or \"faulty\". To experimentally validate the viability of the method, we implemented it in a tool, Whither, using basic block profiling spectra. We experimented with two different similarity measures and the Siemens suite of 132 programs with injected bugs. To measure the success of the tool, we developed a generic method for establishing the quality of a report. The method is based on the way an \"ideal user\" would navigate the program using the report to save effort during debugging. The best results obtained were, on average, above 50%, meaning that our ideal user would avoid looking half of the program."
3.url:http://dx.doi.org/10.1109/ASE.2003.1240292
3.opinion:exclude

4.title:"Tool-assisted unit test selection based on operational violations"
4.abstract:"Unit testing, a common step in software development, presents a challenge. When produced manually, unit test suites are often insufficient to identify defects. The main alternative is to use one of a variety of automatic unit test generation tools: these are able to produce and execute a large number of test inputs that extensively exercise the unit under test. However, without a priori specifications, developers need to manually verify the outputs of these test executions, which is generally impractical. To reduce this cost, unit test selection techniques may be used to help select a subset of automatically generated test inputs. Then developers can verify their outputs, equip them with test oracles, and put them into the existing test suite. In this paper, we present the operational violation approach for unit test selection, a black-box approach without requiring a priori specifications. The approach dynamically generates operational abstractions from executions of the existing unit test suite. Any automatically generated tests violating the operational abstractions are identified as candidates for selection. In addition, these operational abstractions can guide test generation tools to produce better tests. To experiment dynamic approach, we integrated the use of Daikon (a dynamic invariant detection tool) and Jtest (a commercial Java unit testing tool). An experiment is conducted to assess this approach."
4.url:http://dx.doi.org/10.1109/ASE.2003.1240293
4.opinion:exclude

5.title:"A new structural coverage criterion for dynamic detection of program invariants"
5.abstract:"Dynamic detection of program invariants is emerging as an important research area with many challenging problems. Generating suitable test cases that support accurate detection of program invariants is crucial to the dynamic approach of program invariant detection. In this paper, we propose a new structural coverage criterion called invariant-coverage criterion for dynamic detection of program invariants. We also show how the invariant-coverage criterion can be used to improve the accuracy of dynamically detected invariants. We first used the Daikon tool to report likely program invariants using the branch coverage and all definition-use pair coverage test suites for several programs. We then generated invariant-coverage suites for these likely invariants. When Daikon was run with the invariant-coverage suites, several spurious invariants reported earlier by the branch coverage and definition-use pair coverage test suites were removed from the reported invariants. Our approach also produced more meaningful invariants than randomly generated test suites."
5.url:http://dx.doi.org/10.1109/ASE.2003.1240294
5.opinion:exclude

6.title:"DeCo: a declarative coordination framework for scientific model federations"
6.abstract:"Program federation is assembling a software system from cooperating but independent application programs. We present DeCo, a declarative approach to creating and coordinating federations, and discuss its application in the domain of scientific computing. DeCo is meant to automate several aspects of the typically manual process of program federation. A federation is expressed in the functional language Haskell, extended with constructs for describing and coordinating the participating programs and data files. The declarative expression of a federation in terms of data flow among the component programs captures synchronization requirements implicitly and exploits the inherent concurrency automatically. Haskell compilation, notably its rigorous type checking, ensures the consistency of the federation. Operation of the coordination framework is demonstrated on a federation of FORTRAN programs that simulate environmental processes in the Neuse River estuary of North Carolina."
6.url:http://dx.doi.org/10.1109/ASE.2003.1240295
6.opinion:exclude

7.title:"A programmable client-server model: robust extensibility via DSLs"
7.abstract:"The client-server model has been successfully used to support a wide variety of families of services in the context of distributed systems. However, its server-centric nature makes it insensitive to fast changing client characteristics like terminal capabilities, network features, user preferences and evolving needs. To overcome these key limitations, we present an approach to enabling a server to adapt to different clients by making it programmable. A service-description language is used to program server adaptations. This language is designed as a domain-specific language to offer expressiveness and conciseness without compromising safety and security. We show that requiring the deployment of new protocols or server implementations. We illustrate our approach with the Internet Message Access Protocol (IMAP). An IMAP server is made programmable and a language, named Pems, is introduced to program robust variations of e-mail services. Our approach is uniformly used to develop a platform for multimedia communication services. This platform is composed of programmable servers for telephony service, e-mail processing, remote-document processing and stream adapters."
7.url:http://dx.doi.org/10.1109/ASE.2003.1240296
7.opinion:exclude

8.title:"Specification and synthesis of hybrid automata for physics-based animation"
8.abstract:"Physics-based animation programs can often be modeled in terms of hybrid automata. A hybrid automaton includes both discrete and continuous dynamical variables. The discrete variables define the automaton's modes of behavior. The continuous variables are governed by mode-dependent differential equations. This paper describes a system for specifying and automatically synthesizing physics-based animation programs based on hybrid automata. The system presents a program developer with a family of parameterized specification schemata. Each scheme describes a pattern of behavior as a hybrid automaton passes through a sequence of modes. The developer specifies a program by selecting one or more schemata and supplying application-specific instantiation parameters for each of them. Each scheme is associated with a set of axioms in logic of hybrid automata. The axioms serve to document the semantics of the specification scheme. Each scheme is also associated with a set of implementation rules. The rules synthesize program components implementing the specification in general physics-based animation architecture. The system allows animation programs to be developed and tested in an incremental manner. The system itself can be extended to incorporate additional schemata for specifying new patterns o behavior, along with new sets of axioms and implementation rules. It has been implemented and tested on over a dozen examples. We believe this research is a significant step toward a specification and synthesis system that is flexible enough to handle a wide variety of animation programs, yet restricted enough to permit programs to be synthesized automatically."
8.url:http://dx.doi.org/10.1109/ASE.2003.1240297
8.opinion:exclude

9.title:"Debugging overconstrained declarative models using unsatisfiable cores"
9.abstract:"Declarative models, in which conjunction and negation are freely used, are susceptible to unintentional overconstraint. Core extraction is a new analysis that mitigates this problem in the context of a checker based on reduction to SAT (systems analysis tools). It exploits a recently developed facility of SAT solvers that provides an \"unsatisfiable core\" of an unsatisfiable set of clauses, often much smaller than the clause set as a whole. The unsatisfiable core is mapped back into the syntax of the original model, showing the user fragments of the model found to be irrelevant. This information can be a great help in discovering and localizing overconstraint, and in some cases pinpoints it immediately. The construction of the mapping is given for a generalized modeling language, along with a justification of the soundness of the claim that the marked portions of the model are irrelevant. Experiences in applying core extraction to a variety of existing models are discussed."
9.url:http://dx.doi.org/10.1109/ASE.2003.1240298
9.opinion:exclude

10.title:"Parallel breadth-first search LTL model-checking"
10.abstract:"We propose a practical parallel on-the-fly algorithm for enumerative LTL (linear temporal logic) model checking. The algorithm is designed for a cluster of workstations communicating via MPI (message passing interface). The detection of cycles (faulty runs) effectively employs the so called back-level edges. In particular, a parallel level-synchronized breadth-first search of the graph is performed to discover back-level edges. For each level, the back-level edges are checked in parallel by a nested depth-first search to confirm or refute the presence of a cycle. Several optimizations of the basic algorithm are presented and advantages and drawbacks of their application to distributed LTL model-checking are discussed. Experimental implementation of the algorithm shows promising results."
10.url:http://dx.doi.org/10.1109/ASE.2003.1240299
10.opinion:exclude

11.title:"Automated environment generation for software model checking"
11.abstract:"A key problem in model checking open systems is environment modeling (i.e., representing the behavior of the execution context of the system under analysis). Software systems are fundamentally open since their behavior is dependent on patterns of invocation of system components and values defined outside the system but referenced within the system. Whether reasoning about the behavior of whole programs or about program components, an abstract model of the environment can be essential in enabling sufficiently precise yet tractable verification. In this paper, we describe an approach to generating environments of Java program fragments. This approach integrated formally specified assumptions about environment behavior with sound abstractions of environment implementations to form a model of the environment. The approach is implemented in the Bandera environment generator (BEG) which we describe along with our experience using BEG to reason about properties of several nontrivial concurrent Java programs."
11.url:http://dx.doi.org/10.1109/ASE.2003.1240300
11.opinion:exclude

12.title:"Aspectizing server-side distribution"
12.abstract:"We discuss how a collection of domain-specific and domain-independent tools can be combined to \"aspectize\" the distributed character of server-side applications, to a much greater extent than with prior efforts. Specifically, we present a framework that can be used with a large class of unaware applications to turn their objects into distributed objects with minimal programming effort. Our framework is developed on top of three main components: AspectJ (a high-level aspect language), XDoclet (a low-level aspect language), and NRMI (a middleware facility that makes remote calls behave more like local calls). We discuss why each of the three components offers unique advantages and is necessary for an elegant solution, why our approach is general, and how it constitutes a significant improvement over past efforts to isolate distribution concerns."
12.url:http://dx.doi.org/10.1109/ASE.2003.1240301
12.opinion:exclude

13.title:"Automating component adaptation for reuse"
13.abstract:"Reuse is a sound and practical design technique in many engineering disciplines. Although successful instances of software reuse are becoming more common, the cost of reuse tends to outweigh the potential benefits. The costs of software reuse include establishing and maintaining a library of reusable components, searching for applicable components to be reused, as well as adapting components toward a solution to a design problem. In this paper, we present a framework, called SPARTACAS, for automating specification-based component retrieval and adaptation. Components that partially satisfy the constraints of a design problem are adapted using adaptation architectures. Adaptation architectures modify the behavior of a software component by imposing interactions with other components. Based on the functionality specified in the problem and the partially-matched component, a sub-problem that specifies the missing functionality is synthesized. The sub-problem is used to query the library for components for adaptation. The framework was implemented and evaluated empirically, the results suggest that automated adaptation using architectures successfully promotes software reuse, and hierarchically organizes a solution to a design problem."
13.url:http://dx.doi.org/10.1109/ASE.2003.1240302
13.opinion:exclude

14.title:"Model-based verification of Web service compositions"
14.abstract:"In this paper, we discuss a model-based approach to verifying Web service compositions for Web service implementations. The approach supports verification against specification models and assigns semantics to the behavior of implementation model so as to confirm expected results for both the designer and implementer. Specifications of the design are modeled in UML (Unified Modeling Language), in the form of message sequence charts (MSC), and mechanically compiled into the finite state process notation (FSP) to concisely describe and reason about the concurrent programs. Implementations are mechanically translated to FSP to allow a trace equivalence verification process to be performed. By providing early design verification, the implementation, testing, and deployment of Web service compositions can be eased through the understanding of the differences, limitations and undesirable traces allowed by the composition. The approach is supported by a suite of cooperating tools for specification, formal modeling and trace animation of the composition workflow."
14.url:http://dx.doi.org/10.1109/ASE.2003.1240303
14.opinion:exclude

15.title:"What test oracle should I use for effective GUI testing?"
15.abstract:"Test designers widely believe that the overall effectiveness and cost of software testing depends largely on the type and number of test cases executed on the software. In this paper we show that the test oracle used during testing also contributes significantly to test effectiveness and cost. A test oracle is a mechanism that determines whether software executed correctly for a test case. We define a test oracle to contain two essential parts: oracle information that represents expected output; and an oracle procedure that compares the oracle information with the actual output. By varying the level of detail of oracle information and changing the oracle procedure, a test designer can create different types of test oracles. We design 11 types of test oracles and empirically compare them on four software systems. We seed faults in software to create 100 faulty versions, execute 600 test cases on each version, for all 11 types of oracles. In all, we report results of 660,000 test runs on software. We show (1) the time and space requirements of the oracles, (2) that faults are detected early in the testing process when using detailed oracle information and complex oracle procedures, although at a higher cost per test case, and (3) that employing expensive oracles results in detecting a large number of faults using relatively smaller number of test cases."
15.url:http://dx.doi.org/10.1109/ASE.2003.1240304
15.opinion:exclude

16.title:"A type system for statically detecting spreadsheet errors"
16.abstract:"We describe a methodology for detecting user errors in spreadsheets, using the notion of units as our basic elements of checking. We define the concept of a header and discuss two types of relationships between headers, namely is-a and has-a relationships. With these, we develop a set of rules to assign units to cells in the spreadsheet. We check for errors by ensuring that every cell has a well-formed unit. We describe an implementation of the system that allows the user to check Microsoft Excel spreadsheets. We have run our system on practical examples, and even found errors in published spreadsheets."
16.url:http://dx.doi.org/10.1109/ASE.2003.1240305
16.opinion:exclude

17.title:"Testing database transaction concurrency"
17.abstract:"Database application programs are often designed to be executed concurrently by many users. By grouping related database queries into transactions, DBMS (database management system) can guarantee that each transaction satisfies the well-known ACID properties: atomicity, consistency, isolation, and durability. However, if a database application is decomposed into transactions in an incorrect manner, the application may fail when executed concurrently due to potential offline concurrency problems. This paper presents a dataflow analysis technique for identifying schedules of transaction execution aimed at revealing concurrency faults of this nature, along with techniques for controlling the DBMS or the application so that execution of transaction sequences follows generated schedules. The techniques have been integrated into AGENDA, a tool set for testing relational database application programs. Preliminary empirical evaluation is presented."
17.url:http://dx.doi.org/10.1109/ASE.2003.1240306
17.opinion:exclude

18.title:"On the automatic evolution of an OS kernel using temporal logic and AOP"
18.abstract:"Automating software evolution requires both identifying precisely the affected program points and selecting the appropriate modification at each point. This task is particularly complicated when considering a large program, even when the modifications appear to be systematic. We illustrate this situation in the context of evolving the Linux kernel to support Bossa, an event-based framework for process-scheduler development. To support Bossa, events must be added at points scattered throughout the kernel. In each case, the choice of event depends on properties of one or a sequence of instructions. To describe precisely the choice of event, we propose to guide the event insertion by using a set of rules, amounting to an aspect that describes the control-flow contexts in which each event should be generated. In this paper, we present our approach and describe the set of rules that allows proper event insertion. These rules use temporal logic to describe sequences of instructions that require events to be inserted. We also give an overview of an implementation that we have developed to automatically perform this evolution."
18.url:http://dx.doi.org/10.1109/ASE.2003.1240307
18.opinion:exclude

19.title:"Unspeculation"
19.abstract:"Modern architectures, such as the Intel Itanium, support speculation, a hardware mechanism that allows the early execution of expensive operations possibly even before it is known whether the results of the operation are needed. While such speculative execution can improve execution performance considerably, it requires a significant amount of complex support code to deal with and recover from speculation failures. This greatly complicates the tasks of understanding and re-engineering speculative code. This paper describes a technique for removing speculative instructions from optimized binary programs in a way that is guaranteed to preserve program semantics, thereby making the resulting \"unspeculated\" programs easier to understand and more amenable to reengineering using traditional reverse engineering techniques."
19.url:http://dx.doi.org/10.1109/ASE.2003.1240308
19.opinion:exclude

20.title:"SPQR: flexible automated design pattern extraction from source code"
20.abstract:"Previous automated approaches to discovering design patterns in source code have suffered from a need to enumerate static descriptions of structural and behavioral relationships, resulting in a finite library of variations on pattern implementation. Our approach, system for pattern query and recognition, or SPQR, differs in that we do not seek statically to encode each variant of the patterns that we wish to find. Our system finds pattern variants that were not explicitly defined, but instead are inferred dynamically during code analysis by a theorem prover, providing practical tool support for software construction, comprehension, maintenance, and refactoring. We use a logical inference system to reveal large numbers of patterns and their variations from a small number of definitions by encoding in a formal denotational semantics a small number of fundamental OO concepts (elemental design patterns), encode the rules by which these concepts are combined to form patterns (reliance operators), and encode the structural/behavioral relationships among components of objects and classes (rho-calculus). A chain of fully automated tools provides a path from source code to revealed patterns. We describe our approach in this paper with a concrete example to drive the discussion, accompanied by formal treatment of the foundational topics."
20.url:http://dx.doi.org/10.1109/ASE.2003.1240309
20.opinion:exclude

21.title:"Automatically inferring concern code from program investigation activities"
21.abstract:"When performing a program evolution task, developers typically spend a significant amount of effort investigating and reinvestigating source code. To reduce this effort, we propose a technique to automatically infer the essence of program investigation activities as a set of concern descriptions. The concern descriptions produced by our technique list methods and fields of importance in the context of the investigation of an object-oriented system. A developer can rely on this information to perform the change task at hand, or at a later stage for a change that involves the same concerns. The technique involves applying an algorithm to a transcript of a program investigation session. The transcript lists which pieces of source code were accessed by a developer when investigating a program and how the different pieces of code were accessed. We applied the technique to data obtained from program investigation activities for five subjects involved in two different program evolution tasks. The results show that relevant concerns can be identified with a manageable level of noise."
21.url:http://dx.doi.org/10.1109/ASE.2003.1240310
21.opinion:exclude

22.title:"A model-driven approach to non-functional analysis of software architectures"
22.abstract:"We present an approach to managing formal models using model driven architecture (MDA) technologies that deliver analysis techniques through integration with the design tools and repositories that practitioners use. Expert modeling knowledge is captured in domain-specific languages and meta-model constraints. These are represented using UML (Unified Modeling Language) and collocated with designs and analysis models, providing a flexible and visible approach to managing semantic associations. The approach relies on standards to permit deployment in multiple tools. We demonstrate our approach with an example in which queuing-network models are associated with UML design models to predict average case performance."
22.url:http://dx.doi.org/10.1109/ASE.2003.1240311
22.opinion:exclude

23.title:"Extending diagnosis to debug programs with exceptions"
23.abstract:"Even with modern software development methodologies, the actual debugging of source code, i.e., location and identification of errors in the program when errant behavior is encountered during testing, remains a crucial part of software development. To apply model-based diagnosis techniques which have long been state of the art in hardware diagnosis, for automatic debugging a model of a given program must be automatically created from the source code. This work describes a model that reflects the sequential execution semantics of the Java language, including exceptions and unstructured control flow, thereby providing unprecedented scope in the application of model-based diagnosis to programs. Notably, this approach omits the strict view of a component representing one statement of earlier work and provides a more flexible mapping from code to model."
23.url:http://dx.doi.org/10.1109/ASE.2003.1240312
23.opinion:exclude

24.title:"Generating design pattern detectors from pattern specifications"
24.abstract:"We present our approach to support program understanding by a tool that generates static and dynamic analysis algorithms from design pattern specifications to detect design patterns in legacy code. We therefore specify the static and dynamic aspects of patterns as predicates, and represent legacy code by predicates that encode its attributed abstract syntax trees. Given these representations, the static analysis is performed on the legacy code representation as a query derived from the specification of the static pattern aspects. It provides us with pattern candidates in the legacy code. The dynamic specification represents state sequences expected when using a pattern. We monitor the execution of the candidates and check their conformance to this expectation. We demonstrate our approach and evaluate our tool by detecting instances of the observer, composite and decorator patterns in Java code using Prolog to define predicates and queries."
24.url:http://dx.doi.org/10.1109/ASE.2003.1240313
24.opinion:exclude

25.title:"Predicting fault prone modules by the Dempster-Shafer belief networks"
25.abstract:"This paper describes a novel methodology for predicting fault prone modules. The methodology is based on Dempster-Shafer (D-S) belief networks. Our approach consists of three steps: first, building the D-S network by the induction algorithm; second, selecting the predictors (attributes) by the logistic procedure; third, feeding the predictors describing the modules of the current project into the inducted D-S network and identifying fault prone modules. We applied this methodology to a NASA dataset. The prediction accuracy of our methodology is higher than that achieved by logistic regression or discriminant analysis on the same dataset."
25.url:http://dx.doi.org/10.1109/ASE.2003.1240314
25.opinion:exclude

26.title:"Semi-automatic fault localization and behavior verification for physical system simulation models"
26.abstract:"Mathematical modeling and simulation of complex physical systems are emerging as key technologies in engineering. Modern approaches to physical system simulation allow users to specify simulation models with the help of equation-based languages. Due to the high-level declarative abstraction of these languages program errors are extremely hard to find. This paper presents an algorithmic semi-automated debugging framework for equation-based modeling languages. We show how program slicing and dicing performed at the intermediate code level combined with assertion checking techniques can automate, to a large extent, the error finding process and behavior verification for physical system simulation models."
26.url:http://dx.doi.org/10.1109/ASE.2003.1240315
26.opinion:exclude

27.title:"Automatic generation of content management systems from EER-based specifications"
27.abstract:"ERW (entity-relationship Web browser) is an innovative open-source system for handling complex databases using a Web browser. Once the details of an enhanced entity-relationship schema have been specified in XML (eXtended Markup Language), ERW generates a complete application that lets the user interact with the database. Then, specification percolation makes it possible to customize heavily the application while maintaining the flexibility of a model-driven approach."
27.url:http://dx.doi.org/10.1109/ASE.2003.1240316
27.opinion:exclude

28.title:"Automated requirements-based generation of test cases for product families"
28.abstract:"Software product families (PF) are becoming one of the key challenges of software engineering. Despite recent interest in this area, the extent to which the close relationship between PF and requirements engineering is exploited to guide the V&V tasks is still limited. In particular, PF processes generally lack support for generating test cases from requirements. In this paper, we propose a requirements-based approach to functional testing of product lines, based on a formal test generation tool. Here, we outline how product-specific test cases can be automatically generated from PF functional requirements expressed in UML (Unified Modeling Language). We study the efficiency of the generated test cases on a case study."
28.url:http://dx.doi.org/10.1109/ASE.2003.1240317
28.opinion:exclude

29.title:"XRay views: understanding the internals of classes"
29.abstract:"Understanding the internal workings of classes is a key prerequisite to maintaining an object-oriented software system. Unfortunately, classical editing and browsing tools offer mainly linear and textual views of classes and their implementation. These views fail to expose the semantic relationships between the internal parts of a class. We propose X-Ray views - a technique based on concept analysis - which reveal the internal relationships between groups of methods and attributes of a class. X-Ray views are composed of elementary collaborations between attributes and methods, and help the engineer to build a mental model of how a class works internally. In this paper we present X-Ray views, and illustrate the approach by applying it on the Smalltalk class UIBuilder."
29.url:http://dx.doi.org/10.1109/ASE.2003.1240318
29.opinion:exclude

30.title:"Visual constraint diagrams: runtime conformance checking of UML object models versus implementations"
30.abstract:"This paper presents visual constraint diagrams (VCD), an extension to UML (Unified Modeling Language) object diagrams for expressing constraints over object models. VCD allows designers to express well-formedness constraints that cannot be expressed using class diagrams alone; an example of such a constraint is that a linked list data structure cannot have any loops. VCD offers two advances over existing techniques: (1) they allow constraints to be expressed within the visual notation of UML, without resorting to complex textual notations such as OCL; and (2) VCD can be checked at runtime, increasing the value of design documents to developers. An editor and a checker for VCD have been implemented as part of the Rosetta software design tool."
30.url:http://dx.doi.org/10.1109/ASE.2003.1240319
30.opinion:exclude

31.title:"A pragmatic study of binary class relationships"
31.abstract:"A discontinuity exists between modeling and object-oriented programming languages. This discontinuity is a consequence of ambiguous notions in modeling languages and lack of corresponding notions in object-oriented programming languages. It hinders the transition between software implementation and design and hampers software maintenance. This discontinuity is particularly acute for binary class relationships, such as the association, aggregation, and composition relationships. We present a solution to bridge the discontinuity between implementation and design for the binary class relationships: we propose consensual definitions of the binary class relationships in terms of four properties (exclusivity, invocation site, lifetime, multiplicity). We describe algorithms to detect these properties in Java source code."
31.url:http://dx.doi.org/10.1109/ASE.2003.1240320
31.opinion:exclude

32.title:"The feature signatures of evolving programs"
32.abstract:"As programs evolve, their code increasingly becomes tangled by programmers and requirements. This mosaic quality complicated program comprehension and maintenance. Many of these activities can benefit from viewing the program as a collection of features. We introduce an inexpensive and easily comprehensible summary of program changes called the feature signature and investigate its properties. We find a remarkable similarity in the nature of feature signatures across multiple nontrivial programs, developers and magnitude changes. This indicates that feature signatures are a meaningful notion worth studying. We then show numerous applications of feature signatures to software evolution, establishing their utility."
32.url:http://dx.doi.org/10.1109/ASE.2003.1240321
32.opinion:exclude

33.title:"Test suite design for code generation tools"
33.abstract:"In model-based development, executable specifications (models) are used for the design of the software to be developed. New techniques allow the automatic generation of compact code directly from the model via so-called code generators. However, at present, code generators do not possess the same quality characteristics as C or ADA compilers which have been proven in use. The use of test suites, which make it possible to check compilers systematically, is also a promising approach for code generators. This paper describes the design of such a test suite for code generators, and introduces a new testing approach for code generator transformations."
33.url:http://dx.doi.org/10.1109/ASE.2003.1240322
33.opinion:exclude

34.title:"Theoretical foundations of updating systems"
34.abstract:"Software systems inevitably require update and revision during their lifetime. The concept of features is often used to model system update: a feature is a unit of functionality which may be integrated into a base system. Possible features of an email client program include: spam filtering; absence messages; selective forwarding; and encryption. In our work, we use AI techniques to understand the operation of feature integration more clearly. In particular, we have taken SMV (symbolic model verifier) feature integrator (SFI), a tool which automates feature integration on systems described using the model checker SMV. Then we have taken update which is an operation of theory change, closely related to belief revision, and defined over propositional logic. We formulate and prove a theorem stating that SFI feature integration is an update operation."
34.url:http://dx.doi.org/10.1109/ASE.2003.1240323
34.opinion:exclude

35.title:"An incremental approach to task-specific information delivery in SE processes"
35.abstract:"In this paper, we present a system to proactively provide software process participants with information items that are available and relevant in the context of their current tasks. Knowledge on which items to offer is obtained in two ways. First, situation-specific information needs that arise for participants are captured, together with the information sources that are accessed to satisfy them. These are recommended to other participants in similar situations using collaborative filtering techniques. Second, to allow for more systematic recommendation functionality, a formal language is provided to specify preconditions on when to offer certain information items. The resulting hybrid system thus supports an incremental phase-in of knowledge management techniques into an organization, allowing the organization to decide how much effort to spend on knowledge engineering."
35.url:http://dx.doi.org/10.1109/ASE.2003.1240324
35.opinion:exclude

36.title:"Architecture style-based calculi for non-functional properties"
36.abstract:"Engineers wield various \"calculi\" to help determine solutions to their problems, calculation tools varying in power from tensile strength tables to the differential calculus. A calculus is normally based on induction over an algebraic structure. Here the author explores how architecture styles can be used to describe such structures. An example calculus based on an \"integration\" style is presented, which is intended for use as a substyle of other architecture styles. Calculation rules in terms of the architectural elements can be used to compute non-functional attributes of artifacts described in such styles. Naturally, computerized support for calculi will help to automate the tasks of software engineers."
36.url:http://dx.doi.org/10.1109/ASE.2003.1240325
36.opinion:exclude

37.title:"Certifying measurement unit safety policy"
37.abstract:"Measurement unit safety policy checking is a topic in software analysis concerned with ensuring that programs do not violate basic principles of units of measurement. Such violations can hide significant domain-specific errors which are hard or impossible to find otherwise. Measurement unit analysis by means of automatic deduction is addressed in this paper. We draw general design principles for measurement unit certification tools and discuss our prototype for the C language, which includes both dynamic and static checkers. Our approach is based on assume/assert annotations of code, which are properly interpreted by our deduction-based tools and ignored by standard compilers. We do not modify the language in order to support units. The approach can be extended to incorporate other safety policies without great efforts."
37.url:http://dx.doi.org/10.1109/ASE.2003.1240326
37.opinion:exclude

38.title:"Automated software testing using a metaheuristic technique based on Tabu search"
38.abstract:"The use of techniques for automating the generation of software test cases is very important as it can reduce the time and cost of this process. The latest methods for automatic generation of tests use metaheuristic search techniques, i.e. genetic algorithms and simulated annealing. There is a great deal of research into the use of genetic algorithms to obtain a specific coverage in software testing but there is none using the metaheuristic Tabu search technique. In this paper, we explain how we have created an efficient testing technique that combines Tabu search with Korel chaining approach. Our technique automatically generates test data in order to obtain branch coverage in software testing."
38.url:http://dx.doi.org/10.1109/ASE.2003.1240327
38.opinion:exclude

39.title:"Model checking software requirement specifications using domain reduction abstraction"
39.abstract:"As an automated verification and validation tool, model checking can be quite effective in practice. Nevertheless, model checking has been quite inefficient when dealing with systems with data variables over a large (or infinite) domain, which is a serious limiting factor for its applicability in practice. To address this issue, we have investigated a static abstraction technique, domain reduction abstraction, based on data equivalence and trajectory reduction, and implemented it as a prototype extension of the symbolic model checker NuSMV. Unlike on-the-fly dynamic abstraction techniques, domain reduction abstraction statically analyzes specifications and automatically produces an abstract model which can be reused over time; a feature suitable for regression verification."
39.url:http://dx.doi.org/10.1109/ASE.2003.1240328
39.opinion:exclude

40.title:"An approach for tracing and understanding asynchronous architectures"
40.abstract:"Applications built in a strongly decoupled, event-based interaction style have many commendable characteristics, including ease of dynamic configuration, accommodation of platform heterogeneity, and ease of distribution over a network. It is not always easy to humanly grasp the dynamic behavior of such applications, since many threads are active and events are asynchronously (and profusely) transmitted. We present a set of requirements for an aid to assist in the human understanding and exploration of the behavior of such applications through the incremental refinement of rules for determining causality relationships between messages sent among components. A prototype tool is presented, indicating one viable approach to meeting these requirements. Experience with the tool reinforces some of the requirements and indicates others."
40.url:http://dx.doi.org/10.1109/ASE.2003.1240329
40.opinion:exclude

41.title:"A Java component model for evolving software systems"
41.abstract:"This paper presents a component model for designing and implementing flexible software components in Java. Our model defines a mapping of how the fundamental concepts of component-based development (CBD) should be implemented using the object-oriented (OO) constructs, available in the Java programming language. The benefit of this mapping is to shorten the distance between component-based software architecture and its implementation, enhancing the reusability, adaptability and maintainability of component-based software systems."
41.url:http://dx.doi.org/10.1109/ASE.2003.1240331
41.opinion:exclude

42.title:"Depiction and playout of multi-threaded program executions"
42.abstract:"Execution of a shared memory multi-threaded program is non-deterministic even for a fixed input. Consequently, a limited amount of the program behavior should be traced and recorded during run-time. However, if the tracing overheads are too high, we have the risk of slowing down the program considerably and even distorting the program behavior. In this paper, we propose to collect and store only the synchronization dependencies during run-time. These dependences are visualized as a message sequence chart (MSC). We do not record the data dependences across threads resulting from unsynchronized reads and writes of a shared variable. Instead all possible orderings of unsynchronized reads/writes are analyzed post-mortem. To describe all these behaviors, we use an important extension of message sequence charts called live sequence charts (LSC). Our MSC/LSC based description of a multi-threaded program execution can be simulated in an automated manner. This can help in understanding program behavior."
42.url:http://dx.doi.org/10.1109/ASE.2003.1240332
42.opinion:exclude

43.title:"Applying AutoBayes to the analysis of planetary nebulae images"
43.abstract:"We take a typical scientific data analysis task, the analysis of planetary nebulae images taken by the Hubble Space Telescope, and describe how program synthesis can be used to generate the necessary analysis programs from high-level models. We describe the AutoBayes synthesis system, discuss its fully declarative specification language, and present the automatic program derivation starting with the scientists' original analysis."
43.url:http://dx.doi.org/10.1109/ASE.2003.1240333
43.opinion:exclude

44.title:"Automation for exception freedom proofs"
44.abstract:"Run-time errors are typically seen as unacceptable within safety and security critical software. The SPARK approach to the development of high integrity software addresses the problem of run-time errors through the use of formal verification. Proofs are constructed to show that each run-time check will never raise an error, thus proving freedom from run-time exceptions. Here we build upon the success of the SPARK approach by increasing the level of automation that can be achieved in proving freedom from exceptions. Our approach is based upon proof planning and a form of abstract interpretation."
44.url:http://dx.doi.org/10.1109/ASE.2003.1240334
44.opinion:exclude

45.title:"Overview of OpenModel-based validation with partial information"
45.abstract:"Multi-stakeholder distributed systems (MSDS), such as the Internet email and instant messaging systems, and e-business Web service networks, raise new challenges for users, developers, and systems analysts. Traditional requirements engineering, validation, and debugging approaches cannot handle two primary problems of MSDS: the lack of consistent high level requirements and the ignorance problem caused by lack of communication among stakeholders. OpenModel described by R. Hall (2002) addresses this ignorance problem: each MSDS node publishes a behavioral model of itself so that remote stakeholders can reason about their interactions with it. However, stakeholders will typically wish to hold back private state information, such as user identities and cryptographic keys. An OpenModel-based validation tool must tolerate missing information and yet still give useful analyses where possible. These paper overviews OMV, a novel approach to validation in the face of partial information based upon symbolic simulation of OpenModel models. We briefly illustrate our studies of the OMV tool in the domains of email and instant messaging."
45.url:http://dx.doi.org/10.1109/ASE.2003.1240335
45.opinion:exclude

46.title:"Detecting requirements interactions: a three-level framework"
46.abstract:"This paper deals with the problem of requirements interaction. We introduce a three level framework to detect requirements interactions at different levels of cost, time, and complexity. Level 2 where we use semiformal methods to detect interactions contains the main contribution of the research. Also we combine existing approaches (e.g. informal and formal) with our semiformal approach to provide a comprehensive framework for developers to use. The approach is illustrated using two case studies, one from the telecommunications domain and the other one being a lift control system. The results obtained are very encouraging with regards to the time and effort spent on requirements interaction detection."
46.url:http://dx.doi.org/10.1109/ASE.2003.1240336
46.opinion:exclude

47.title:"Automating relative debugging"
47.abstract:"The creation of a new program version based on an existing version is known as software evolution. In 1994, Abramson and Sosic proposed relative debugging to assist users to locate errors in programs developed with software evolutionary techniques. Relative debugging is a paradigm described by D. Abramson et al. (1996) that enables programmers to locate errors by comparing the developed (evolved) program with the original (existing) program as they are concurrently executed. The aim of the proposed research is to further enhance the currently defined paradigm by (partially or completely) automating the process of relatively debugging. We investigate the possibility of automatically identifying the data structures and program points, normally performed by the user, where values should be equivalent during program execution. Minimizing the user's involvement will reduce the cost of enhancing, maintaining and porting software, and has the potential to provide significant productivity gains on current practices in software development."
47.url:http://dx.doi.org/10.1109/ASE.2003.1240337
47.opinion:exclude

48.title:"Communicating requirements using end-user GUI constructions with argumentation"
48.abstract:"Unsuccessful communication is often at the root of inadequate requirements specification according to C. Potts et al. (1994). This can lead to requirements that do not capture complete stakeholder expectations. Stakeholders can include managers, software engineers, end-users, clients, etc. End-users provide a rich source of information about a system as they will directly interact with the final system. They also tend to have a solid knowledge of the domain including the tasks being automated. Thus, a major goal early in the software engineering process is gathering meaningful requirements from end-users. This paper explores the use of mock end-user graphical interface construction supplemented with textual argumentation as a means of communicating software requirements information to software requirements analysts and providing automated assistance for requirements analysts examining this information."
48.url:http://dx.doi.org/10.1109/ASE.2003.1240338
48.opinion:exclude

49.title:"Graph rewriting and transformation (GReAT): a solution for the model integrated computing (MIC) bottleneck"
49.abstract:"Graph grammars and transformations (GGT) have been a field of theoretical study for over two decades. However, it has produced only a handful of practical implementations. GGT needs a widely used practical application to exploit its potential. On the other hand model integrated computing (MIC) has grown from the practical standpoint and is widely used and recognized in both industry and practice today. In the MIC approach, developing model-interpreters is time consuming and costly, proving to be a bottleneck. This reduces MIC's reach and impact on the programming community. In this paper I propose to use GGT methodologies to solve MIC's bottleneck problem. The solution should place the MIC technology such that it can play a defining role in the next generation of high-level programming languages."
49.url:http://dx.doi.org/10.1109/ASE.2003.1240339
49.opinion:exclude

50.title:"Visual specification of concurrent systems"
50.abstract:"The work on a visual formalism for specification of concurrent systems is presented. It is proposed to match requirements of state-of-the-art component-based design methods. Special emphasis is given to specification of heterogeneous systems in which the different models of computation can be mixed together. We briefly summarize recent research related to the topic and give a sketch of the basic ideas for definition of the proposed language. The already achieved results of our work are presented as well."
50.url:http://dx.doi.org/10.1109/ASE.2003.1240340
50.opinion:exclude

51.title:"VUML: a viewpoint oriented UML extension"
51.abstract:"One of the main challenges of our modern societies is the development of information systems accessible to every citizen with respect to his culture, rights, education etc. A number of such information systems (servers) are now provided on the Web in e-learning, tourism, environment, health, transport, etc. But the development and the maintenance of those systems are not guided by users' profile (viewpoints) and thus such systems are very difficult to adapt, reuse and maintain when a viewpoint must be added/removed/updated. To meet these requirements, we propose an extension of UML called VUML (View based Unified Modeling Language). VUML provides the concept of multiviews component whose goal is to store and deliver information according to users' viewpoints. A multiviews component consists of a default view and a set of specific views related to the base through an extension relation. This approach allows for dynamic change of viewpoints and offers mechanisms to describe views dependencies. To favor reuse and transition to coding, we propose an implementation generic pattern targeting object-oriented languages."
51.url:http://dx.doi.org/10.1109/ASE.2003.1240341
51.opinion:exclude

52.title:"An infrastructure to support meta-differencing and refactoring of source code"
52.abstract:"The proposed research aims to construct an underlying infrastructure to support (semi) automated construction of refactorings and system wide transformation via a fine grained syntax level differencing approach. We term this differencing approach meta-differencing as it has additional knowledge of the types of entities being differenced. The general approach is built on top of an XML representation of the source code, specifically srcXML by J. Maletic et al. (2002). This representation explicitly embeds high level syntactic information within the source code in such a way as to not interfere with program development and maintenance. Because both the source code and the difference are represented in XML, the transformational language, XSLT, can be used to model these changes. We propose to develop an environment (development/maintenance) that automatically generates XSLT programs based on changes to a program."
52.url:http://dx.doi.org/10.1109/ASE.2003.1240342
52.opinion:exclude

53.title:An Empirical Study on Groupware Support for Software Inspection Meetings
53.abstract:Software inspection is an effective way to assess product quality and to reduce the number of defects. In a software inspection the inspection meeting is a key activity to agree on collated defects, to eliminate false positives, and to disseminate knowledge among the team members. However, inspection meetings often require high effort and may lose defects found in earlier inspection steps due to ineffective meeting techniques. Only few tools are available for this task. We have thus been developing a set of groupware tools to lower the effort of inspection meetings and to increase their efficiency. We conducted an experiment in an academic environment with 37 subjects to empirically investigate the effect of groupware tool support for inspection meetings. The main findings of the experiment are that tool support considerably lowered the meeting effort, supported inspectors in identifying false positives, and reduced the number of true defects lost.
53.url:http://https://www.computer.org/csdl/proceedings/ase/2003/2035/00/20350004-abs.html
53.opinion:exclude

54.title:Refactoring C with Conditional Compilation
54.abstract:Refactoring, an important technique for increasing flexibility of the source code, can be applied with much ease and efficiency by using automated tools. There is currently a lack of refactoring tools for C with full support for preprocessor directives because directives complicate refactorings in many ways.<div></div> This paper describes refactoring of C programs in the presence of conditional compilation directives and how we propose to support them in a refactoring tool.
54.url:https://www.computer.org/csdl/proceedings/ase/2003/2035/00/20350323-abs.html
54.opinion:exclude
