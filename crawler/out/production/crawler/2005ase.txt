1.title:The power of software
1.abstract:Software is the core constituent of many modern products and services. Many industrial sectors can be considered an "application" of computer science and software engineering, as they increasingly rely on software to provide their features and functionality. For instance, mobile phones have completed their transformation from specialized telecom devices to mobile computers: making phone calls (possibly using VoIP) is just one of the many (software) features offered to the customer. In general, many market sectors are dramatically changing their nature, structure, and operations as a result of their transformation into software-centered businesses.The main consequence of this dramatic change is that software has become a strategic asset for companies, institutions, individuals, and governments. Free software and software patents are perhaps the hottest topics that are being nowadays discussed by researchers and practitioners. In general, it is essential to understand how software impacts on business models, the management of Intellectual Property, industrial development, and the society in general (GNU.org claims that "free software is a matter of liberty"). As such, the subject is extremely complex as it has different dimensions and facets. It is not just a matter of technology: rather, it is a combination of technological, legal, social, economical, and even ethical and emotional issues. This means that it is difficult to identify a common ground of discussion and to properly assess claims and opinions.The talk will provide an overview of the trends in the software industry and in representative industrial sectors that are increasingly relying on software. It will then briefly discuss how software is affecting our society in general, and the role of software patents in innovation. This analysis will be instrumental to present some reflections and proposals to master the complexity of the problems we are challenged to face with.
1.url:http://doi.acm.org/10.1145/1101908.1101909
1.opinion:exclude

2.title:Virtual humans: lessons learned in integrating a large-scale AI project
2.abstract:Virtual humans are computer generated characters that can populate games or simulations. The behaviors of virtual humans are not scripted, but instead they use sophisticated AI techniques to reason about their environment and events as they unfold. Based on that reasoning, the virtual humans respond in believable ways. Virtual humans can interact in natural language, understanding speech and responding with synthesized speech. In addition to verbal communication, virtual humans can use non-verbal communication means such as gestures. In addition to rational behavior, virtual humans also have the ability to model and mimic human emotions.At the USC Institute for Creative Technologies, we have been engaged in the construction of virtual humans for the past six years. This project is not only a significant AI research effort, but it is also a significant software engineering task because a number of research projects must be integrated and work together to realize the virtual human. In this talk I will outline the virtual human effort, describe some of the synergies that have emerged from the software integration effort, and report on the lessons we have learned in this large-scale integration effort.
2.url:http://doi.acm.org/10.1145/1101908.1101910
2.opinion:exclude

3.title:Designing and implementing a family of intrusion detection systems
3.abstract:Intrusion detection systems are distributed applications that analyze the events in a networked system to identify malicious behavior. The analysis is performed using a number of attack models (or signatures) that are matched against a specific event stream. Intrusion detection systems may operate in heterogeneous environments, analyzing different types of event streams. Currently, intrusion detection systems and the corresponding attack modeling languages are developed following an ad hoc approach to match the characteristics of specific target environments. As the number of systems that have to be protected increases, this approach results in increased development effort. To overcome this limitation, we developed a framework, called STAT, that supports the development of new intrusion detection functionality in a modular fashion. The STAT framework can be extended following a well-defined process to implement intrusion detection systems tailored to specific environments, platforms, and event streams. The STAT framework is novel in the fact that the extension process also includes the extension of the attack modeling language. The resulting intrusion detection systems represent a software family whose members share common attack modeling features and the ability to reconfigure their behavior dynamically. The STAT framework allows an Intrusion Detection Administrator to express high-level configuration requirements that are mapped automatically to a detailed deployment and/or reconfiguration plan. This approach supports automation of the administrator tasks and better assurance of the effectiveness and consistency of the deployed sensing infrastructure.
3.url:http://doi.acm.org/10.1145/1101908.1101911
3.opinion:exclude

4.title:Exploiting predicate structure for efficient reachability detection
4.abstract:Partial order (p.o.) reduction techniques are a popular and effective approach for tackling state space explosion in the verification of concurrent systems. These techniques generate a reduced search space that could be exponentially smaller than the complete state space. Their major drawback is that the amount of reduction achieved is highly sensitive to the properties being verified. For the same program, different properties could result in very different amounts of reduction achieved.We present a new approach which combines the benefits of p.o. reduction with the added advantage that the size of the constructed state space is completely independent of the properties being verified. As in p.o. reduction, we use the notion of persistent sets to construct a representative interleaving for each maximal trace of the program. However, we retain concurrency information by assigning vector timestamps to the events in each interleaving. Our approach hinges upon the use of efficient algorithms that parse the encoded concurrency information in the representative interleaving to determine whether a safety violation exists in any interleaving of the corresponding trace. We show that, for some types of predicates, reachability detection can be performed in time that is polynomial in the length of the interleaving. Typically, these predicates exhibit certain characteristics that can be exploited by the detection algorithm.We implemented our algorithms in the popular model checker SPIN, and present experimental results that demonstrate the effectiveness of our techniques. For example, we verified a distributed dining philosophers protocol in 0.03 seconds, using 1.253 MB of memory. SPIN, using traditional p.o. reduction techniques, took 759.71 seconds and 439.116 MB of memory.
4.url:http://doi.acm.org/10.1145/1101908.1101913
4.opinion:exclude

5.title:Application of design for verification with concurrency controllers to air traffic control software
5.abstract:We present an experimental study which demonstrates that model checking techniques can be effective in finding synchronization errors in safety critical software when they are combined with a design for verification approach. We apply the concurrency controller design pattern to the implementation of the synchronization operations in Java programs. This pattern enables a modular verification strategy by decoupling the behaviors of the concurrency controllers from the behaviors of the threads that use them using interfaces specified as finite state machines. The behavior of a concurrency controller can be verified with respect to arbitrary numbers of threads using infinite state model checking techniques, and the threads which use the controller classes can be checked for interface violations using finite state model checking techniques. We present techniques for thread isolation which enables us to analyze each thread in the program separately during interface verification. We conducted an experimental study investigating the effectiveness of the presented design for verification approach on safety critical air traffic control software. In this study, we first reengineered the Tactical Separation Assisted Flight Environment (TSAFE) software using the concurrency controller design pattern. Then, using fault seeding, we created 40 faulty versions of TSAFE and used both infinite and finite state verification techniques for finding the seeded faults. The experimental study demonstrated the effectiveness of the presented modular verification approach and resulted in a classification of faults that can be found using the presented approach.
5.url:http://doi.acm.org/10.1145/1101908.1101914
5.opinion:exclude

6.title:Efficient temporal-logic query checking for presburger systems
6.abstract:This paper develops a framework for solving temporal-logic query-checking problems for a class of infinite-state system models that compute with integer-valued variables (so-called Presburger systems, in which Presburger formulas are used to define system behavior). The temporal-logic query checking problem may be formulated as follows: given a model and a temporal logic formula with placeholders, compute a set of assignments of formulas to placeholders such that the resulting temporal formula is satisfied by the given model. Temporal-logic query checking has proved useful as a means for requirements and design understanding; existing work, however, has focused only on propositional temporal logic and finite-state systems.Our method is based on a symbolic model-checking technique that relies on proof search. The paper first introduces this model-checking approach and then shows how it can be adapted to solving the temporal queries in which formulas may contain integer variables. We also present experimental results showing the computational efficacy of our approach.
6.url:http://doi.acm.org/10.1145/1101908.1101915
6.opinion:exclude

7.title:A component model for internet-scale applications
7.abstract:This paper describes a component model where the overall semantics of a component is included in the interface definition. Such a model is necessary for future computing where programs will run at Internet-scales and will employ a combination of web services, grid technologies, peer-to-peer sharing, autonomic capabilities, and open source implementations. The component model is based on packages and supports static and dynamic objects, interfaces, structures, and exceptions. The interface definitions provide a practical approach to defining functional semantics and include appropriate extensions to provide semantics for security, privacy, recovery, and costs. The component model has been implemented in a prototype framework and demonstrated in an Internet-scale example.
7.url:http://doi.acm.org/10.1145/1101908.1101917
7.opinion:exclude

8.title:Automating the performance management of component-based enterprise systems through the use of redundancy
8.abstract:Component technologies are increasingly being used for building enterprise systems, as they can address complex functionality and flexibility problems and reduce development and maintenance costs. Nonetheless, current component technologies provide little support for predicting and controlling the emerging performance of software systems that are assembled from distinct components.This paper presents a framework for automating the performance management of complex, component-based systems. The adopted approach is based on the alternate usage of multiple component variants with equivalent functional characteristics, each one optimized for a different running environment. A fully-automated framework prototype for J2EE is presented, along with results from managing a sample enterprise application on JBoss. A mechanism that uses monitoring data to learn and automatically improve the framework's management behaviour is proposed. The framework imposes no extra requirements on component providers, or on the component technologies.
8.url:http://doi.acm.org/10.1145/1101908.1101918
8.opinion:exclude

9.title:UMLDiff: an algorithm for object-oriented design differencing
9.abstract:This paper presents UMLDiff, an algorithm for automatically detecting structural changes between the designs of subsequent versions of object-oriented software. It takes as input two class models of a Java software system, reverse engineered from two corresponding code versions. It produces as output a change tree, i.e., a tree of structural changes, that reports the differences between the two design versions in terms of (a) additions, removals, moves, renamings of packages, classes, interfaces, fields and methods, (b) changes to their attributes, and (c) changes of the dependencies among these entities. UMLDiff produces an accurate report of the design evolution of the software system, and enables subsequent design-evolution analyses from multiple perspectives in support of various evolution activities. UMLDiff and the analyses it enables can assist software engineers in their tasks of understanding the rationale of design evolution of the software system and planning future development and maintenance activities. We evaluate UMLDiff's correctness and robustness through a real-world case stud.
9.url:http://doi.acm.org/10.1145/1101908.1101919
9.opinion:exclude

10.title:Identifying traits with formal concept analysis
10.abstract:Traits are basically mixins or interfaces but with method bodies. In languages that support traits, classes are composed out of traits. There are two main advantages with traits. Firstly, decomposing existing classes into traits from which they can be recomposed improves the factoring of hierarchies. Secondly it increases the library reuse potential by providing more reusable traits. Identifying traits and decomposing class hierarchies into traits is therefore an important and challenging task to facilitate maintainability and evolution. In this paper we present how we use Formal Concept Analysis to identify traits in inheritance hierarchies. Our approach is two-staged: first we identify within a hierarchy maximal groups of methods that have a set of classes in common, second we cluster cohesive groups of methods based on method invocations as potential traits. We applied our approach on two significant hierarchies and compare our results with the manual refactorization of the same code which was done by the authors of traits.
10.url:http://doi.acm.org/10.1145/1101908.1101921
10.opinion:exclude

11.title:Precise identification of composition relationships for UML class diagrams
11.abstract:Knowing which associations are compositions is important in a tool for the reverse engineering of UML class diagrams. Firstly, recovery of composition relationships bridges the gap between design and code. Secondly, since composition relationships explicitly state a requirement that certain representations cannot be exposed, it is important to determine if this requirement is met by component code. Verifying that compositions are implemented properly may prevent serious program flaws due to representation exposure.We propose an implementation-level composition model based on ownership and a novel approach for identifying compositions in Java software. Our approach uses static ownership inference based on points-to analysis and is designed to work on incomplete programs. In our experiments, on average 40% of the examined fields account for relationships that are identified as compositions. We also present a precision evaluation which shows that for our code base our analysis achieves almost perfect precision---that is, it almost never misses composition relationships. The results indicate that precise identification of interclass relationships can be done with a simple and inexpensive analysis, and thus can be easily incorporated in reverse engineering tools that support iterative model-driven development.
11.url:http://doi.acm.org/10.1145/1101908.1101922
11.opinion:exclude

12.title:On dynamic feature location
12.abstract:Feature location aims at locating pieces of code that implement a given set of features (requirements). It is a necessary first step in every program comprehension and maintenance task if the connection between features and code has been lost.We have developed a semi-automatic technique for feature location using a combination of static and dynamic program analysis. Formal concept analysis is used to explore the results of the dynamic analysis.We describe new experiences with our technique. Specifically, we investigate the gain of information and increase of costs when the system under analysis is profiled at basic block level rather than routine level as in our earlier work. Furthermore, we explore the influence of the scenarios used for the dynamic analysis (minimal versus combined scenarios).
12.url:http://doi.acm.org/10.1145/1101908.1101923
12.opinion:exclude

13.title:Blowtorch: a framework for firewall test automation
13.abstract:Firewalls play a crucial role in network security. Experience has shown that the development of firewall rule sets is complex and error prone. Rule set errors can be costly, by allowing damaging traffic in or by blocking legitimate traffic and causing essential applications to fail. Consequently, firewall testing is extremely important. Unfortunately, it is also hard and there is little tool support available.Blowtorch is a C++ framework for firewall test generation. The central construct is the packet iterator: an event-driven generator of timestamped packet streams. Blowtorch supports the development of packet iterators with a library for packet header creation and parsing, a transmit scheduler for multiplexing of multiple packet streams, and a receive monitor for demultiplexing of arriving packet streams. The framework provides iterators which generate packet streams using covering arrays, production grammars, and replay of captured TCP traffic. Blowtorch has been used to develop tests for industrial firewalls that are placed between an IT network and a process control network.
13.url:http://doi.acm.org/10.1145/1101908.1101925
13.opinion:exclude

14.title:An analysis of rule coverage as a criterion in generating minimal test suites for grammar-based software
14.abstract:The term grammar-based software describes software whose input can be specified by a context-free grammar. This grammar may occur explicitly in the software, in the form of an input specification to a parser generator, or implicitly, in the form of a hand-written parser, or other input-verification routines. Grammar-based software includes not only programming language compilers, but also tools for program analysis, reverse engineering, software metrics and documentation generation. Such tools often play a crucial role in automated software development, and ensuring their completeness and correctness is a vital prerequisite for their us.In this paper we propose a strategy for the construction of test suites for grammar based software, and illustrate this strategy using the ISO CPP grammar. We use the concept of rule coverage as a pivot for the reduction of implementation-based and specification-based test suites, and demonstrate a significant decrease in the size of these suites. To demonstrate the validity of the approach, we use the reduced test suite to analyze three grammar-based tools for CPP++. We compare the effectiveness of the reduced test suite with the original suite in terms of code coverage and fault detection.
14.url:http://doi.acm.org/10.1145/1101908.1101926
14.opinion:exclude

15.title:Automatic test factoring for java
15.abstract:Test factoring creates fast, focused unit tests from slow system-wide tests; each new unit test exercises only a subset of the functionality exercised by the system test. Augmenting a test suite with factored unit tests should catch errors earlier in a test run.One way to factor a test is to introduce mock objects. If a test exercises a component T, which interacts with another component E (the "environment"), the implementation of E can be replaced by a mock. The mock checks that T's calls to E are as expected, and it simulates E's behavior in response. We introduce an automatic technique for test factoring. Given a system test for T and E, and a record of T's and E's behavior when the system test is run, test factoring generates unit tests for T in which E is mocked. The factored tests can isolate bugs in T from bugs in E and, if E is slow or expensive, improve test performance or cost.Our implementation of automatic dynamic test factoring for the Java language reduces the running time of a system test suite by up to an order of magnitude.
15.url:http://doi.acm.org/10.1145/1101908.1101927
15.opinion:exclude

16.title:ClassSheets: automatic generation of spreadsheet applications from object-oriented specifications
16.abstract:Spreadsheets are widely used in all kinds of business applications. Numerous studies have shown that they contain many errors that sometimes have dramatic impacts. One reason for this situation is the low-level, cell-oriented development process of spreadsheets.We improve this process by introducing and formalizing a higher-level object-oriented model termed ClassSheet. While still following the tabular look-and feel of spreadsheets, ClassSheets allow the developer to express explicitly business object structures within a spreadsheet, which is achieved by integrating concepts from the UML (Unified Modeling Language). A stepwise automatic transformation process generates a spreadsheet application that is consistent with the ClassSheet model. Thus, by deploying the formal underpinning of ClassSheets, a large variety of errors can be prevented that occur in many existing spreadsheet applications today.The presented ClassSheet approach links spreadsheet applications to the object-oriented modeling world and advocates an automatic model-driven development process for spreadsheet applications of high quality.
16.url:http://doi.acm.org/10.1145/1101908.1101929
16.opinion:exclude

17.title:Generation of visual editors as eclipse plug-ins
17.abstract:Visual Languages (VLs) play an important role in software system development. Especially when looking at well-defined domains, a broad variety of domain specific visual languages are used for the development of new applications. These languages are typically developed specifically for a certain domain in a way that domain concepts occur as primitives in the language alphabet. Visual modeling environments are needed to support rapid development of domain-specific solutions.In this contribution we present a general approach for defining visual languages and for generating language-specific tool environments. The visual language definition is again given in a visual manner and precise enough to completely generate the visual environment. The underlying technology is Eclipse with its plug-in capabilities on the one hand, and formal graph transformation techniques on the other hand. More precisely, we present an Eclipse plug-in generating Java code for visual modeling plug-ins which can be directly executed in the Eclipse Runtime-Workbench.
17.url:http://doi.acm.org/10.1145/1101908.1101930
17.opinion:exclude

18.title:Clearwater: extensible, flexible, modular code generation
18.abstract:Distributed applications typically interact with a number of heterogeneous and autonomous components that evolve independently. Methodical development of such applications can benefit from approaches based on domain-specific languages (DSLs). However, the evolution and customization of heterogeneous components introduces significant challenges to accommodating the syntax and semantics of a DSL in addition to the heterogeneous platforms on which they must run. In this paper, we address the challenge of implementing code generators for two such DSLs that are flexible (resilient to changes in generators or input formats), extensible (able to support multiple output targets and multiple input variants), and modular (generated code can be re-written). Our approach, Clearwater, leverages XML and XSLT standards: XML supports extensibility and mutability for in-progress specification formats, and XSLT provides flexibility and extensibility for multiple target languages. Modularity arises from using XML meta-tags in the code generator itself, which supports controlled addition, subtraction, or replacement to the generated code via XML-weaving. We discuss the use of our approach and show its advantages in two non-trivial code generators: the Infopipe Stub Generator (ISG) to support distributed flow applications, and the Automated Composable Code Translator to support automated distributed application deployment. As an example, the ISG accepts as input an XML description and generates output for C, C++, or Java using a number of communications platforms such as sockets and publish-subscribe.
18.url:http://doi.acm.org/10.1145/1101908.1101931
18.opinion:exclude

19.title:Secure sharing between untrusted users in a transparent source/binary deployment model
19.abstract:The Nix software deployment system is based on the paradigm of transparent source/binary deployment: distributors deploy descriptors that build components from source, while client machines can transparently optimise such source builds by downloading pre-built binaries from remote repositories. This model combines the simplicity and flexibility of source deployment with the efficiency of binary deployment. A desirable property is sharing of components: if multiple users install from the same source descriptors, ideally only one remotely built binary should be installed. The problem is that users must trust that remotely downloaded binaries were built from the sources they are claimed to have been built from, while users in general do not have a trust relation with each other or with the same remote repositories.This paper presents three models that enable sharing: the extensional model that requires that all users on a system have the same remote trust relations, the intensional model that does not have this requirement but may be suboptimal in terms of space use, and the mixed model that merges the best properties of both. The latter two models are achieved through a novel technique of hash rewriting in content-addressable component stores, and were implemented in the context of the Nix system.
19.url:http://doi.acm.org/10.1145/1101908.1101933
19.opinion:exclude

20.title:Automating experimentation on distributed testbeds
20.abstract:Engineering distributed systems is a challenging activity. This is partly due to the intrinsic complexity of distributed systems, and partly due to the practical obstacles that developers face when evaluating and tuning their design and implementation decisions.This paper addresses the latter aspect, providing techniques for software engineers to automate the experimentation activity. Our approach is founded on a suite of models that characterize the distributed system under experimentation, the testbeds upon which the experiments are to be carried out, and the client behaviors that drive the experiments. The models are used by generative techniques to automate construction of the workloads,as well as construction of the scripts for deploying and executing the experiments on distributed testbeds. The framework is not targeted at a specific system or application model, but rather is a generic, programmable tool. We have validated our approach by performing experiments on a variety of distributed systems. For two of these systems, the experiments were deployed and executed on the PlanetLab wide-area testbed.Our experience shows that this framework can be readily applied to different kinds of distributed system architectures,and that using it for meaningful experimentation,especially in large-scale network environments, is advantageous.
20.url:http://doi.acm.org/10.1145/1101908.1101934
20.opinion:exclude

21.title:AMNESIA: analysis and monitoring for NEutralizing SQL-injection attacks
21.abstract:The use of web applications has become increasingly popular in our routine activities, such as reading the news, paying bills, and shopping on-line. As the availability of these services grows, we are witnessing an increase in the number and sophistication of attacks that target them. In particular, SQL injection, a class of code-injection attacks in which specially crafted input strings result in illegal queries to a database, has become one of the most serious threats to web applications. In this paper we present and evaluate a new technique for detecting and preventing SQL injection attacks. Our technique uses a model-based approach to detect illegal queries before they are executed on the database. In its static part, the technique uses program analysis to automatically build a model of the legitimate queries that could be generated by the application. In its dynamic part, the technique uses runtime monitoring to inspect the dynamically-generated queries and check them against the statically-built model. We developed a tool, AMNESIA, that implements our technique and used the tool to evaluate the technique on seven web applications. In the evaluation we targeted the subject applications with a large number of both legitimate and malicious inputs and measured how many attacks our technique detected and prevented. The results of the study show that our technique was able to stop all of the attempted attacks without generating any false positives.
21.url:http://doi.acm.org/10.1145/1101908.1101935
21.opinion:exclude

22.title:Timna: a framework for automatically combining aspect mining analyses
22.abstract:To realize the benefits of Aspect Oriented Programming (AOP), developers must refactor active and legacy code bases into an AOP language. When refactoring, developers first need to identify refactoring candidates, a process called aspect mining. Humans perform mining by using a variety of clues to determine which code to refactor. However, existing approaches to automating the aspect mining process focus on developing analyses of a single program characteristic. Each analysis often finds only a subset of possible refactoring candidates and is unlikely to find candidates which humans find by combining analyses. In this paper, we present Timna, a framework for enabling the automatic combination of aspect mining analyses. The key insight is the use of machine learning to learn when to refactor, from vetted examples. Experimental evaluation of the cost-effectiveness of Timna in comparison to Fan-in, a leading aspect mining analysis, indicates that such a framework for automatically combining analyses is very promising.
22.url:http://doi.acm.org/10.1145/1101908.1101937
22.opinion:exclude

23.title:A parameterized interpreter for modeling different AOP mechanisms
23.abstract:We present a parameterized interpreter for modeling aspect-oriented mechanisms. The interpreter takes several parameters to cover different AOP mechanisms found in AspectJ, Hyper/J, and Demeter. The interpreter helps our understanding of the AOP mechanisms in two ways. First, its core part represents the common mechanical structure shared by different AOP mechanisms. Second, by reconstructing the existing AOP mechanisms and using parameters to configure the interpreter, we can illustrate the differences and similarities of those mechanisms clearly. This will also be helpful in rapid-prototyping a new AOP mechanism or a reflective AOP system that supports different mechanisms.
23.url:http://doi.acm.org/10.1145/1101908.1101938
23.opinion:exclude

24.title:A generic approach to supporting diagram differencing and merging for collaborative design
24.abstract:Differentiation tools enable team members to compare two or more text files, e.g. code or documentation, after change. Although a number of general-purpose differentiation tools exist for comparing text documents very few tools exist for comparing diagrams. We describe a new approach for realising visual differentiation in CASE tools via a set of plug-in components. We have added diagram version control, visual differentiation and merging support as component-based plug-ins to the Pounamu meta-CASE tool. The approach is generic across a wide variety of diagram types and has also been deployed with an Eclipse diagramming plug-in. We describe our approach's architecture, key design and implementation issues, illustrate feasibility of our approach via implementation of it as plug-in components and evaluate its effectiveness.
24.url:http://doi.acm.org/10.1145/1101908.1101940
24.opinion:exclude

25.title:Visualization-based analysis of quality for large-scale software systems
25.abstract:We propose an approach for complex software analysis based on visualization. Our work is motivated by the fact that in spite of years of research and practice, software development and maintenance are still time and resource consuming, and high-risk activities. The most important reason in our opinion is the complexity of many phenomena related to software, such as its evolution and its reliability. In fact, there is very little theory explaining them. Today, we have a unique opportunity to empirically study these phenomena, thanks to large sets of software data available through open-source programs and open repositories. Automatic analysis techniques, such as statistics and machine learning, are usually limited when studying phenomena with unknown or poorly-understood influence factors. We claim that hybrid techniques that combine automatic analysis with human expertise through visualization are excellent alternatives to them. In this paper, we propose a visualization framework that supports quality analysis of large-scale software systems. We circumvent the problem of size by exploiting perception capabilities of the human visual system.
25.url:http://doi.acm.org/10.1145/1101908.1101941
25.opinion:exclude

26.title:Automatic verification of design patterns in Java
26.abstract:Design patterns are widely used by designers and developers for building complex systems in object-oriented programming languages such as Java. However, systems evolve over time, increasing the chance that the pattern in its original form will be broken.To verify that a design pattern has not been broken requires specifying the original intent of the design pattern. Whilst informal descriptions of design patterns exist, no formal specifications are available due to differences in implementations between programming languages.We present a pattern specification language, Spine, that allows patterns to be defined in terms of constraints on their implementation in Java. We also present some examples of patterns defined in Spine and show how they are processed using a proof engine called Hedgehog.The conclusion discusses the type of patterns that are amenable to defining in Spine, and highlights some repeated mini-patterns discovered in the formalisation of these design patterns.
26.url:http://doi.acm.org/10.1145/1101908.1101943
26.opinion:exclude

27.title:Optimized run-time race detection and atomicity checking using partial discovered types
27.abstract:Concurrent programs are notorious for containing errors that are difficult to reproduce and diagnose. Two common kinds of concurrency errors are data races and atomicity violations (informally, atomicity means that executing methods concurrently is equivalent to executing them serially). Several static and dynamic (run-time) analysis techniques exist to detect potential races and atomicity violations. Run-time checking may miss errors in unexecuted code and incurs significant run-time overhead. On the other hand, run-time checking generally produces fewer false alarms than static analysis; this is a significant practical advantage, since diagnosing all of the warnings from static analysis of large codebases may be prohibitively expensive.This paper explores the use of static analysis to significantly decrease the overhead of run-time checking. Our approach is based on a type system for analyzing data races and atomicity. A type discovery algorithm is used to obtain types for as much of the program as possible (complete type inference for this type system is NP-hard, and parts of the program might be untypable). Warnings from the typechecker are used to identify parts of the program from which run-time checking can safely be omitted. The approach is completely automatic, scalable to very large programs, and significantly reduces the overhead of run-time checking for data races and atomicity violations.
27.url:http://doi.acm.org/10.1145/1101908.1101944
27.opinion:exclude

28.title:Reasoning about real-time statecharts in the presence of semantic variations
28.abstract:This paper describes a framework that allows for reasoning about and verification of concurrent statecharts with real-time constraints subject to semantic variations. The major problems addressed by this paper include the capture of multiple semantic variations of real-time statecharts, and the use of the resulting semantics for further analysis. Our solution is based on a theoretical framework involving a two-dimensional temporal logic that is used to independently capture flow of control through statecharts as well as flow of time. The independence of these dimensions, along with the high-level nature of temporal logic allows for simple adaptation to alternate semantics of statecharts as well as real-time models. The paper defines our logic, shows how the semantics of real-time statecharts can be expressed in this formalism, and describes our tools for capturing and reasoning about these semantics. The underlying goal is the formal analysis of real-time software behavior in a way that captures designer intentions.
28.url:http://doi.acm.org/10.1145/1101908.1101945
28.opinion:exclude

29.title:Automated replay and failure detection for web applications
29.abstract:User-session-based testing of web applications gathers user sessions to create and continually update test suites based on real user input in the field. To support this approach during maintenance and beta testing phases, we have built an automated framework for testing web-based software that focuses on scalability and evolving the test suite automatically as the application's operational profile changes. This paper reports on the automation of the replay and oracle components for web applications, which pose issues beyond those in the equivalent testing steps for traditional, stand-alone applications. Concurrency, nondeterminism, dependence on persistent state and previous user sessions, a complex application infrastructure, and a large number of output formats necessitate developing different replay and oracle comparator operators, which have tradeoffs in fault detection effectiveness, precision of analysis, and efficiency. We have designed, implemented, and evaluated a set of automated replay techniques and oracle comparators for user-session-based testing of web applications. This paper describes the issues, algorithms, heuristics, and an experimental case study with user sessions for two web applications. From our results, we conclude that testers performing user-session-based testing should consider their expectations for program coverage and fault detection when choosing a replay and oracle technique.
29.url:http://doi.acm.org/10.1145/1101908.1101947
29.opinion:exclude

30.title:Locating faulty code using failure-inducing chops
30.abstract:Software debugging is the process of locating and correcting faulty code. Prior techniques to locate faulty code either use program analysis techniques such as backward dynamic program slicing or exclusively use delta debugging to analyze the state changes during program execution. In this paper, we present a new approach that integrates the potential of delta debugging algorithm with the benefit of forward and backward dynamic program slicing to narrow down the search for faulty code. Our approach is to use delta debugging algorithm to identify a minimal failure-inducing input, use this input to compute a forward dynamic slice and then intersect the statements in this forward dynamic slice with the statements in the backward dynamic slice of the erroneous output to compute a failure-inducing chop. We implemented our technique and conducted experiments with faulty versions of several programs from the Siemens suite to evaluate our technique. Our experiments show that failure-inducing chops can greatly reduce the size of search space compared to the dynamic slices without significantly compromising the capability to locate the faulty code. We also applied our technique to several programs with known memory related bugs such as buffer overflow bugs. The failure-inducing chop in several of these cases contained only 2 to 4 statements which included the code causing memory corruption.
30.url:http://doi.acm.org/10.1145/1101908.1101948
30.opinion:exclude

31.title:Empirical evaluation of the tarantula automatic fault-localization technique
31.abstract:The high cost of locating faults in programs has motivated the development of techniques that assist in fault localization by automating part of the process of searching for faults. Empirical studies that compare these techniques have reported the relative effectiveness of four existing techniques on a set of subjects. These studies compare the rankings that the techniques compute for statements in the subject programs and the effectiveness of these rankings in locating the faults. However, it is unknown how these four techniques compare with Tarantula, another existing fault-localization technique, although this technique also provides a way to rank statements in terms of their suspiciousness. Thus, we performed a study to compare the Tarantula technique with the four techniques previously compared. This paper presents our study---it overviews the Tarantula technique along with the four other techniques studied, describes our experiment, and reports and discusses the results. Our studies show that, on the same set of subjects, the Tarantula technique consistently outperforms the other four techniques in terms of effectiveness in fault localization, and is comparable in efficiency to the least expensive of the other four techniques.
31.url:http://doi.acm.org/10.1145/1101908.1101949
31.opinion:exclude

32.title:Automated test generation for engineering applications
32.abstract:In test generation based on model-checking, white-box test criteria are represented as trap conditions written in a temporal logic. A model checker is used to refute trap conditions with counter-examples. From a feasible counter-example test inputs are then generated. The major problems of applying this approach to engineering applications derive from the fact that engineering programs have an infinite state space and non-linear numerical computations. Our solution is to combine predicate abstraction (which reduces the state space) with a numerical decision procedure (which supports predicate abstraction by solving non-linear constraints) based on interval analysis. We have developed a prototype and applied it to MC/DC (Modified Condition/Decision Coverage) test case generation. We have used the prototype on a number of C modules taken from a conflict detection and avoidance system and from a Boeing 737 autopilot simulator. The modules range from tens of lines up to thousands of lines in size. Our experience shows that although in theory the inclusion of a decision procedure for non-linear arithmetic may lead to non-terminating behavior and false positives (as abstraction-based model checking already does), our prototype is able to automatically produce feasible counterexamples with only a few exceptions. Furthermore, the process runs with acceptable execution times, without requiring any other knowledge of the specification, and without tampering with the original C programs.
32.url:http://doi.acm.org/10.1145/1101908.1101951
32.opinion:exclude

33.title:Let's agree to disagree
33.abstract:Almost every kind of software development periodically needs to merge models. Perhaps they come from different stakeholders during the requirements analysis phase, or perhaps they are modifications of the same model done independently by several groups of people. Sometimes these models are consistent and can be merged. Sometimes they are not, and negotiation between the stakeholders is needed in order to resolve inconsistencies. While various methods support merging, we need formal approaches that help stakeholders negotiate. We present a formal framework for merging and conflict resolution. It facilitates automatic merging of consistent models, enables users to visualize and explore potential disagreements and identify their priorities, and suggests ways to resolve the priority items.
33.url:http://doi.acm.org/10.1145/1101908.1101952
33.opinion:exclude

34.title:A similarity-aware approach to testing based fault localization
34.abstract:Debugging is a time-consuming task in software development and maintenance. To accelerate this task, several approaches have been proposed to automate fault localization. In particular, testing based fault localization (TBFL), which utilizes the testing information to localize the faults, seem to be very promising. However, the similarity between test cases in the test suite has been ignored in the research on TBFL. In this paper, we investigate this similarity issue and propose a novel approach named similarity-aware fault localization (SAFL), which can calculate the suspicion probability of each statement with little impact by the similarity issue. To address and deal with the similarity between test cases, SAFL applies the theory of fuzzy sets to remove the uneven distribution of the test cases. We also performed an experimental study for two real-world programs at different size levels to evaluate SAFL together with another two approaches to TBFL. Experimental results show that SAFL is more effective than the other two approaches when the test suites contain injected redundancy, and SAFL can achieve a competitive result with normal test suites. SAFL can also be more effective than applying test suite reduction to current approaches to TBFL.
34.url:http://doi.acm.org/10.1145/1101908.1101953
34.opinion:exclude

35.title:Process support to help novices design software faster and better
35.abstract:In earlier work we have argued that formal process definitions can be useful in improving our understanding and performance of software development processes. There has, however, been considerable sentiment that formalized processes cannot capture the creative process of software design. This paper describes our experimentation with the hypothesis that both design speed and design quality can be improved through the use of formalized process definitions. Our experimentation supports this hypothesis.
35.url:http://doi.acm.org/10.1145/1101908.1101954
35.opinion:exclude

36.title:The PLUSS toolkit?: extending telelogic DOORS and IBM-rational rose to support product line use case modeling
36.abstract:The PLUSS approach (Product Line Use case modeling for Systems and Software engineering) is a domain modeling method tailored towards the development of long lived software intensive systems. PLUSS provides means to maintain a common and complete use case model for a whole family of systems. In this paper, we describe how the commercial requirements management tool Telelogic DOORS and the UML modeling tool IBM-Rational Rose can be extended and used for managing system family models in accordance with the PLUSS approach.
36.url:http://doi.acm.org/10.1145/1101908.1101955
36.opinion:exclude

37.title:A strategy for efficient verification of relational specifications, based on monotonicity analysis
37.abstract:We introduce a strategy for the verification of relational specifications based on the analysis of monotonicity of variables within formulas. By comparing with the Alloy Analyzer, we show that for a relevant class of problems this technique drastically outperforms analysis of the same problems using SAT-solvers, while consuming a fraction of the memory SAT-solvers require.
37.url:http://doi.acm.org/10.1145/1101908.1101956
37.opinion:exclude

38.title:Quasi-random testing
38.abstract:Quasi-random sequences, also known as low-discrepancy or low-dispersion sequences, are sequences of points in an n-dimensional unit hypercube. These sequences have the property that points are spread more evenly throughout the cube than random point sequences, which result in regions where there are clusters of points and others that are sparsely populated. Based on the observation that program faults tend to lead to contiguous failure regions within a program's input domain, and that an even spread of random tests enhances the failure detection effectiveness for certain failure patterns, we examine the use of these sequences as a replacement for random sequences in automated testing.The limited number of quasi-random sequences available from the standard algorithms poses significant practical problems for use when testing real programs, and especially for evaluating its effectiveness. We examine the use of randomised quasi-random sequences, which are permuted in a nondeterministic fashion but still retain their low discrepancy properties, to overcome this problem, and show that testing using randomised quasi-random sequences is often significantly more effective than random testing.
38.url:http://doi.acm.org/10.1145/1101908.1101957
38.opinion:exclude

39.title:Constraint-based test data generation in the presence of stack-directed pointers
39.abstract:Constraint-Based Test data generation (CBT) exploits constraint satisfaction techniques to generate test data able to kill a given mutant or to reach a selected branch in a program. When pointer variables are present in the program, aliasing problems may arise and may lead to the failure of current CBT approaches. In our work, we propose an overall CBT method that exploits the results of an intraprocedural points-to analysis and provides two specific constraint combinators for automatically generating test data able to reach a selected branch. Our approach correctly handles multi-levels stack-directed pointers that are mainly used in real-time control systems. The method has been fully implemented in the test data generation tool INKA and first experiences in applying it to a variety of existing programs tend to show the interest of the approach.
39.url:http://doi.acm.org/10.1145/1101908.1101958
39.opinion:exclude

40.title:QoS-aware dynamic service composition in ambient intelligence environments
40.abstract:Due to the large success of wireless networks and handheld devices, the ambient intelligence (AmI) paradigm is becoming a reality. One of the most challenging objectives to achieve in AmI environments is to enable a user to perform a task by composing on the fly networked services available at a specific time and place. Towards this goal, we propose a solution based on semantic Web services, and we show how service capabilities described as conversations can be integrated to perform a user task that is also described as a conversation, further meeting the QoS requirements of the user task. Experimental results show that the runtime overhead of our algorithm is reasonable, and further, that QoS-awareness improves its performance.
40.url:http://doi.acm.org/10.1145/1101908.1101959
40.opinion:exclude

41.title:A visual language and environment for composing web services
41.abstract:Implementing complex web service-based systems requires tools to effectively describe and co-ordinate the composition of web service components. We have developed a new domain-specific visual language called ViTABaL-WS and built a prototype design tool to support modelling complex interactions between web service components. ViTABaL-WS uses a "Tool Abstraction" metaphor for describing relationships between service definitions, and multiple-views of data-flow, control-flow and event propagation in a modelled process. The tool supports the generation of Business Process Execution Language (BPEL) definitions from a model, directly deploys a generated model to a workflow engine, and supports dynamic visualisation of a running BPEL process.
41.url:http://doi.acm.org/10.1145/1101908.1101960
41.opinion:exclude

42.title:Learning to verify branching time properties
42.abstract:We present a new model checking algorithm for verifying computation tree logic (CTL) properties. To our knowledge, this is the first CTL model checking algorithm for infinite state systems that can also handle fairness constraints. Our technique is based on using language inference to learn the fixpoints necessary for checking a CTL formula instead of computing them iteratively as is done in traditional model checking. This allows us to analyze infinite or large state-space systems where the traditional iterations may not converge or may take too long to converge. Our procedure is guaranteed to terminate with the correct answer if fixpoints needed for all subformulas of the CTL property are regular. We have extended our LEVER tool to use the technique presented in this paper and demonstrate its effectiveness by verifying a number of parametric and integer systems.
42.url:http://doi.acm.org/10.1145/1101908.1101961
42.opinion:exclude

43.title:Simon: modeling and analysis of design space structures
43.abstract:The structure of the coupling relation on design decisions is a key factor influencing the evolvability properties and the economic value of a design. The work of Baldwin and Clark is an important step toward a theory of the relationship between structure and value. A key step to enabling rigorous validation and perhaps the eventual use of their ideas for software engineering is formalization of their model. In this paper, we present a brief overview of such a formal model and a prototype software tool, Simon, implementing it. We present Simon's functions for deriving design structure matrices and computing impacts of changes in design decisions, and we sketch an initial experimental evaluation in the form of a replication study of our earlier analysis of Parnas's 1972 paper on information hiding modularity.
43.url:http://doi.acm.org/10.1145/1101908.1101962
43.opinion:exclude

44.title:Lattice-based adaptive random testing
44.abstract:Adaptive Random Testing (ART) denotes a family of testing algorithms that have a better performance compared to pure random testing with respect to the number of test cases necessary to detect the first failure. Many of these algorithms, however, are not very efficient regarding runtime. A new ART algorithm is presented that has a better performance than all other ART methods for the block failure pattern. Its runtime is linear in the number of test cases selected, which is nearly as efficient as pure random testing, as opposed to most other ART methods. This new ART algorithm selects the test cases based on a lattice.
44.url:http://doi.acm.org/10.1145/1101908.1101963
44.opinion:exclude

45.title:A unified fitness function calculation rule for flag conditions to improve evolutionary testing
45.abstract:Evolutionary testing (ET), automatically generating test data with good quality, is an effective technique based on evolutionary algorithm. However, the presence of flag variables will make it degenerate to random testing in structural testing. Much of previous work has addressed this problem, but all can be characterized as program-specific. In this paper, flag cost function is introduced as the main component of fitness function, whose value changes with the variation of flag problem. Based on this, a unified fitness calculation rule for flag conditions is proposed. The experiments on programs with flag problems, once considered as inextricable in previous work, and the Traffic Alert and Collision Avoidance System (TCAS) code showed the effectiveness of our unified approach.
45.url:http://doi.acm.org/10.1145/1101908.1101964
45.opinion:exclude

46.title:A threat-driven approach to modeling and verifying secure software
46.abstract:This paper presents a formal approach to threat-driven modeling and verification of secure software using aspect-oriented Petri nets. Based on the behavior model of intended functions, we identify and build formal models of security threats, which are potential misuses and anomalies of the intended functions that violate security goals. Threat mitigations are further modeled in an aspect-oriented paradigm. Taking Petri nets as a formal basis for modeling behaviors, threats, and mitigations as a whole, we verify properties of and consistency between behaviors and threats, and absence of identified threats from the integrated model of functions and threat mitigations. This makes it possible to achieve a design that is provably resistant to the anticipated threats and thus reduce significant design-level vulnerabilities.
46.url:http://doi.acm.org/10.1145/1101908.1101965
46.opinion:exclude

47.title:Automated path generation for software fault localization
47.abstract:Localizing the cause(s) of an observable error lies at the heart of program debugging. Fault localization often proceeds by comparing the failing program run with some "successful" run (a run which does not demonstrate the error). An issue here is to generate or choose a "suitable" successful run; this task is often left to the programmer. In this paper, we present an efficient technique where the construction of the successful run as well its comparison with the failing run is automated. Our method constructs a successful program run by toggling the outcomes of some conditional branch instances in the failing run. If such a successful run exists, program statements for these branches are returned as bug report. In our experiments with the Siemens benchmark suite, we found that the quality of our bug report compares well with those produced by existing fault localization approaches where the programmer manually provides or chooses a successful run.
47.url:http://doi.acm.org/10.1145/1101908.1101966
47.opinion:exclude

48.title:EA-Miner: a tool for automating aspect-oriented requirements identification
48.abstract:Aspect-Oriented requirements engineering helps to achieve early separation of concerns by supporting systematic analysis of broadly-scoped properties such as security, real-time constraints, etc. The early identification and separation of aspects and base abstractions crosscut by them helps to avoid costly refactorings at later stages such as design and code. However, if not handled effectively, the aspect identification task can become a bottleneck requiring a significant effort due to the large amount of, often poorly structured or imprecise, information available to a requirements engineer. In this paper, we describe a tool, EA-Miner, that provides effective automated support for identifying and separating aspectual and non-aspectual concerns as well as their crosscutting relationships at the requirements level. The tool utilises natural language processing techniques to reason about the properties of the concerns and model their structure and relationships.
48.url:http://doi.acm.org/10.1145/1101908.1101967
48.opinion:exclude

49.title:yagg: an easy-to-use generator for structured test inputs
49.abstract:Automated testing typically uses specifications to drive the generation of test inputs and/or the checking of program outputs. Many software systems have structurally complex inputs that cannot be adequately described using simple formalisms such as context-free grammars. In order to generate such inputs, many automated testing environments require the user to express the structure of the input using an unfamiliar formal notation. This raises the cost of employing automated testing, thereby offsetting the benefits gained. We present yagg (yet another generator-generator), a tool that allows the programmer to specify the input using a syntax very similar to that of LEX and YACC, widely used scanner and parser generators. yagg allows the user to bound the input space using several different techniques, and generates an input generator that systematically enumerates inputs. We evaluate the ease of use and performance of the tool relative to a model checker-based generator used in previous research. Our experiences indicate that yagg generators can be somewhat slower, but that the ease-of-use afforded by the familiar syntax may be attractive to users.
49.url:http://doi.acm.org/10.1145/1101908.1101969
49.opinion:exclude

50.title:Determining the cost-quality trade-off for automated software traceability
50.abstract:Major software development standards mandate the establishment of trace links among software artifacts such as requirements, architectural elements, or source code without explicitly stating the required level of detail of these links. However, the level of detail vastly affects the cost and quality of trace link generation and important applications of trace analysis such as conflict analysis, consistency checking, or change impact analysis. In this paper, we explore these cost-quality trade-offs with three case study systems from different contexts - the open-source ArgoUML modeling tool, an industrial route-planning system, and a movie player. We report the cost-quality trade-off of automated trace generation with the Trace Analyzer approach and discuss its expected impact onto several applications that consume its trace information. In the study we explore simple techniques to predict and manipulate the cost-benefit trade-off with threshold-based filtering. We found that (a) 80% of the benefit comes from only 20% of the cost and (b) weak trace links are predominantly false trace links and can be efficiently eliminated through thresholds.
50.url:http://doi.acm.org/10.1145/1101908.1101970
50.opinion:exclude

51.title:A uniform deductive approach for parameterized protocol safety
51.abstract:We present a uniform verification method of safety properties for classes of parameterized protocols. Properties like mutual exclusion or cache coherence are automatically verified for any number of similar processes communicating by broadcast and rendezvous. The protocols are specified in a language of generalized substitutions on array data structures. Sets of states are expressed by first-order formulae with equality. Predecessors are computed by an iterative semi-algorithm. Reaching an initial state or the fixpoint is shown to be decidable and an original decision procedure is provided. As a running example, the MESI protocol illustrates this approach. Experimental results show its applicability to various properties and protocol classes.
51.url:http://doi.acm.org/10.1145/1101908.1101971
51.opinion:exclude

52.title:A component-based development framework for supporting functional and non-functional analysis in control system design
52.abstract:The use of component-based development (CBD) is growing in the software engineering community and it has been successfully applied in many engineering domains such as office applications and in web-based distributed applications. Recently, the need of CBD is growing also in other domains related to dependable and embedded systems, namely, in the control engineering domain. However, the widely used commercial component technologies are unable to provide solutions to the requirements of embedded systems as they require too much resources and they do not provide methods and tools for developing predictable and analyzable embedded systems. There is a need for new component-based technologies appropriate to development of embedded systems. In this paper we briefly present a component-based development framework called SAVEComp. SAVEComp is developed for safety-critical real-time systems. One of the main characteristics of SAVEComp is syntactic and semantic simplicity which enables a high analyzability of properties important for embedded systems. We discuss how SAVEComp is able to provide an efficient support for designing and implementing embedded control systems by mainly focusing on simplicity and analyzability of functional requirements and of real-time and dependability quality attributes. In particular we discuss the typical solutions of control systems in which feedback loops are used and which significantly complicate the design process. We provide a solution for increasing design abstraction level and still being able to reason about system properties using SAVEComp approach. Finally, we discuss an extension of SAVEComp with dynamic run-time property checking by utilizing run-time spare capacity that is normally induced by real-time analysis.
52.url:http://doi.acm.org/10.1145/1101908.1101972
52.opinion:exclude

53.title:A rigorous approach for proving model refactorings
53.abstract:Both model and program refactorings are usually proposed in an ad hoc way because it is difficult to prove that they are sound with respect to a formal semantics. In this paper, we propose guidelines on how to rigorously prove model refactorings for Alloy, a formal object-oriented modeling language. We use the Prototype Verification System (PVS) to specify and prove the soundness of the transformations. Proposing refactorings in this way can facilitate not only design, but also improve the quality of refactoring tools.
53.url:http://doi.acm.org/10.1145/1101908.1101973
53.opinion:exclude

54.title:Compositional reasoning for port-based distributed systems
54.abstract:Many distributed systems using IP-based communication protocols consist of chains of components that run concurrently and communicate asynchronously with their neighbours through ports. We present a compositional reasoning method using model checking and theorem proving to verify liveness properties of a communication protocol for chains of connections consisting of an unknown number of components. We outline how our method is used to verify properties of the call protocol of AT&T's Distributed Feature Composition (DFC) architecture.
54.url:http://doi.acm.org/10.1145/1101908.1101974
54.opinion:exclude

55.title:Model-based self-monitoring embedded programs with temporal logic specifications
55.abstract:We propose a model-based framework for developing self-monitoring embedded programs with temporal logic specifications. In our framework the requirement specification of an embedded program is encoded in the temporal logic MEDL. We propose an algorithm that synthesizes a model-based monitor from a MEDL script. We also introduce a technique that instruments a system model to emit events defined in the model-based primitive event definition language mPEDL. The synthesized model-based monitor may be composed with the instrumented model to form a self-monitoring model, which can be simulated for design-level verification; the composed self-monitoring model can also be used to generate a self-monitoring embedded program, which can monitor its own execution on the target platform in addition to its normal functions. Our approach combines the rigidness of temporal logic specifications with the easy use of a toolkit M2IST that we developed to automate the process of building a self-monitoring embedded program from a system model and its requirement specification.
55.url:http://doi.acm.org/10.1145/1101908.1101975
55.opinion:exclude

56.title:Specialization and extrapolation of software cost models
56.abstract:Despite the widespread availability of software effort estimation models (e.g. COCOMO [2], Price-S [12], SEER-SEM [13], SLIM [14]), most managers still estimate new projects by extrapolating from old projects [3, 5, 7]. In this delta method, the cost of the next project is the cost of the last project multiplied by some factors modeling the difference between old and new projects [2].Delta estimation is simple, fast, and best of all, can take full advantage of local costing information. However delta estimation fails when the experience base (the old projects) can not be extrapolated to the new projects. Previously [10], we have shown that for a set of NASA projects, delta estimation would usually fail since most of the features and coefficients of the learned model vary wildly across sub-samples of the training data. In that prior work, no solution was offered for this problem.Here, we offer a solution and report the results of experiment with feature subset selection (FSS) and extrapolation. FSS methods are usually assessed via the mean change in model performance. However, as shown below, FSS can significantly reduce the variance as well. Hence, FSS should be routinely used in cost estimation.Our results should stop the trend in the effort modeling community of continually adding to the number of features in a model in order to improve estimation performance. Here we show that there are benefits in intelligently subtracting model features.
56.url:http://doi.acm.org/10.1145/1101908.1101976
56.opinion:exclude

57.title:NFRs-aware architectural evolution of component-based software
57.abstract:During software maintenance, some non-functional properties may be lost. This is due to the lack of an explicit definition of their links with the corresponding architectural choices. In this paper, we present a solution that automates the checking of non-functional properties after the evolution of a component-based software. Our approach emphasizes the interest of formally documenting the links binding non-functional requirements to architectural choices. The proposed formalism is based on the Object Constraint Language (OCL) applied to a software component metamodel. We also present a prototype tool which uses this documentation to warn the developer of possible effects of an architectural change on non-functional requirements.
57.url:http://doi.acm.org/10.1145/1101908.1101977
57.opinion:exclude

58.title:Code security analysis with assertions
58.abstract:Designing and implementing cryptographic protocols is known to be difficult. A lot of research has been devoted to develop formal techniques to analyze abstract designs of cryptographic protocols. Less attention has been paid to the verification of implementation-relevant aspects of cryptographic protocols. This is an important challenge since it is non-trivial to securely implement secure designs, because a specification by its nature is more abstract than the corresponding implementation, and the additional information may introduce attacks not present on the design level. We propose an approach to determine security goals provided by a protocol implementation based on control flow graphs and automated theorem provers for first-order logic. More specifically, here we explain how to make use of assertions in the source code for a practical and efficient security analysis.
58.url:http://doi.acm.org/10.1145/1101908.1101978
58.opinion:exclude

59.title:Data mining and cross-checking of execution traces: a re-interpretation of Jones, Harrold and Stasko test information
59.abstract:The current trend in debugging and testing is to cross-check information collected during several executions. Jones et al., for example, propose to use the instruction coverage of passing and failing runs in order to visualize suspicious statements. This seems promising but lacks a formal justification. In this paper, we show that the method of Jones et al. can be re-interpreted as a data mining procedure. More particularly, they define an indicator which characterizes association rules between data. With this formal framework we are able to explain intrinsic limitations of the above indicator.
59.url:http://doi.acm.org/10.1145/1101908.1101979
59.opinion:exclude

60.title:Properties and scopes in web model checking
60.abstract:We consider a formal framework for property verification of web applications using Spin model checker. Some of the web related properties concern all states of the model, while others - only a proper subset of them. To be able to discriminate states of interest in the state space, we solve the problem of property specification in LTL over a subset of states of a system under test while ignoring the valuation of the properties in the rest of them. We introduce specialized operators that facilitate specifying properties over propositional scopes, where each scope constitutes a subset of states that satisfy a propositional logic formula. Using the proposed operators, the user can specify web properties more concisely and intuitively. We illustrate the proposed solution in specifying properties of web applications and discuss other potential applications.
60.url:http://doi.acm.org/10.1145/1101908.1101980
60.opinion:exclude

61.title:Synthesis of correct and distributed adaptors for component-based systems: an automatic approach
61.abstract:Building a distributed system from third-party components introduces a set of problems, mainly related to compatibility and communication. Our approach to solve these problems is to build an adaptor which forces the system to exhibit only a set of safe or desired behaviors. By exploiting an abstract and partial specification of the global behavior that must be enforced, we automatically build a centralized adaptor. It mediates the interaction among components by both performing the specified behavior and, simultaneously, avoiding possible deadlocks. However in a distributed environment it is not always possible or convenient to insert a centralized adaptor. In contrast, building a distributed adaptor might increase the applicability of the approach in a real-scale context. In this paper we show how it is possible to automatically generate a distributed adaptor by exploiting an approach to the definition of distributed IDS (Intrusion Detection Systems) filters developed by us to increase security measures in component based systems. Firstly, by taking into account a high level specification of the global behavior that must be enforced, we synthesize a behavioral model of a centralized adaptor that allows the composed system to only exhibit the specified behavior and, simultaneously, avoid possible unspecified deadlocks. This model represents a lower level specification of the global behavior that is enforced by the adaptor. Secondly, by taking into account the synthesized adaptor model, we generate a set of component filters that validate the centralized adaptor behavior by simply looking at local information. In this way we address the problem of mechanically generating correct and distributed adaptors for real-scale component-based systems.
61.url:http://doi.acm.org/10.1145/1101908.1101981
61.opinion:exclude

62.title:A context-sensitive structural heuristic for guided search model checking
62.abstract:In this paper we build on the FSM distance heuristic for guided model checking by using the runtime stack to reconstruct calling context in procedural calls. We first build a more accurate static representation of the program by including a bounded level of calling context. We then use the calling context in the runtime stack with the more accurate control flow graph to estimate the distance to the possible error state. The heuristic is computed using both the dynamic and static construction of the program. We evaluate the new heuristic on models with concurrency errors. In these examples, experimental results show that for programs with function calls, the new heuristic better guides the search toward the error while the traditional FSM distance heuristic degenerates into a random search.
62.url:http://doi.acm.org/10.1145/1101908.1101982
62.opinion:exclude

63.title:Test input generation for red-black trees using abstraction
63.abstract:We consider the problem of test input generation for code that manipulates complex data structures. Test inputs are sequences of method calls from the data structure interface. We describe test input generation techniques that rely on state matching to avoid generation of redundant tests. Exhaustive techniques use explicit state model checking to explore all the possible test sequences up to predefined input sizes. Lossy techniques rely on abstraction mappings to compute and store abstract versions of the concrete states; they explore under-approximations of all the possible test sequences. We have implemented the techniques on top of the Java PathFinder model checker and we evaluate them using a Java implementation of red-black trees.
63.url:http://doi.acm.org/10.1145/1101908.1101983
63.opinion:exclude

64.title:Testing in resource constrained execution environments
64.abstract:Software for resource constrained embedded devices is often implemented in the Java programming language because the Java compiler and virtual machine provide enhanced safety, portability, and the potential for run-time optimization. It is important to verify that a software application executes correctly in the environment in which it will normally execute, even if this environment is an embedded one that severely constrains memory resources. Testing can be used to isolate defects within and establish a confidence in the correctness of a Java application that executes in a resource constrained environment. However, executing test suites with a Java virtual machine (JVM) that uses dynamic compilation to create native code bodies can introduce significant testing time overheads if memory resources are highly constrained. This paper describes an approach that uses adaptive code unloading to ensure that it is feasible to perform testing in the actual memory constrained execution environment. The experiments demonstrate that code unloading can reduce both the test suite execution time by 34% and the code size of the test suite and application under test by 78% while maintaining the overall size of the JVM.
64.url:http://doi.acm.org/10.1145/1101908.1101984
64.opinion:exclude

65.title:Prufrock: a framework for constructing polytypic theorem provers
65.abstract:Current formal software engineering methodologies provide a vast array of languages for specifying correctness properties, as well as a wide assortment automated tools that aid in the verification of specified properties. Unfortunately, the implementation of each such tool requires an early commitment to a particular methodology and language, in terms of both high-level semantic concerns and the lower-level syntactic representations of properties and proofs. In this paper, we present Prufrock, a novel approach to automated reasoning systems, which abstracts semantic concerns over entire classes of potential implementation languages. Prufrock utilizes polytypic programming techniques to create independent, reusable modules defining proof in different logics, independent of the language used to represent formulae in the logic, as well as the exact implementation of low-level prover functionality.
65.url:http://doi.acm.org/10.1145/1101908.1101985
65.opinion:exclude

66.title:Bamboo: an architecture modeling and code generation framework for configuration management systems
66.abstract:We describe an architecture modeling and code generation framework called Bamboo. Using Bamboo, engineers design SCM repository and feature models, and then generate a running SCM system from the models.
66.url:http://doi.acm.org/10.1145/1101908.1101987
66.opinion:exclude

67.title:Using communicative acts in high-level specifications of user interfaces for their automated synthesis
67.abstract:User interfaces are very important for the success of many computer-based applications these days. However, their development takes time, requires experts for user-interface design as well as experienced programmers and is very expensive. This problem becomes even more severe through the ubiquitous use of a variety of devices such as PCs, mobile phones, PDAs etc., since each of these devices has its own specifics that require a special user interface.Therefore, we developed a tool-supported approach to automatically synthesize multi-device user interfaces from high-level specifications in the form of models. In contrast to previous approaches focusing on abstracting the user interface per se, we make use of communicative acts derived from speech act theory for the specification of desired user intentions in interactions. In this way, we approach a solution to the given problem, since user interfaces can be efficiently provided without experience in implementing them.
67.url:http://doi.acm.org/10.1145/1101908.1101988
67.opinion:exclude

68.title:A tool for automatic UML model consistency checking
68.abstract:Automated consistency checking of UML models becomes necessary as models grow in size and complexity. Because the UML metamodel does not enforce model consistency, there are no guidelines as how to approach the consistency problem. Current solutions are partial and tools are mostly of academic nature. The translation of the metamodel and the user designed model into Description Logics has proved to be useful in detecting a large set of inconsistencies. We present MCC, a UML model consistency checker, built as a plug-in for Poseidon for UML, and relying on Racer as a reasoning engine. We propose a usable and scalable solution, interoperable with a known modeling tool.
68.url:http://doi.acm.org/10.1145/1101908.1101989
68.opinion:exclude

69.title:Automated population of causal models for improved software risk assessment
69.abstract:Recent work in applying causal modeling (Bayesian networks) to software engineering has resulted in improved decision support systems for software project managers. Once the causal models are built there are commercial tools that can run them. However, data to populate the models is typically entered manually and this is an impediment to their more widespread use. Hence, here we present a prototype tool for automatically extracting a range of relevant software metrics from popular project management and CASE tools. This information is used to populate Bayesian networks with the aim of providing better real world predictions of the risks associated with software costs, timescales and reliability.
69.url:http://doi.acm.org/10.1145/1101908.1101990
69.opinion:exclude

70.title:Introduction to doctoral symposium
70.abstract:An abstract is not available.
70.url:http://doi.acm.org/10.1145/1101908.1101992
70.opinion:exclude

71.title:Automated generation of testing tools for domain-specific languages
71.abstract:Domain-specific languages (DSLs) assist a domain expert (or end-user) in writing a software program using idioms that are closer to the abstractions found in a specific problem domain. Language tool support for DSLs is lacking, however, when compared to the capabilities provided for standard general purpose languages (e.g., Java and C++). For example, support for debugging and testing a program written in a DSL is often nonexistent. A DSL grammar serves as the primary artifact for defining DSLs from a higher level of abstraction. This paper describes an investigation into a grammar-driven technique to build a testing tool from existing DSL grammars. The DSL grammars are used to generate the hooks needed to interface with a supporting infrastructure written for an Integrated Development Environment (IDE) that assists in debugging and testing a program written in a DSL. We describe a DSL framework that we have implemented to generate DSL testing tools (e.g., debugger and test engine). This framework demonstrates the feasibility and applicability of using the information derived from DSL grammars and existing software components and services to support end-user debugging and testing in a domain friendly programming environment.
71.url:http://doi.acm.org/10.1145/1101908.1101993
71.opinion:exclude

72.title:Constructing interaction test suites with greedy algorithms
72.abstract:Combinatorial approaches to testing are used in several fields, and have recently gained momentum in the field of software testing through software interaction testing. One-test-at-a-time greedy algorithms are used to automatically construct such test suites. This paper discusses basic criteria of why greedy algorithms have been appropriate for this test generation problem in the past and then expands upon how greedy algorithms can be utilized to address test suite prioritization.
72.url:http://doi.acm.org/10.1145/1101908.1101994
72.opinion:exclude

73.title:Verifying the correctness of hume programs: an approach combining deductive and algorithmic reasoning
73.abstract:Hume is a programming language targeted at safety-critical, resource-bounded systems. Bounded time and space usage is achieved by a clear separation of coordination and computation in the design of the language. However, there is no correctness verification. Such verification is imperative in safety-critical environments. It is our contention that the language design makes a combination of deductive and algorithmic reasoning tractable.
73.url:http://doi.acm.org/10.1145/1101908.1101995
73.opinion:exclude

74.title:A model transformation approach to automatic model construction and evolution
74.abstract:As models are elevated to first-class artifacts within the software development lifecycle, the task of construction and evolution of large-scale system models becomes a manually intensive effort that can be very time consuming and error prone. To address these problems, this dissertation abstract presents a model transformation approach. There are three main features of this research. First, tasks of model construction and evolution are specified in a model transformation language (called the Embedded Constraint Language). Second, a core transformation engine (called C-SAW) is used to perform model transformation in an automated manner by executing the ECL transformation specification. Finally, testing and debugging tools at the modeling level are provided to assist in detecting errors in the model transformation.
74.url:http://doi.acm.org/10.1145/1101908.1101996
74.opinion:exclude

75.title:In regression testing selection when source code is not available
75.abstract:An abstract is not available.
75.url:http://doi.acm.org/10.1145/1101908.1101997
75.opinion:exclude

76.title:Formal support for merging and negotiation
76.abstract:Model merging is an important activity in software development. We often need to integrate a set of models coming from different sources so as to create a unified model encompassing all the given models. Inconsistencies between models can make model merging significantly more complex. To deal with inconsistencies efficiently, a systematic negotiation process is needed. This paper outlines a formal approach to merging and negotiation over behavioural models and presents the results achieved so far.
76.url:http://doi.acm.org/10.1145/1101908.1101998
76.opinion:exclude

77.title:Specification and automated processing of security requirements (SAPS'05)
77.abstract:The first edition of SAPS (in 2004) focused on the development of tools to automate software engineering processes with support for security. Our focus in this edition will be on the tools to automate processing, validation and monitoring of security requirements, both during development and during operation of the system. Security and reliability issues are rarely considered at the initial stages of software development. The erroneous consideration of security technology as supplementary, and the lack of integration of security engineering techniques within software engineering processes have very negative consequences. Approaches integrating security issues in software engineering processes are especially relevant to SAPS. Good security measures can fail due to errors in design or implementation. Therefore, automated tools are essential for the analysis and deployment of secure systems. Comprehensive approaches, encompassing all phases of development are encouraged. Furthermore, in the near future, the increasing dynamism, heterogeneity and complexity of emerging computing paradigms and environments such as grid computing, mixed-mode systems or ambient intelligence, along with the disappearing notion of system boundaries introduced by these paradigms, will make it impossible for security engineers to foresee all possible situations that may arise during system operation, therefore increasing the need for automated support for the processing of security.
77.url:http://doi.acm.org/10.1145/1101908.1102000
77.opinion:exclude

78.title:Software security assurance tools, techniques and metrics (SSATTM)
78.abstract:The purpose of the workshop is to convene researchers, developers, and government and industrial users of software security assurance (SSA) tools to refine the taxonomy of flaws and the taxonomy of SSA tool functions, converge on which SSA functions should first have specifications and tests developed, gather SSA tool developers for "target practice" on the reference datasets, and identify gaps or requirements for research in SSA functions. There are contributions describing basic research, novel applications, and experience relevant to SSA tools and their evaluation. The reference datasets are code with known flaws and vulnerabilities, with corresponding correct versions, to be used as references for tool testing, to make research easier, and to be a standard of evaluation. Tools ranging from commercial products to university projects "shoot holes" in the datasets to suggest extensions, improvements, etc. This is a U.S. National Institute of Standards and Technology SAMATE (http://samate.nist.gov/) workshop.
78.url:http://doi.acm.org/10.1145/1101908.1102001
78.opinion:exclude

79.title:3rd international workshop on traceability in emerging forms of software engineering (TEFSE 2005)
79.abstract:Establishing and maintaining traceability links and consistency between software artifacts produced or modified in the software life-cycle are costly and tedious activities that are crucial but frequently neglected in practice. Traceability between the free text documentation associated with the development and maintenance cycle of a software system and its source code are crucial in a number of tasks such as program comprehension, software maintenance, and software verification & validation. Finally, maintaining traceability links between subsequent releases of a software system is important for evaluating relative source code deltas, highlighting effort/code variation inconsistencies, and assessing the change history. The main theme of the workshop is focused on understanding and defining the foundations for consistency and change management of software systems within the scope of artifact-to-artifact (model-to-model) traceability.The workshop will address the following issues: A formal definition of model to model traceability Traceability between artifacts and processes The semantics of traceability links Recovery of traceability links Visualization of traceability links Interoperable approaches to support traceability Traceability in emerging forms of software engineering including production lines, frameworks, components, etc..The goals of the workshop are to: Broaden awareness within the software engineering community of the potential for the application of traceability Facilitate the exchange of ideas and interaction between international researchers Define open research problems faced in realizing usable approaches for traceability Construct a foundation of materials for future research on traceability .For more information please visit the workshop web site is: http://re.cs.depaul.edu/tefse05/. The workshop proceedings are available through the ACM digital library.
79.url:http://doi.acm.org/10.1145/1101908.1102002
79.opinion:exclude

80.title:Software certificate management (SoftCeMent'05)
80.abstract:The goal of this workshop is to explore new technologies, underlying principles, and general methodologies for supporting software certificate management. Software certification demonstrates the reliability, safety, or security of software systems in such a way that it can be checked by an independent authority with minimal trust in the techniques and tools used in the certification process itself. It can build on existing validation and verification (V&V) techniques but introduces the notion of explicit software certificates, which contain all the information necessary for an independent assessment of the demonstrated properties. Software certificates support a product-oriented assurance approach, combining different techniques and forms of evidence (e.g., fault trees, "sign-offs", safety cases, formal proofs, ...) and linking them to the details of the underlying software. A software certificate management system provides the infrastructure to create, maintain, and analyze software certificates. It combines functionalities of a database (e.g., storing and retrieving certificates) and a make-tool (e.g., incremental re-certification). It can also maintain links between system artifacts (e.g., design documents, engineering data sets, or programs) and different varieties of certificates, check the validity of certificates, provide access to explicit audit trails, enable browsing of certification histories, and enforce system-wide certification and release policies. It can at any time provide current information about the certification status of each component in the system, check whether certificates have been audited, compute which certificates remain valid after a system modification, or even automatically start an incremental recertification.
80.url:http://doi.acm.org/10.1145/1101908.1102003
80.opinion:exclude

81.title:2nd Workshop on the state of the art in automated software engineering
81.abstract:The 2nd Workshop on the State of the Art in Automated Software Engineering took place on the 27 th of July 2005, at City University, London - UK. During the workshop there were seven presentations about various aspects concerning the theory and practice of automating the software engineering process. The presentations generated interesting discussions among the participants of the workshop. The title, name of the speaker, and abstract of each presentation are given below in the order that they were presented during the workshop. We would like to take this opportunity to thank all the presenters and participants of the workshop who helped make the day possible and interesting. [2] S. Ishtiaq, P. O'Hearn, BI as an Assertion Language for Mutable Data Structures, In the Proceedings of the 28th Annual ACM Symposium on Principles of Programming Languages, ACM, 2001. [3] J. Reynolds, Separation Logic: A Logic for Shared Mutable Data Structures, In the Proceedings of the 17th Annual IEEE Symposium on Logic in Computer Science, IEEE, 2002. Recent Evolutions of the Tobias Combinatoria.
81.url:http://doi.acm.org/10.1145/1101908.1102004
81.opinion:exclude

