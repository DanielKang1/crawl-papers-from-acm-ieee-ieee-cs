1.title:"title":"A Petri Net Based Debugging Environment for QVT Relations"
1.abstract:"abstract":"In the Model-Driven Architecture (MDA) paradigm the Query/View/Transformation (QVT) standard plays a vital role for model transformations. Especially the high-level declarative QVT Relations language, however, has not yet gained widespread use in practice. This is not least due to missing tool support in general and inadequate debugging support in particular. Transformation engines interpreting QVT Relations operate on a low level of abstraction, hide the operational semantics of a transformation and scatter metamodels, models, QVT code, and trace information across different artifacts. We therefore propose a model-based debugger representing QVT Relations on bases of TROPIC, a model transformation language utilizing a variant of Colored Petri Nets (CPNs). As a prerequisite for convenient debugging, TROPIC provides a homogeneous view on all artifacts of a transformation on basis of a single formalism. Besides that, this formalism also provides a runtime model, thus making the afore hidden operational semantics of the transformation explicit. Using an explicit runtime model allows to employ model-based techniques for debugging, e.g., using the Object Constraint Language (OCL) for simply defining breakpoints and querying the execution state of a transformation."
1.url:http://dx.doi.org/10.1109/ASE.2009.99
1.opinion:exclude

2.title:"title":"Validating Automotive Control Software Using Instrumentation-Based Verification"
2.abstract:"abstract":"This paper discusses the results of an application of a formally based verification technique, called Instrumentation-Based Verification (IBV), to a production automotive lighting controller. The goal of the study is to assess, from both a tools as well as a methodological perspective, the performance of IBV in an industrial setting. The insights obtained as a result of the project include a refinement of a previously developed architecture for requirements specifications; observations about changes to model-based design workflows; insights into the role of requirements during development; and the capability of automated verification to detect inconsistencies among requirements as well as between requirements and design models."
2.url:http://dx.doi.org/10.1109/ASE.2009.98
2.opinion:exclude

3.title:"title":"Semi-automated Test Planning for e-ID Systems by Using Requirements Clustering"
3.abstract:"abstract":"In acceptance testing, customer requirements as specified in system specifications have to be tested for their successful implementation. This is a time-consuming task due to inherent system complexity and thus a large number of requirements. In order to reduce efforts in acceptance testing, we introduce a novel approach that exploits redundancies and implicit relations in requirements specifications, which are based on multi-viewpoint techniques, in our case the reference model for open distributed processing (RM-ODP). It deploys requirements clustering and linguistic analysis techniques for reducing the total number of test cases. We report on concrete experiences with this approach within joint R&#x00026;D work of the Software Quality Lab (s-lab) of the University of Paderborn and HJP Consulting, an international consulting company, specialized in planning, procurement and acceptance testing of national electronic identification (e-ID) systems. The paper is concluded with an overview on the current tool support especially for automated detection of the redundancies and implicit relations in requirements. Also the future work on the tool support for the overall test specification process is discussed."
3.url:http://dx.doi.org/10.1109/ASE.2009.86
3.opinion:exclude

4.title:"title":"A Quantum Algorithm for Software Engineering Search"
4.abstract:"abstract":"Quantum computers can solve a few basic problems, such as factoring an integer and searching a database, much faster than classical computers. However, the complexity of software artifacts, and the types of questions software engineers ask about them, pose significant challenges for applying existing quantum approaches to software engineering search (SES) problems. This paper first describes a new quantum search algorithm, IDGS-FA, whose design is motivated by the characteristics of SES problems. Next, it describes how to apply quantum searching to three SES problems: FSM property checking, software test generation, and library-based software synthesis. Next, the paper gives the main ideas in QSAT, a novel toolkit supporting efficient simulation of the algorithms and applications discussed. Finally, it concludes with a substantial simulation-based study of IDGS-FA, showing that it improves both the reliability and speed of other approaches."
4.url:http://dx.doi.org/10.1109/ASE.2009.51
4.opinion:exclude

5.title:"title":"Understanding the Value of Software Engineering Technologies"
5.abstract:"abstract":"When AI search methods are applied to software process models, then appropriate technologies can be discovered for a software project. We show that those recommendations are greatly affected by the business context of its use. For example, the automatic defect reduction tools explored by the ASE community are only relevant to a subset of software projects, and only according to certain value criteria. Therefore, when arguing for the value of a particular technology, that argument should include a description of the value function of the target user community."
5.url:http://dx.doi.org/10.1109/ASE.2009.93
5.opinion:exclude

6.title:"title":"Type Inference for Soft-Error Fault-Tolerance Prediction"
6.abstract:"abstract":"Software systems are becoming increasingly vulnerable to a new class of soft errors, originating from voltage spikes produced by cosmic radiation. The standard technique for assessing the source-level impact of these soft errors, fault injection--essentially a black-box testing technique--provides limited high-level information. Since soft errors can occur anywhere, even control-structured white-box techniques offer little insight. We propose a type-based approach, founded on data-flow structure, to classify the usage pattern of registers and memory cells. To capture all soft errors, the type system is defined at the assembly level, close to the hardware, and allows inferring types in the untyped assembly representation. In a case study, we apply our type inference scheme to a prototype brake-by-wire controller, developed by Volvo Technology, and identify a high correlation between types and fault-injection results. The case study confirms that the inferred types are good predictors for soft-error impact."
6.url:http://dx.doi.org/10.1109/ASE.2009.61
6.opinion:exclude

7.title:"title":"Evaluating the Accuracy of Fault Localization Techniques"
7.abstract:"abstract":"We investigate claims and assumptions made in several recent papers about fault localization (FL) techniques. Most of these claims have to do with evaluating FL accuracy. Our investigation centers on a new subject program having properties useful for FL experiments. We find that Tarantula (Jones et al.) works well on the program, and we show weak support for the assertion that coverage-based test suites help Tarantula to localize faults. Baudry et al. used automatically-generated mutants to evaluate the accuracy of an FL technique that generates many distinct scores for program locations. We find no evidence to suggest that the use of mutants for this purpose is invalid. However, we find evidence that the standard method for evaluating FL accuracy is unfairly biased toward techniques that generate many distinct scores, and we propose a fairer method of accuracy evaluation. Finally, Denmat et al. suggest that data mining techniques may apply to FL. We investigate this suggestion with the data mining tool Weka, using standard techniques for evaluating the accuracy of data mining classifiers. We find that standard classifiers suffer from the class imbalance problem. However, we find that adding cost information improves accuracy."
7.url:http://dx.doi.org/10.1109/ASE.2009.89
7.opinion:exclude

8.title:"title":"Spectrum-Based Multiple Fault Localization"
8.abstract:"abstract":"Fault diagnosis approaches can generally be categorized into spectrum-based fault localization (SFL, correlating failures with abstractions of program traces), and model-based diagnosis (MBD, logic reasoning over a behavioral model). Although MBD approaches are inherently more accurate than SFL, their high computational complexity prohibits application to large programs. We present a framework to combine the best of both worlds, coined BARINEL. The program is modeled using abstractions of program traces (as in SFL) while Bayesian reasoning is used to deduce multiple-fault candidates and their probabilities (as in MBD). A particular feature of BARINEL is the usage of a probabilistic component model that accounts for the fact that faulty components may fail intermittently. Experimental results on both synthetic and real software programs show that BARINEL typically outperforms current SFL approaches at a cost complexity that is only marginally higher. In the context of single faults this superiority is established by formal proof."
8.url:http://dx.doi.org/10.1109/ASE.2009.25
8.opinion:exclude

9.title:"title":"Towards a Comprehensive Test Suite for Detectors of Design Patterns"
9.abstract:"abstract":"Detection of design patterns is an important part of reverse engineering. Availability of patterns provides for a better understanding of code and also makes analysis more efficient in terms of time and cost. In recent years, we have observed a continual improvement in the field of automatic detection of design patterns in source code. Existing approaches can detect a fairly broad range of design patterns, targeting both structural and behavioral aspects of patterns. However, it is not straightforward to assess and compare these approaches. There is no common ground on which to evaluate the accuracy of the detection approaches, given the existence of variants and specific code constructs used to implement a design pattern. We propose a systematic approach to constructing a comprehensive test suite for detectors of design patterns. This approach is applied to construct a test suite covering the Singleton pattern. The test suite contains many implementation variants of these patterns, along with such code constructs as method forwarding, access modifiers, and long inheritance paths. Furthermore, we use this test suite to compare three detection tools and to identify their strengths and weaknesses."
9.url:http://dx.doi.org/10.1109/ASE.2009.85
9.opinion:exclude

10.title:"title":"Improving API Usage through Automatic Detection of Redundant Code"
10.abstract:"abstract":"Software projects often rely on third-party libraries made accessible through Application Programming Interfaces (APIs). We have observed many cases where APIs are used in ways that are not the most effective. We developed a technique and tool support to automatically detect such patterns of API usage in software projects. The main hypothesis underlying our technique is that client code imitating the behavior of an API method without calling it may not be using the API effectively because it could instead call the method it imitates. Our technique involves analyzing software systems to detect cases of API method imitations. In addition to warning developers of potentially re-implemented API methods, we also indicate how to improve the use of the API. Applying our approach on 10 Java systems revealed over 400 actual cases of potentially suboptimal API usage, leading to many improvements to the quality of the code we studied."
10.url:http://dx.doi.org/10.1109/ASE.2009.62
10.opinion:exclude

11.title:"title":"Clone-Aware Configuration Management"
11.abstract:"abstract":"Recent research results show several benefits of the management of code clones. In this paper, we introduce Clever, a novel clone-aware software configuration management (SCM) system. In addition to traditional SCM functionality, Clever provides clone management support, including clone detection and update, clone change management, clone consistency validating, clone synchronizing, and clone merging. Clever represents source code and clones as (sub)trees in Abstract Syntax Trees (ASTs), measures code similarity based on structural characteristic vectors, and describes code changes as tree editing scripts. The key techniques of Clever include the algorithms to compute tree editing scripts; to detect and update code clones and their groups; and to analyze the changes of cloned code to validate their consistency and recommend the relevant synchronization. Our empirical study on many real-world programs shows that Clever is highly efficient and accurate in clone detection and updating, and provides useful analysis of clone changes."
11.url:http://dx.doi.org/10.1109/ASE.2009.90
11.opinion:exclude

12.title:"title":"SMT-Based Bounded Model Checking for Embedded ANSI-C Software"
12.abstract:"abstract":"Propositional bounded model checking has been applied successfully to verify embedded software but is limited by the increasing propositional formula size and the loss of structure during the translation. These limitations can be reduced by encoding word-level information in theories richer than propositional logic and using SMT solvers for the generated verification conditions. Here, we investigate the application of different SMT solvers to the verification of embedded software written in ANSI-C. We have extended the encodings from previous SMT-based bounded model checkers to provide more accurate support for variables of finite bit width, bit-vector operations, arrays, structures, unions and pointers. We have integrated the CVC3, Boolector, and Z3 solvers with the CBMC front-end and evaluated them using both standard software model checking benchmarks and typical embedded software applications from telecommunications, control systems, and medical devices. The experiments show that our approach can analyze larger problems and substantially reduce the verification time."
12.url:http://dx.doi.org/10.1109/ASE.2009.63
12.opinion:exclude

13.title:"title":"Static Validation of C Preprocessor Macros"
13.abstract:"abstract":"The widely used C preprocessor (CPP) is generally considered a source of difficulty for understanding and maintaining C/C++ programs. The main reason for this difficulty is CPP's purely lexical semantics, i.e., its treatment of both input and output as token streams. This can easily lead to errors that are difficult to diagnose, and it has been estimated that up to 20&#x025; of all macros are erroneous. To reduce such errors, more restrictive, replacement languages for CPP have been proposed to limit expanded macros to be valid C syntactic units. However, there is no practical tool that can effectively validate CPP\\ macros in legacy applications. In this paper, we introduce a novel, general characterization of inconsistent macro usage as a strong indicator of macro errors. Our key insight is that all applications of the same macro should behave similarly. In particular, we map each macro call c in a source file f to c's normalized syntactic constructs within the abstract syntax tree (AST) for f's preprocessed source, and use syntactic similarity as the basis for comparing macro calls of the same macro definition. Utilizing this characterization, we have developed an efficient algorithm to statically validate macro usage in C/C++ programs. We have implemented the algorithm; evaluation results show that our tool is effective in detecting common macro-related errors and reports few false positives, making it a practical tool for validating macro usage."
13.url:http://dx.doi.org/10.1109/ASE.2009.75
13.opinion:exclude

14.title:"title":"Looper: Lightweight Detection of Infinite Loops at Runtime"
14.abstract:"abstract":"When a running program becomes unresponsive, it is often impossible for a user to determine if the program is performing some useful computation or if it has entered an infinite loop. We present LOOPER, an automated technique for dynamically analyzing a running program to prove that it is non-terminating. LOOPER uses symbolic execution to produce simple non-termination arguments for infinite loops dependent on both program values and the shape of heap. The constructed arguments are verified with an off-the-shelf SMT solver. We have implemented our technique in a prototype tool for Java applications, and we demonstrate our technique&#x02019;s effectiveness on several non-terminating benchmarks, including a reported infinite loop bug in open-source text editor jEdit. Our tool is able to dynamically detect infinite loops deep in the execution of large Java programs with no false warnings, producing symbolic arguments that can aid in debugging non-termination."
14.url:http://dx.doi.org/10.1109/ASE.2009.87
14.opinion:exclude

15.title:"title":"Improving the Efficiency of Dependency Analysis in Logical Decision Models"
15.abstract:"abstract":"To address the problem that existing software dependency extraction methods do not work on higher-level software artifacts, do not express decisions explicitly, and do not reveal implicit or indirect dependencies, our recent work explored the possibility of formally defining and automatically deriving a pairwise dependence relation from an augmented constraint networks (ACN) that models the assumption relation among design decisions. The current approach is difficult to scale, requiring constraint solving and solution enumeration. We observe that the assumption relation among design decisions for most software systems can be abstractly modeled using a special form of ACN. For these more restrictive, but highly representative models, we present an O(n^3) algorithm to derive the dependency relation without solving the constraints. We evaluate our approach by computing design structure matrices for existing ACNs that model multiple versions of heterogenous real software designs, often reducing the running time from hours to seconds."
15.url:http://dx.doi.org/10.1109/ASE.2009.55
15.opinion:exclude

16.title:"title":"Explicit Concern-Driven Development with ArchEvol"
16.abstract:"abstract":"Supporting developers in examining and evolving a software system in terms of concerns is considered a critical capability in the face of the scale and complexity of today&#x02019;s software. A number of existing approaches make an inroad to providing this support, but they fall short in key ways. This paper introduces ArchEvol, a new programming environment that embodies a new kind of approach, one we term explicit concern-driven development. The vision is threefold: (1) a fine-grained concern model maps concerns to code, (2) concerns are visualized at both the code level, to assist in the actual act of making changes, and the architectural level, to assist in gauging levels of scattering and tangling, and (3) automated support assists developers in maintaining the concern mapping over time. Developers, then, continuously examine, structure, and modify the software they produce in terms of concerns. We introduce our approach, discuss how we have realized it in ArchEvol, and present the results of a first set of evaluations that demonstrate its potential."
16.url:http://dx.doi.org/10.1109/ASE.2009.70
16.opinion:exclude

17.title:"title":"Design Rule Hierarchies and Parallelism in Software Development Tasks"
17.abstract:"abstract":"As software projects continue to grow in scale, being able to maximize the work that developers can carry out in parallel as a set of concurrent development tasks, without incurring excessive coordination overhead, becomes increasingly important. Prevailing design models, however, are not explicitly conceived to suggest how development tasks on the software modules they describe can be effectively parallelized. In this paper, we present a design rule hierarchy based on the assumption relations among design decisions. Software modules located within the same layer of the hierarchy suggest independent, hence parallelizable, tasks. Dependencies between layers or within a module suggest the need for coordination during concurrent work. We evaluate our approach by investigating the source code and mailing list of Apache Ant. We observe that technical communication between developers working on different modules within the same hierarchy layer, as predicted, is significantly less than communication between developers working across layers."
17.url:http://dx.doi.org/10.1109/ASE.2009.53
17.opinion:exclude

18.title:"title":"Automated Test Order Generation for Software Component Integration Testing"
18.abstract:"abstract":"The order in which software components are tested can have a significant impact on the number of stubs required during component integration testing. This paper presents an efficient approach that applies heuristics based on a given software component test dependency graph to automatically generate a test order that requires a (near) minimal number of test stubs. Thus, the approach reduces testing effort and cost. The paper describes the proposed approach, analyses its complexity and illustrates its use. Comparison with three well known graph-based approaches, for a real-world software application, shows that only the classic Le Traon et al.&#x02019;s approach and ours give an optimal number of stubs. However, experiments on randomly simulated dependency models with 100 to 10,000 components show that our approach has a significant performance advantage with a reduction in the average running time of 96.01&#x025;."
18.url:http://dx.doi.org/10.1109/ASE.2009.84
18.opinion:exclude

19.title:"title":"A Divergence-Oriented Approach to Adaptive Random Testing of Java Programs"
19.abstract:"abstract":"Adaptive Random Testing (ART) is a testing technique which is based on an observation that a test input usually has the same potential as its neighbors in detection of a specific program defect. ART helps to improve the efficiency of random testing in that test inputs are selected evenly across the input spaces. However, the application of ART to object-oriented programs (e.g., C++ and Java) still faces a strong challenge in that the input spaces of object-oriented programs are usually high dimensional, and therefore an even distribution of test inputs in a space as such is difficult to achieve. In this paper, we propose a divergence-oriented approach to adaptive random testing of Java programs to address this challenge. The essential idea of this approach is to prepare for the tested program a pool of test inputs each of which is of significant difference from the others, and then to use the ART technique to select test inputs from the pool for the tested program. We also develop a tool called ARTGen to support this testing approach, and conduct experiment to test several popular opensource Java packages to assess the effectiveness of the approach. The experimental result shows that our approach can generate test cases with high quality."
19.url:http://dx.doi.org/10.1109/ASE.2009.13
19.opinion:exclude

20.title:"title":"Adaptive Random Test Case Prioritization"
20.abstract:"abstract":"Regression testing assures changed programs against unintended amendments. Rearranging the execution order of test cases is a key idea to improve their effectiveness. Paradoxically, many test case prioritization techniques resolve tie cases using the random selection approach, and yet random ordering of test cases has been considered as ineffective. Existing unit testing research unveils that adaptive random testing (ART) is a promising candidate that may replace random testing (RT). In this paper, we not only propose a new family of coverage-based ART techniques, but also show empirically that they are statistically superior to the RT-based technique in detecting faults. Furthermore, one of the ART prioritization techniques is consistently comparable to some of the best coverage-based prioritization techniques (namely, the \"additional\" techniques) and yet involves much less time cost."
20.url:http://dx.doi.org/10.1109/ASE.2009.77
20.opinion:exclude

21.title:"title":"Model-Based Customization and Deployment of Eclipse-Based Tools: Industrial Experiences"
21.abstract:"abstract":"Developers of software engineering tools are facing high expectations regarding capabilities and usability. Users expect tools tailored to their specific needs and integrated in their working environment. This increases tools' complexity and complicates their customization and deployment despite available mechanisms for adaptability and extensibility. A main challenge lies in understanding and managing the dependencies between different technical mechanisms for realizing tool variability. We report on industrial experiences of applying a model-based and tool-supported product line approach for the customization and deployment of two Eclipse-based tools. We illustrate challenges of customizing these tools to different development contexts: In the first case study we developed variability models of a product line tool suite used by an industry partner and utilized these models for tool customization and deployment. In the second case study we applied the same approach to a maintenance and setup tool of our industry partner. Our experiences suggest to design software tools as product lines; to formally describe the tools' variability in models; and to provide end-user capabilities for customizing and deploying the tools."
21.url:http://dx.doi.org/10.1109/ASE.2009.11
21.opinion:exclude

22.title:"title":"Self-Repair through Reconfiguration: A Requirements Engineering Approach"
22.abstract:"abstract":"High variability software systems can deliver their functionalities in multiple ways by reconfiguring their components. High variability has become important because of current trends towards software systems that come in product families, offer high levels of personalization, and fit well within a service-oriented architecture. The purpose of our research is to propose a framework that exploits such variability to allow a software system to self-repair in cases of failure. We propose an autonomic architecture that consists of monitoring, diagnosis, reconfiguration and execution components. This architecture uses requirements models as a basis for monitoring, diagnosis, and reconfiguration. We illustrate our proposal with a medium-sized publicly available case study (an Automated Teller Machine (ATM) simulation), and evaluate its performance through a series of experiments. Our experimental results demonstrate that it is feasible to scale our approach to software systems with medium-size requirements."
22.url:http://dx.doi.org/10.1109/ASE.2009.66
22.opinion:exclude

23.title:"title":"Model Checking of Domain Artifacts in Product Line Engineering"
23.abstract:"abstract":"In product line engineering individual products are derived from the domain artifacts of the product line. The reuse of the domain artifacts is constraint by the product line variability. Since domain artifacts are reused in several products, product line engineering benefits from the verification of domain artifacts. For verifying development artifacts, model checking is a well-established technique in single system development. However, existing model checking approaches do not incorporate the product line variability and are hence of limited use for verifying domain artifacts. In this paper we present an extended model checking approach which takes the product line variability into account when verifying domain artifacts. Our approach is thus able to verify that every permissible product (specified with I/O-automata) which can be derived from the product line fulfills the specified properties (specified with CTL). Moreover, we use two examples to validate the applicability of our approach and report on the preliminary validation results."
23.url:http://dx.doi.org/10.1109/ASE.2009.16
23.opinion:exclude

24.title:"title":"Alattin: Mining Alternative Patterns for Detecting Neglected Conditions"
24.abstract:"abstract":"To improve software quality, static or dynamic verification tools accept programming rules as input and detect their violations in software as defects. As these programming rules are often not well documented in practice, previous work developed various approaches that mine programming rules as frequent patterns from program source code. Then these approaches use static defect-detection techniques to detect pattern violations in source code under analysis. These existing approaches often produce many false positives due to various factors. To reduce false positives produced by these mining approaches, we develop a novel approach, called Alattin, that includes a new mining algorithm and a technique for detecting neglected conditions based on our mining algorithm. Our new mining algorithm mines alternative patterns in example form \"P1 or P2\", where P1 and P2 are alternative rules such as condition checks on method arguments or return values related to the same API method. We conduct two evaluations to show the effectiveness of our Alattin approach. Our evaluation results show that (1) alternative patterns reach more than 40&#x025; of all mined patterns for APIs provided by six open source libraries; (2) the mining of alternative patterns helps reduce nearly 28&#x025; of false positives among detected violations."
24.url:http://dx.doi.org/10.1109/ASE.2009.72
24.opinion:exclude

25.title:"title":"Mining Temporal Specifications from Object Usage"
25.abstract:"abstract":"A caller must satisfy the callee's precondition--that is, reach a state in which the callee may be called. Preconditions describe the state that needs to be reached, but not how to reach it. We combine static analysis with model checking to mine Computation Tree Logic (CTL) formulas that describe the operations a parameter goes through: \"In parseProperties(String xml), the parameter xml normally stems from getProperties().\" Such operational preconditions can be learned from program code, and the code can be checked for their violations. Applied to AspectJ, our Tikanga prototype found 189 violations of operational preconditions, uncovering 9 unique defects and 36 unique code smells---with 44&#x025; true positives in the 50 top-ranked violations."
25.url:http://dx.doi.org/10.1109/ASE.2009.30
25.opinion:exclude

26.title:"title":"Inferring Resource Specifications from Natural Language API Documentation"
26.abstract:"abstract":"Typically, software libraries provide API documentation, through which developers can learn how to use libraries correctly. However, developers may still write code inconsistent with API documentation and thus introduce bugs, as existing research shows that many developers are reluctant to carefully read API documentation. To find those bugs, researchers have proposed various detection approaches based on known specifications. To mine specifications, many approaches have been proposed, and most of them rely on existing client code. Consequently, these mining approaches would fail to mine specifications when client code is not available. In this paper, we propose an approach, called Doc2Spec, that infers resource specifications from API documentation. For our approach, we implemented a tool and conducted an evaluation on Javadocs of five libraries. The results show that our approach infers various specifications with relatively high precisions, recalls, and F-scores. We further evaluated the usefulness of inferred specifications through detecting bugs in open source projects. The results show that specifications inferred by Doc2Spec are useful to detect real bugs in existing projects."
26.url:http://dx.doi.org/10.1109/ASE.2009.94
26.opinion:exclude

27.title:"title":"Specification and Control of Interface Responses to User Input in Rich Internet Applications"
27.abstract:"abstract":"The complexity of data-intensive business processes is typically reflected in the user interfaces of the information systems supporting them. To facilitate ease of use despite the visual and semantic complexity of dialog forms, users should be guided in ways such as highlighting and describing invalid input, showing/hiding or enabling/disabling particular UI widgets. Manual implementation of the rules governing these user interface responses typically requires considerable effort, as they involve business and presentation concerns, and are often dependent on each other. In addition, users expect the interface to respond immediately to any input, which can be especially challenging in web applications. In this paper, we formalize the aspects that must be considered in formulating input evaluation rules, and present a run-time framework that encapsulates the necessary logic, enabling domain experts to specify the business rules instead of requiring developers to implement them."
27.url:http://dx.doi.org/10.1109/ASE.2009.10
27.opinion:exclude

28.title:"title":"Code Completion from Abbreviated Input"
28.abstract:"abstract":"Abbreviation Completion is a novel technique to improve the efficiency of code-writing by supporting code completion of multiple keywords based on non-predefined abbreviated input -- a different approach from conventional code completion that finds one keyword at a time based on an exact character match. Abbreviated input is expanded into keywords by a Hidden Markov Model learned from a corpus of existing code. The technique does not require the user to memorize abbreviations and provides incremental feedback of the most likely completions. This paper presents the algorithm for abbreviation completion, integrated with a new user interface for multiple-keyword completion. We tested the system by sampling 3000 code lines from open source projects and found that more than 98&#x025; of the code lines could be resolved from acronym-like abbreviations. A user study found 30&#x025; reduction in time usage and 41&#x025; reduction of keystrokes over conventional code completion."
28.url:http://dx.doi.org/10.1109/ASE.2009.64
28.opinion:exclude

29.title:"title":"Task-First or Context-First? Tool Integration Revisited"
29.abstract:"abstract":"If software engineering tools are not \"properly integrated\", they can reduce engineers' productivity. Associating and retrieving information scattered across the tools become unsystematic and inefficient. Our work provides empirical evidence on what is a \"poor\" and a \"proper\" tool integration, focusing on practitioners' perspectives. We interviewed 62 engineers and analyzed the content of their project artifacts. We identified problem situations and practices related to tool integration. Engineers agreed that tool integration approaches must support change, heterogeneity and automatic linking of change to context. To quantify our results, we conducted a field experiment with 27 and a survey with 782 subjects. We found a strong correlation between change frequency and preferred integration approaches. Particularly in projects with short release cycles, tasks should be used to link information handled by different tools. We also found that half of engineers' work is not defined as tasks. Therefore, a context-based tool integration approach is more effective than a task-based one."
29.url:http://dx.doi.org/10.1109/ASE.2009.36
29.opinion:exclude

30.title:"title":"Mining Hierarchical Scenario-Based Specifications"
30.abstract:"abstract":"Scalability over long traces, as well as comprehensibility and expressivity of results, are major challenges for dynamic analysis approaches to specification mining. In this work we present a novel use of object hierarchies over traces of inter-object method calls, as an abstraction/refinement mechanism that enables user-guided, top-down or bottom-up mining of layered scenario-based specifications, broken down by hierarchies embedded in the system under investigation. We do this using data mining methods that provide statistically significant sound and complete results modulo user-defined thresholds, in the context of Damm and Harel&#x02019;s live sequence charts (LSC); a visual, modal, scenario-based, inter-object language. Thus, scalability, comprehensibility, and expressivity are all addressed. Our technical contribution includes a formal definition of hierarchical inter-object traces, and algorithms for &#x02018;zoomingout&#x02019; and &#x02018;zooming-in&#x02019;, used to move between abstraction levels on the mined specifications. An evaluation of our approach based on several case studies shows promising results."
30.url:http://dx.doi.org/10.1109/ASE.2009.19
30.opinion:exclude

31.title:"title":"Automatic Generation of Object Usage Specifications from Large Method Traces"
31.abstract:"abstract":"Formal specifications are used to identify programming errors, verify the correctness of programs, and as documentation. Unfortunately, producing them is error-prone and time-consuming, so they are rarely used in practice. Inferring specifications from a running application is a promising solution. However, to be practical, such an approach requires special techniques to treat large amounts of runtime data. We present a scalable dynamic analysis that infers specifications of correct method call sequences on multiple related objects. It preprocesses method traces to identify small sets of related objects and method calls which can be analyzed separately. We implemented our approach and applied the analysis to eleven real-world applications and more than 240 million runtime events. The experiments show the scalability of our approach. Moreover, the generated specifications describe correct and typical behavior, and match existing API usage documentation."
31.url:http://dx.doi.org/10.1109/ASE.2009.60
31.opinion:exclude

32.title:"title":"Efficient Formalism-Independent Monitoring of Parametric Properties"
32.abstract:"abstract":"Parametric properties provide an effective and natural means to describe object-oriented system behaviors, where the parameters are typed by classes and bound to object instances at runtime. Efficient monitoring of parametric properties, in spite of increasingly growing interest due to applications such as testing and security, imposes a highly non-trivial challenge on monitoring approaches due to the potentially huge number of parameter instances. Existing solutions usually compromise their expressiveness for performance or vice versa. In this paper, we propose a generic, in terms of specification formalism, yet efficient, solution to monitoring parametric specifications. Our approach is based on a general algorithm for slicing parametric traces and makes use of {\\em static} knowledge about the desired property to optimize monitoring. The needed knowledge is not specific to the underlying formalism and can be easily computed when generating monitoring code from the property. Our approach works with any specification formalism, providing better and extensible expressiveness. Also, a thorough evaluation shows that our technique outperforms other state-of-art techniques optimized for particular logics or properties."
32.url:http://dx.doi.org/10.1109/ASE.2009.50
32.opinion:exclude

33.title:"title":"Automatically Recommending Triage Decisions for Pragmatic Reuse Tasks"
33.abstract:"abstract":"Planning a complex software modification task imposes a high cognitive burden on developers, who must juggle navigating the software, understanding what they see with respect to their task, and deciding how their task should be performed given what they have discovered. Pragmatic reuse tasks, where source code is reused in a white-box fashion, is an example of a complex and error-prone modification task: the developer must plan out which portions of a system to reuse, extract the code, and integrate it into their own system. In this paper we present a recommendation system that automates some aspects of the planning process undertaken by developers during pragmatic reuse tasks. In a retroactive evaluation, we demonstrate that our technique was able to provide the correct recommendation 64&#x025; of the time and was incorrect 25&#x025; of the time. Our case study suggests that developer investigative behaviour is positively influenced by the use of the recommendation system."
33.url:http://dx.doi.org/10.1109/ASE.2009.65
33.opinion:exclude

34.title:"title":"Mining Health Models for Performance Monitoring of Services"
34.abstract:"abstract":"Online services such as search and live applications rely on large infrastructures in data centers, consisting of both stateless servers (e.g., web servers) and stateful servers (e.g., database servers). Acceptable performance of such infrastructures, and hence the availability of online services, rely on a very large number of parameters such as per-process resources and configurable system/application parameters. These parameters are available for collection as performance counters distributed across various machines, but services have had a hard time determining which performance counters to monitor and what thresholds to use for performance alarms in a production environment. In this paper, we present a novel framework called PerfAnalyzer, a storage-efficient and pro-active performance monitoring framework for correlating service health with performance counters. PerfAnalyzer automatically infers and builds health models for any service by running the standard suite of predeployment tests for the service and data mining the resulting performance counter data-set. A filtered set of performance counters and thresholds of alarms are produced by our framework. The health model inferred by our framework can then be used to detect performance degradation and collect detailed data for root-cause analysis in a production environment. We have applied PerfAnalyzer on five simple stress scenarios &#x02013; CPU, memory, I/O, disk, and network, and two real system &#x02013; Microsoft&#x02019;s SQL Server 2005 and IIS 7.0 Web Server, with promising results."
34.url:http://dx.doi.org/10.1109/ASE.2009.95
34.opinion:exclude

35.title:"title":"Inferring Method Effect Summaries for Nested Heap Regions"
35.abstract:"abstract":"Effect systems are important for reasoning about the side effects of a program. Although effect systems have been around for decades, they have not been widely adopted in practice because of the large number of annotations that they require. A tool that infers effects automatically can make effect systems practical. We present an effect inference algorithm and an Eclipse plug-in, DPJizer, which alleviate the burden of writing effect annotations for a language called Deterministic Parallel Java (DPJ). The key novel feature of the algorithm is the ability to infer effects on nested heap regions. Besides DPJ, we also illustrate how the algorithm can be used for a different effect system based on object ownership. Our experience shows that DPJizer is both useful and effective: (i) inferring effect annotations automatically saves significant programming burden; and (ii) inferred effects are more precise than those written manually, and are fine-grained enough to enable the compiler to prove determinism of the program."
35.url:http://dx.doi.org/10.1109/ASE.2009.68
35.opinion:exclude

36.title:"title":"ReAssert: Suggesting Repairs for Broken Unit Tests"
36.abstract:"abstract":"Developers often change software in ways that cause tests to fail. When this occurs, developers must determine whether failures are caused by errors in the code under test or in the test code itself. In the latter case, developers must repair failing tests or remove them from the test suite. Repairing tests is time consuming but beneficial, since removing tests reduces a test suite's ability to detect regressions. Fortunately, simple program transformations can repair many failing tests automatically. We present ReAssert, a novel technique and tool that suggests repairs to failing tests' code which cause the tests to pass. Examples include replacing literal values in tests, changing assertion methods, or replacing one assertion with several. If the developer chooses to apply the repairs, ReAssert modifies the code automatically. Our experiments show that ReAssert can repair many common test failures and that its suggested repairs correspond to developers' expectations."
36.url:http://dx.doi.org/10.1109/ASE.2009.17
36.opinion:exclude

37.title:"title":"Cache-Based Model Checking of Networked Applications: From Linear to Branching Time"
37.abstract:"abstract":"Many applications are concurrent and communicate over a network. The non-determinism in the thread and communication schedules makes it desirable to model check such systems. However, a simple state space exploration scheme is not applicable, as backtracking results in repeated communication operations. A cache-based approach solves this problem by hiding redundant communication operations from the environment. In this work, we propose a change from a linear-time to a branching-time cache, allowing us to relax restrictions in previous work regarding communication traces that differ between schedules. We successfully applied the new algorithm to real-life programs where a previous solution is not applicable."
37.url:http://dx.doi.org/10.1109/ASE.2009.43
37.opinion:exclude

38.title:"title":"State-Space Coverage Estimation"
38.abstract:"abstract":"Software model checking is the process of systematically exploring a program's state space to find hard-to-discover errors. Because of the exponential size of the state space, an exhaustive search of the state space is often impossible given the memory resources. In such cases, an estimate of how much of the state space is covered can help the verifier to decide whether to employ additional computational resources or to use more aggressive abstraction techniques. Our work focuses on coverage estimation for explicit-state model checking of software programs. In this paper, we present an estimation algorithm that is based on Monte Carlo techniques that sample the unexplored portion of the reachability graph. We implemented our algorithm in Java Pathfinder and evaluated our approach on a suite of Java programs, simulating out-of-memory errors after a known percentage of a program's state space had been searched. Our empirical studies show that, on average, our algorithm's coverage estimates differ from the actual coverage by less than 10 percentage points, with a standard deviation of about 5 percentage points - regardless of whether the actual state-space coverage is low (3&#x025;) or high (95&#x025;)."
38.url:http://dx.doi.org/10.1109/ASE.2009.24
38.opinion:exclude

39.title:"title":"A Framework for State-Space Exploration of Java-Based Actor Programs"
39.abstract:"abstract":"The actor programming model offers a promising model for developing reliable parallel and distributed code. Actors provide flexibility and scalability: local execution may be interleaved, and distributed nodes may operate asynchronously. The resulting nondeterminism is captured by nondeterministic processing of messages. To automate testing, researchers have developed several tools tailored to specific actor systems. As actor languages and libraries continue to evolve, such tools have to be reimplemented. Because many actor systems are compiled to Java bytecode, we have developed Basset, a general framework for testing actor systems compiled to Java bytecode. We illustrate Basset by instantiating it for the Scala programming language and for the ActorFoundry library for Java. Our implementation builds on Java PathFinder, a widely used model checker for Java. Experiments show that Basset can effectively explore executions of actor programs; e.g., it discovered a previously unknown bug in a Scala application."
39.url:http://dx.doi.org/10.1109/ASE.2009.88
39.opinion:exclude

40.title:"title":"Symbolic Deadlock Analysis in Concurrent Libraries and Their Clients"
40.abstract:"abstract":"Methods in object-oriented concurrent libraries hide internal synchronization details. However, information hiding may result in clients causing thread safety violations by invoking methods in an unsafe manner. Given such a library, we present a technique for inferring interface contracts that specify permissible concurrent method calls and patterns of aliasing among method arguments, such that the derived contracts guarantee deadlock free execution for the methods in the library. The contracts also help client developers by documenting required assumptions about the library methods. Alternatively, the contracts can be statically enforced in the client code to detect potential deadlocks in the client. Our technique combines static analysis with a symbolic encoding for tracking lock dependencies, allowing us to synthesize contracts using a SMT solver. Our prototype tool analyzes over a million lines of code for some widely-used Java libraries within an hour, thus demonstrating its scalability and efficiency. Furthermore, the contracts inferred by our approach have been able to pinpoint real deadlocks in clients, i.e. deadlocks that have been a part of bug-reports filed by users and developers of the client code."
40.url:http://dx.doi.org/10.1109/ASE.2009.14
40.opinion:exclude

41.title:"title":"Precise Data Race Detection in a Relaxed Memory Model Using Heuristic-Based Model Checking"
41.abstract:"abstract":"Most approaches to reasoning about multithreaded programs, including model checking, make the implicit assumption that the system being considered is sequentially consistent. This is, however, invalid in most modern computer architectures and results in unsound reasoning for programs that contain data races, where data races are defined by the memory model of the programming environment. We describe an extension to the model checker Java PathFinder that incorporates knowledge of the Java Memory Model to precisely detect data races in Java byte code. Our tool incorporates special purpose heuristic algorithms that result in shorter counterexample paths. Once data races have been eliminated from a program, Java PathFinder can be soundly employed to verify additional properties."
41.url:http://dx.doi.org/10.1109/ASE.2009.82
41.opinion:exclude

42.title:"title":"A Formal Syntax for Probabilistic Timed Property Sequence Charts"
42.abstract:"abstract":"Probabilistic properties are considered as the most important requirements for a variety of software systems, since they are used to formulate extra-functional requirements such as reliability, availability, safety, security and performance requirements. Currently, several probabilistic logics have been proposed to specify such important properties. However, due to the inherent complexity of the underlying temporal logics, these probabilistic logics are rather complex and software developers have problems using them to correctly specify the intended properties. To overcome this problem, we define a formal and graphical property specification language called Probabilistic Timed Property Sequence Charts (PTPSC) which is a probabilistic extension of Property Sequence Charts (PSC). We illustrate the use of PTPSC in the context of a vehicle-tovehicle communication device for avoiding traffic accidents."
42.url:http://dx.doi.org/10.1109/ASE.2009.56
42.opinion:exclude

43.title:"title":"Let the Ants Deploy Your Software - An ACO Based Deployment Optimisation Strategy"
43.abstract:"abstract":"Decisions regarding the mapping of software components to hardware nodes affect the quality of the resulting system. Making these decisions is hard when considering the ever-growing complexity of the search space, as well as conflicting objectives and constraints. An automation of the solution space exploration would help not only to make better decisions but also to reduce the time of this process. In this paper, we propose to employ Ant Colony Optmisation (ACO) as a multi-objective optimisation strategy. The constructive approach is compared to an iterative optimisation procedure - a Genetic Algorithm (GA) adaptation - and was observed to perform suprisingly similar, although not quite on a par with the GA, when validated based on a series of experiments."
43.url:http://dx.doi.org/10.1109/ASE.2009.59
43.opinion:exclude

44.title:"title":"Using String Distances for Test Case Prioritisation"
44.abstract:"abstract":"Test case prioritisation aims at finding an ordering which enhances a certain property of an ordered test suite. Traditional techniques rely on the availability of code or a specification of the program under test. In this paper, we propose to use string distances on the text of test cases for their comparison and elaborate a prioritisation algorithm. Such a prioritisation does not require code and can be useful for initial testing and in cases when code is difficult to instrument. We also briefly report on preliminary results of an experiment where the proposed prioritisation technique was compared with random permutations and four classical string distance metrics were evaluated."
44.url:http://dx.doi.org/10.1109/ASE.2009.23
44.opinion:exclude

45.title:"title":"Reggae: Automated Test Generation for Programs Using Complex Regular Expressions"
45.abstract:"abstract":"Test coverage such as branch coverage is commonly measured to assess the sufficiency of test inputs. To reduce tedious manual efforts in generating high-covering test inputs, various automated techniques have been proposed. Some recent effective techniques include Dynamic Symbolic Execution (DSE) based on path exploration. However, these existing DSE techniques cannot generate high-covering test inputs for programs using complex regular expressions due to large exploration space; these complex regular expressions are commonly used for input validation and information extraction. To address this issue, we propose an approach, named Reggae, to reduce the exploration space of DSE in test generation. In our evaluation, we apply Reggae on various input-validation programs that use complex regular expressions. Empirical results show that Reggae helps a test-generation tool generate test inputs to achieve 79&#x025; branch coverage of validators, improved from 29&#x025; achieved without the help of Reggae."
45.url:http://dx.doi.org/10.1109/ASE.2009.67
45.opinion:exclude

46.title:"title":"A Methodology and Framework to Simplify Usability Analysis of Mobile Applications"
46.abstract:"abstract":"Usability analysis is an important step in software development in order to improve certain aspects of the system. However, it is often a challenge especially when it comes to evaluating applications running on mobile devices because of the restrictions posed by the device and the lack of supporting tools and software available to collect the necessary usability data. This paper proposes a methodology and framework to aid developers in preparing the mobile system for usability analysis. The focus is on the simplification of the developer's task in preparing the system for evaluation and the processing of the collected usability data by automating some of the tasks involved in the process."
46.url:http://dx.doi.org/10.1109/ASE.2009.12
46.opinion:exclude

47.title:"title":"EA-Analyzer: Automating Conflict Detection in Aspect-Oriented Requirements"
47.abstract:"abstract":"One of the aims of Aspect-Oriented Requirements Engineering is to address the composability and subsequent analysis of crosscutting and non-crosscutting concerns during requirements engineering. Composing concerns may help to reveal conflicting dependencies that need to be identified and resolved. However, detecting conflicts in a large set of textual aspect-oriented requirements is an error-prone and time-consuming task. This paper presents EA-Analyzer, the first automated tool for identifying conflicts in aspect-oriented requirements specified in natural-language text. The tool is based on a novel application of a Bayesian learning method that has been effective at classifying text. We present an empirical evaluation of the tool with three industrial-strength requirements documents from different real-life domains. We show that the tool achieves up to 92.97&#x025; accuracy when one of the case study documents is used as a training set and the other two as a validation set."
47.url:http://dx.doi.org/10.1109/ASE.2009.31
47.opinion:exclude

48.title:"title":"An Automated Passive Testing Approach for the IMS PoC Service"
48.abstract:"abstract":"Although the adoption of the IP Multimedia Subsystem (IMS) keeps growing, IMS applications are often integrated to the system without being formally tested. In this work, we are interested in the IMS Push over Cellular (PoC) service, an OMA standard. We propose a conformance passive testing approach to check that its implementation respects the main standard requirements. This approach is based on a set of formal invariants representing the most relevant expected properties to be tested. Two testing phases are applied: the verification of the invariants against the service specification and their testing on the PoC collected execution traces."
48.url:http://dx.doi.org/10.1109/ASE.2009.33
48.opinion:exclude

49.title:"title":"Adding Examples into Java Documents"
49.abstract:"abstract":"Code examples play an important role to explain the usage of Application Programming Interfaces (APIs), but most API documents do not provide sufficient code examples. For example, for the JDK 5 documents (JavaDocs), only 2&#x025; of APIs have code examples. In this paper, we propose a technique that automatically augments API documents with code examples. Our approach finds and embeds code examples for more than 75&#x025; of the APIs in JavaDocs 5."
49.url:http://dx.doi.org/10.1109/ASE.2009.39
49.opinion:exclude

50.title:"title":"Enhanced Automation for Managing Model and Metamodel Inconsistency"
50.abstract:"abstract":"Model-Driven Engineering (MDE) introduces additional challenges for managing evolution. For example, a metamodel change may affect instance models. Existing tool supported approaches for updating models in response to a metamodel change assume extra effort from metamodel developers. When no existing approach is applicable, metamodel users must update their models manually, an error prone and tedious task. In this paper, we describe the technical challenges faced when using the Eclipse Modeling Framework (EMF) and existing approaches for updating models in response to a metamodel change. We then motivate and describe alternative techniques, including: a mechanism for loading, storing and mainpulating inconsistent models; a mapping of inconsistent models to a human-usable notation for semi-automated and collaborative co-evolution; and integration with an inter-model reference manager, achieving automatic consistency checking as part of metamodel distribution."
50.url:http://dx.doi.org/10.1109/ASE.2009.57
50.opinion:exclude

51.title:"title":"Generating Fixes from Object Behavior Anomalies"
51.abstract:"abstract":"Advances in recent years have made it possible in some cases to locate a bug (the source of a failure) automatically. But debugging is also about correcting bugs. Can tools do this automatically? The results reported in this paper, from the new PACHIKA tool, suggest that such a goal may be reachable. PACHIKA leverages differences in program behavior to generate program fixes directly. It automatically summarizes executions to object behavior models, determines differences between passing and failing runs, generates possible fixes, and assesses them via the regression test suite. Evaluated on the ASPECTJ bug history, PACHIKA generates a valid fix for 3 out of 18 crashing bugs; each fix pinpoints the bug location and passes the ASPECTJ test suite."
51.url:http://dx.doi.org/10.1109/ASE.2009.15
51.opinion:exclude

52.title:"title":"Service Substitution Revisited"
52.abstract:"abstract":"In this paper, we propose a framework that reduces the complexity of service substitution. The framework is based on two substitution relations and corresponding theorems. The proposed relations and theorems allow organizing available services into groups. Then, the complexity of retrieving candidate substitute services for the target service and generating corresponding adapters scales up with the number of available groups, instead of scaling up with the number of available services."
52.url:http://dx.doi.org/10.1109/ASE.2009.58
52.opinion:exclude

53.title:"title":"A Verification-Driven Approach to Traceability and Documentation for Auto-Generated Mathematical Software"
53.abstract:"abstract":"Automated code generators are increasingly used in safety-critical applications, but since they are typically not qualified, the generated code must still be fully tested, reviewed, and certified. For mathematical and engineering software this requires reviewers to trace subtle details of textbook formulas and algorithms to the code, and to match requirements (e.g., physical units or coordinate frames) not represented explicitly in models or code. We support these tasks by using the AutoCert verification system to identify and verify mathematical concepts in the code, recovering verified traceability links between concepts, code, and verification conditions. We then exploit these links to construct a natural language report that provides a high-level structured argument explaining where the code uses specified assumptions and why and how it complies with the requirements. We have applied our approach to generate review documents for several sub-systems of NASA's Project Constellation."
53.url:http://dx.doi.org/10.1109/ASE.2009.71
53.opinion:exclude

54.title:"title":"Towards Augmenting Requirements Models with Preferences"
54.abstract:"abstract":"The analysis of stakeholder requirements is a critical aspect of software engineering. A common way of specifying stakeholder requirements is in terms of a hierarchy of goals whose AND/OR decomposition captures a family of software solutions that comply with the goals. In this paper, we extend this goal modeling framework to include the specification of optional user requirements and user preferences, aggregated together into weighted formulae to be optimized. We team this with an automated reasoning tool, adapted from state of the art research in artificial intelligence planning with preferences, in order to synthesize solutions that both comply with the goals and optimize stakeholder preferences and optional requirements."
54.url:http://dx.doi.org/10.1109/ASE.2009.91
54.opinion:exclude

55.title:"title":"Automated Comprehension Tasks in Software Exploration"
55.abstract:"abstract":"Finding issues in software usually requires a serie of comprehension tasks. After every task, an engineer explores the results and decides whether further tasks are required. Software comprehension therefore is a combination of tasks and a supported exploration of the results typically in an adequate visualization. In this paper, we describe how we simplify the combination of existing automated procedures to sequentially solve common software comprehension tasks. Beyond that we improve the understanding of the outcomes with interactive and explorative visualization concepts in a time efficient workflow. We validate the presented concept with basic comprehension tasks in an extended CocoViz tool implementation."
55.url:http://dx.doi.org/10.1109/ASE.2009.47
55.opinion:exclude

56.title:"title":"Pointcut Rejuvenation: Recovering Pointcut Expressions in Evolving Aspect-Oriented Software"
56.abstract:"abstract":"Pointcut fragility is a well-documented problem in Aspect-Oriented Programming; changes to the base-code can lead to join points incorrectly falling in or out of the scope of pointcuts. We present an automated approach that limits fragility problems by providing mechanical assistance in pointcut maintenance. The approach is based on harnessing arbitrarily deep structural commonalities between program elements corresponding to join points selected by a pointcut. The extracted patterns are then applied to later versions to offer suggestions of new join points that may require inclusion. We demonstrate the usefulness of our technique by rejuvenating pointcuts in multiple versions of several open-source AspectJ programs. The results show that our parameterized heuristic algorithm was able to automatically infer new join points in subsequent versions with an average recall of 0.93. Moreover, these join points appeared, on average, in the top 4th percentile of the suggestions, indicating that the results were precise."
56.url:http://dx.doi.org/10.1109/ASE.2009.37
56.opinion:exclude

57.title:"title":"Applications of Simulation and AI Search: Assessing the Relative Merits of Agile vs Traditional Software Development"
57.abstract:"abstract":"This paper augments Boehm-Turner's model of agile and plan-based software development augmented with an AI search algorithm. The AI search finds the key factors that predict for the success of agile or traditional plan-based software developments. According to our simulations and AI search algorithm: (1) in no case did agile methods perform worse than plan-based approaches; (2) in some cases, agile performed best. Hence, we recommend that the default development practice for organizations be an agile method. The simplicity of this style of analysis begs the question: why is so much time wasted on evidence-less debates on software process when a simple combination of simulation plus automatic search can mature the dialog much faster?"
57.url:http://dx.doi.org/10.1109/ASE.2009.42
57.opinion:exclude

58.title:"title":"Automating the Implementation of Analysis Concerns in Workflow Applications"
58.abstract:"abstract":"In workflow management systems, analysis concerns related to monitoring, measurement, and control aim at identifying potential improvements of workflow applications. However, the specification of analysis concerns is done using a specific workflow language and engine, producing entangled code which is detrimental to their maintainability. The purpose of this paper is twofold. First, it presents briefly a domain-specific language to specify analysis concerns, independently of any workflow technology and in a modularized way. Second, it shows a strategy to assist developers to enhance a given workflow technology to support the automated implementation of analysis concerns into its workflow applications. Thus, given a workflow application and its analysis concerns, they are automatically integrated producing an enhanced executable workflow application."
58.url:http://dx.doi.org/10.1109/ASE.2009.29
58.opinion:exclude

59.title:"title":"Static Typing for Ruby on Rails"
59.abstract:"abstract":"Ruby on Rails (or just \"Rails\") is a popular web application framework built on top of Ruby, an object-oriented scripting language. While Ruby&#x02019;s powerful features such as dynamic typing help make Rails development extremely lightweight, this comes at a cost. Dynamic typing in particular means that type errors in Rails applications remain latent until run time, making debugging and maintenance harder. In this paper, we describe DRails, a novel tool that brings static typing to Rails applications to detect a range of run time errors. DRails works by translating Rails programs into pure Ruby code in which Rails&#x02019;s numerous implicit conventions are made explicit. We then discover type errors by applying DRuby, a previously developed static type inference system, to the translated program. We ran DRails on a suite of applications and found that it was able to detect several previously unknown errors."
59.url:http://dx.doi.org/10.1109/ASE.2009.80
59.opinion:exclude

60.title:"title":"Towards Automating Class-Splitting Using Betweenness Clustering"
60.abstract:"abstract":"Large, unwieldy classes are a significant maintenance problem. Programmers dislike them because the fundamental logic is often obscured, making them hard to understand and modify. This paper proposes a solution - a semi-automatic technique for splitting large classes into smaller, more cohesive ones. The core of the technique is the use of betweenness clustering to identify the best way of partitioning a class. This turned a tedious manual process into a quick and simple semi-automated one in roughly one third of the cases we examined."
60.url:http://dx.doi.org/10.1109/ASE.2009.21
60.opinion:exclude

61.title:"title":"Reducing Features to Improve Bug Prediction"
61.abstract:"abstract":"Recently, machine learning classifiers have emerged as a way to predict the existence of a bug in a change made to a source code file. The classifier is first trained on software history data, and then used to predict bugs. Two drawbacks of existing classifier-based bug prediction are potentially insufficient accuracy for practical use, and use of a large number of features. These large numbers of features adversely impact scalability and accuracy of the approach. This paper proposes a feature selection technique applicable to classification-based bug prediction. This technique is applied to predict bugs in software changes, and performance of Naive Bayes and Support Vector Machine (SVM) classifiers is characterized."
61.url:http://dx.doi.org/10.1109/ASE.2009.76
61.opinion:exclude

62.title:"title":"Generating Vulnerability Signatures for String Manipulating Programs Using Automata-Based Forward and Backward Symbolic Analyses"
62.abstract:"abstract":"Given a program and an attack pattern (specified as a regular expression), we automatically generate string-based vulnerability signatures, i.e., a characterization that includes all malicious inputs that can be used to generate attacks. We use an automata-based string analysis framework. Using forward reachability analysis we compute an over-approximation of all possible values that string variables can take at each program point. Intersecting these with the attack pattern yields the potential attack strings if the program is vulnerable. Using backward analysis we compute an over-approximation of all possible inputs that can generate those attack strings. In addition to identifying existing vulnerabilities and their causes, these vulnerability signatures can be used to filter out malicious inputs. Our approach extends the prior work on automata-based string analysis by providing a backward symbolic analysis that includes a symbolic pre-image computation for deterministic finite automata on common string manipulating functions such as concatenation and replacement."
62.url:http://dx.doi.org/10.1109/ASE.2009.20
62.opinion:exclude

63.title:"title":"Weaving Context Sensitivity into Test Suite Construction"
63.abstract:"abstract":"Context-aware applications capture environmental changes as contexts and self-adapt their behaviors dynamically. Existing testing research has not explored context evolutions or their patterns inherent to individual test cases when constructing test suites. We propose the notation of context diversity as a metric to measure how many changes in contextual values of individual test cases. In this paper, we discuss how this notion can be incorporated in a test case generation process by pairing it with coverage-based test data selection criteria"
63.url:http://dx.doi.org/10.1109/ASE.2009.79
63.opinion:exclude

64.title:"title":"Optimizing a Structural Constraint Solver for Efficient Software Checking"
64.abstract:"abstract":"Several static analysis techniques, e.g., symbolic execution or scope-bounded checking, as well as dynamic analysis techniques, e.g., specification-based testing, use constraint solvers as an enabling technology. To analyze code that manipulates structurally complex data, the underlying solver must support structural constraints. Solving such constraints can be expensive due to the large number of aliasing possibilities that the solver must consider. This paper presents a novel technique to selectively reduce the number of test cases to be generated. Our technique applies across a class of structural constraint solvers. Experimental results show that the technique enables an order of magnitude reduction in the number of test cases to be considered."
64.url:http://dx.doi.org/10.1109/ASE.2009.52
64.opinion:exclude

65.title:"title":"A Case for Automated Debugging Using Data Structure Repair"
65.abstract:"abstract":"Automated debugging is becoming increasingly important as the size and complexity of software increases. This paper makes a case for using constraint-based data structure repair, a recently developed technique for fault recovery, as a basis for automated debugging. Data structure repair uses given structural integrity constraints for key data structures to monitor their correctness during the execution of a program. If a constraint violation is detected, repair performs mutations on the data structures, i.e., corrupt program state, and transforms it into another state, which satisfies the desired constraints. The primary goal of data structure repair is to transform an erroneous state into an acceptable state. Therefore, the mutations performed by repair actions provide a basis of debugging faults in code (assuming the errors are due to bugs). A key challenge to embodying this insight into a mechanical technique arises due to the difference in the concrete level of the program states and the abstract level of the program code: repair actions apply to concrete data structures that exist at runtime, whereas debugging applies to code. We observe that static structures (program variables) hold handles to dynamic structures (heap-allocated data), which allows bridging the gap between the abstract and concrete levels. We envision a tool-chain where a data structure repair tool generates repair logs that are used by a fault localization tool and a repair abstraction tool that apply in synergy to not only identify the location of fault(s) in code but also to synthesize debugging suggestions. An embodiment of our vision can significantly reduce the cost of developing reliable software."
65.url:http://dx.doi.org/10.1109/ASE.2009.92
65.opinion:accept

66.title:"title":"Generation of Simulation Views for Domain Specific Modeling Languages Based on the Eclipse Modeling Framework"
66.abstract:"abstract":"The generation of tools for domain specific modeling languages (DSMLs) is a key issue in model-driven development. Various tools already support the generation of domain-specific visual editors from models, but tool generation for visual behavior modeling languages is not yet supported in a satisfactory way. In this paper we propose a generic approach to specify DSML environments visually by models and transformation rules based on the Eclipse Modeling Framework (EMF). Editing rules define the behavior of generated visual editors, whereas simulation rules describe a model's operational semantics. From a DSML definition (model and transformation rules), an Eclipse plug-in is generated, implementing a visual DSML environment including an editor and (possibly multiple) simulators for different simulation views on the model. We present the basic components of Tiger2, our EMF-based generation environment, along the environment generation process for a small DSML modeling the behavior of ants in an ant hill."
66.url:http://dx.doi.org/10.1109/ASE.2009.46
66.opinion:exclude

67.title:"title":"Using Spectrum-Based Fault Localization for Test Case Grouping"
67.abstract:"abstract":"Model-based test case generation allows one to derive almost arbitrary numbers of test cases from models. If resulting test suites are executed against real implementations, there are often huge numbers of failed test cases. Thus, the analysis of the test execution, i.e. the identification of failures for error reporting, becomes a tedious and time consuming task. In this paper we investigate a technique for grouping test runs that most likely reveal the same failure. This reduces the post analysis time and enables the generation of small regression test suites. The test case grouping is implemented by means of spectrum-based fault localization at the level of the specification. We calculate the grouping by relating the spectra of the test cases. Besides a brief discussion of our approach we present results of applying our approach to the Session Initiation Protocol."
67.url:http://dx.doi.org/10.1109/ASE.2009.78
67.opinion:exclude

68.title:"title":"Cluster-Based I/O-Efficient LTL Model Checking"
68.abstract:"abstract":"I/O-efficient algorithms take the advantage of large capacities of external memories to verify huge state spaces even on a single machine with low-capacity RAM. On the other hand, parallel algorithms are used to accelerate the computation and their usage may significantly increase the amount of available RAM memory if clusters of computers are involved. Since both the large amount of memory and high speed computation are desired in verification of large-scale industrial systems, extending I/O-efficient model checking to work over a network of computers can bring substantial benefits. In this paper we propose an explicit state cluster-based I/O efficient LTL model checking algorithm that is capable to verify systems with approximately $10^{10}$ states within hours."
68.url:http://dx.doi.org/10.1109/ASE.2009.32
68.opinion:exclude

69.title:"title":"A Linear Programming Approach for Automated Localization of Multiple Faults"
69.abstract:"abstract":"In this paper, we address the problem of localizing faults by analyzing execution traces of successful and unsuccessful invocations of the application when run against a suite of tests. We present a new algorithm, based on a linear programming model, which is designed to be particularly effective for the case where multiple faults are present in the application under investigation. Through an extensive empirical study, we show that in the case of both single and multiple faults, our approach outperforms a host of prominent fault localization methods from the literature."
69.url:http://dx.doi.org/10.1109/ASE.2009.54
69.opinion:exclude

70.title:"title":"Lost in Translation: Forgetful Semantic Anchoring"
70.abstract:"abstract":"Assigning behavioral semantics to domain-specific languages (DSLs) opens the door for the application of formal methods, yet is largely an unresolved problem. Previously proposed solutions include semantic anchoring, in which a transformation from the DSL to an external framework that can supply both behavioral semantics and apply formal methods is constructed. The drawback of this approach is that it loses the structural constraints of the original DSL along with the details of the transformation, which can lead to erroneous results when formal methods are applied. We demonstrate this problem of &#x0201C;forgetful&#x0201D; semantic anchoring using existing approaches through a translation from dataflow systems to interface automata. We then describe our modeling tool FORMULA and apply it to the same example, showing how forgetful semantic anchoring can be avoided."
70.url:http://dx.doi.org/10.1109/ASE.2009.83
70.opinion:exclude

71.title:"title":"An IDE-based, Integrated Solution to Schema Evolution of Object-Oriented Software"
71.abstract:"abstract":"With the wide support for serialization in objectoriented programming languages, persistent objects have become common place. Retrieving previously &#x0201C;persisted&#x0201D; objects from classes whose schema changed is however difficult, and may lead to invalidating the consistency of the application. The ESCHER framework addresses this issues through an IDE-based approach that handles schema evolution by managing versions of the code and generating transformation functions automatically. The infrastructure also enforces class invariants to prevent the introduction of any corrupt objects. This article describes the principles behind invariant-safe schema evolution,and the design and implementation of the ESCHER system."
71.url:http://dx.doi.org/10.1109/ASE.2009.100
71.opinion:exclude

72.title:"title":"KaitoroBase: Visual Exploration of Software Architecture Documents"
72.abstract:"abstract":"This paper describes a software architecture documentation tool (KaitoroBase) built within the Thinkbase Visual Wiki to provide support for non-linear navigation and visualization of Software Architecture Documents (SADs) produced using the Attribute-Driven Design (ADD) method. This involves constructing the meta-model for the SAD in Freebase which provides the foundation for the graph-based interactive visualization enabled by Thinkbase. The resulting tool displays a graphical, high-level structure of SAD, allows for exploratory search, non-linear navigation, and at the same time connects to low-level details of SADs in a wiki."
72.url:http://dx.doi.org/10.1109/ASE.2009.26
72.opinion:exclude

73.title:"title":"phpModeler - A Web Model Extractor"
73.abstract:"abstract":"This paper presents phpModeler, a tool for reverse engineering of legacy php web applications that generates static UML diagrams showing resources which the current web page is using, its functions and dependencies it has on other web pages. Once the models describing individual web pages have been generated, phpModeler can analyze them and generate dependency models that for each entity in every page model show all entities dependent on it. phpModeler can also be used to highlight the difference between page models &#x02013; a feature that, when combined with a SVN repository shows the way how the current web page has evolved over time. phpModeler is a plugin for the Eclipse IDE."
73.url:http://dx.doi.org/10.1109/ASE.2009.40
73.opinion:exclude

74.title:"title":"Zoltar: A Toolset for Automatic Fault Localization"
74.abstract:"abstract":"Locating software components which are responsible for observed failures is the most expensive, error-prone phase in the software development life cycle. Automated diagnosis of software faults can improve the efficiency of the debugging process, and is therefore an important process for the development of dependable software. In this paper we present a toolset for automatic fault localization, dubbed Zoltar, which hosts a range of spectrum-based fault localization techniques featuring BARINEL, our latest algorithm. The toolset provides the infrastructure to automatically instrument the source code of software programs to produce runtime data, which is subsequently analyzed to return a ranked list of diagnosis candidates. Aimed at total automation (e.g., for runtime fault diagnosis), Zoltar has the capability of instrumenting the program under analysis with fault screeners as a run-time replacement for design-time test oracles."
74.url:http://dx.doi.org/10.1109/ASE.2009.27
74.opinion:exclude

75.title:"title":"Supporting Requirements Validation: The EuRailCheck Tool"
75.abstract:"abstract":"We present the EuRailCheck tool, which supports the formalization and the validation of requirements, based on the use of formal methods. The tool allows the user to analyze the requirements in natural language and to categorize and structure them. It allows to formalize the requirements into a subset of UML enriched with static and temporal constraints for which we defined a formal semantics. Finally, the tool allows to apply model checking techniques specialized for the validation of formal requirements. The tool has been developed and validated within a project funded by the European Railway Agency for the validation of the European Train Control System specification. By now, the tool has been successfully used by about thirty railway experts of different companies."
75.url:http://dx.doi.org/10.1109/ASE.2009.49
75.opinion:exclude

76.title:"title":"Loopfrog: A Static Analyzer for ANSI-C Programs"
76.abstract:"abstract":"Practical software verification is dominated by two major classes of techniques. The first is model checking, which provides total precision, but suffers from the state space explosion problem. The second is abstract interpretation, which is usually much less demanding, but often returns a high number of false positives. We present Loopfrog, a static analyzer that combines the best of both worlds: the precision of model checking and the performance of abstract interpretation. In contrast to traditional static analyzers, it also provides `leaping' counterexamples to aid in the diagnosis of errors."
76.url:http://dx.doi.org/10.1109/ASE.2009.35
76.opinion:exclude

77.title:"title":"A Tool Suite for the Generation and Validation of Configurations for Software Availability"
77.abstract:"abstract":"The Availability Management Framework (AMF) is a service responsible for managing the availability of services provided by applications that run under its control. Standardized by the Service Availability Forum (SAF), AMF requires for its operations a complete and compliant AMF configuration of the applications to be managed. In this paper, we describe two complementary and integrated tools for AMF configurations generation and validation. Indeed, writing manually an AMF configuration is a tedious and error prone task as a large number of requirements defined in the standard have to be taken into consideration during the process. One solution for ensuring compliance with the standard is the validation of the configurations against all the AMF requirements. For this, we have designed and implemented a domain model for AMF configurations and use it as a basis for an AMF configuration validator. To further ease the task of a configuration designer, we have devised and implemented a method for generating automatically AMF configurations."
77.url:http://dx.doi.org/10.1109/ASE.2009.18
77.opinion:exclude

78.title:"title":"A Tool for Attributed Goal-Oriented Requirements Analysis"
78.abstract:"abstract":"his paper presents an integrated supporting tool for Attributed Goal-Oriented Requirements Analysis (AGORA), which is an extended version of goal-oriented analysis. Our tool assists seamlessly requirements analysts and stakeholders in their activities throughout AGORA steps including constructing goal graphs with group work, prioritizing goals, and version control of goal graphs."
78.url:http://dx.doi.org/10.1109/ASE.2009.34
78.opinion:exclude

79.title:"title":"Jtop: Managing JUnit Test Cases in Absence of Coverage Information"
79.abstract:"abstract":"Test case management may make the testing process more efficient and thus accelerate software delivery. With the popularity of using JUnit for testing Java software, researchers have paid attention to techniques to manage JUnit test cases in regression testing of Java software. Typically, most existing test case management tools are based on the coverage information. However, coverage information may need extra efforts to obtain. In this paper, we present an Eclipse IDE plug-in (named Jtop) for managing JUnit test cases in absence of coverage information. Jtop statically analyzes the program under test and its corresponding JUnit test cases to perform the following management tasks: regression test case selection, test suite reduction and test case prioritization. Furthermore, Jtop also enables the programmer to manually manipulate test cases through a graphical user interface."
79.url:http://dx.doi.org/10.1109/ASE.2009.22
79.opinion:exclude

80.title:"title":"An Automated Tool for Generating UML Models from Natural Language Requirements"
80.abstract:"abstract":"This paper describes a domain independent tool, named, UML Model Generator from Analysis of Requirements (UMGAR), which generates UML models like the Use-case Diagram, Analysis class model, Collaboration diagram and Design class model from natural language requirements using efficient Natural Language Processing (NLP) tools. UMGAR implements a set of syntactic reconstruction rules to process complex requirements into simple requirements. UMGAR also provides a generic XMI parser to generate XMI files for visualizing the generated models in any UML modeling tool. With respect to the existing tools in this area, UMGAR provides more comprehensive support for generating models with proper relationships, which can be used for large requirement documents."
80.url:http://dx.doi.org/10.1109/ASE.2009.48
80.opinion:exclude

81.title:"title":"AOWP: Web-Specific AOP Framework for PHP"
81.abstract:"abstract":"Aspect-oriented programming (AOP) is a technique for modularizing crosscutting concerns (CCCs). A variety of CCCs can be found in typical Web applications. Most CCCs are scattered over Web-specific events such as page requests. AOWP, a PHP-based AOP framework, provides Web-specific aspect instantiations for dealing with session management and pointcut &#x00026; advice mechanisms for capturing Web-specific events. CCCs in Web applications can be clearly modularized by introducing AOWP."
81.url:http://dx.doi.org/10.1109/ASE.2009.45
81.opinion:exclude

82.title:"title":"A Modelling Language for Interactive Web Applications"
82.abstract:"abstract":"Web applications are increasingly becoming the most important platform for software applications in industry, with many modelling languages proposed to handle the complexity of developing, documenting and deploying these applications. New technology has allowed for development of Rich Internet Applications (RIAs) which increase usability and reliability; however, existing modelling languages fall short of modelling many of these new concepts. The research in this Ph.D. seeks to identify these new modelling challenges, and develop an approach that is suitable for modelling RIAs."
82.url:http://dx.doi.org/10.1109/ASE.2009.44
82.opinion:exclude

83.title:"title":"Automated Software Tool Support for Checking the Inconsistency of Requirements"
83.abstract:"abstract":"Handling inconsistency in software requirements is a complicated task which has attracted the interest of many groups of researchers. Formal and semi-formal specifications often have inconsistencies in the depicted requirements that need to be managed and resolved. This is particularly challenging when refining informal to formalized requirements. We propose an automated tool with traceability and consistency checking techniques to support analysis of requirements and traceability between different representations: textual, visual, informal and formal."
83.url:http://dx.doi.org/10.1109/ASE.2009.38
83.opinion:exclude

84.title:"title":"A Holistic Approach to Mobile Service Provisioning"
84.abstract:"abstract":"Mobile service provisioning is concerned with enabling handheld devices, such as phones and PDAs, to host services. This naturally raises significant challenges that stem from mobile devices' inherent mobility and limited resources. Furthermore, mobile services tend to operate in dynamic, insecure and heterogeneous operating environments. The aim of our research is to develop a novel toolkit for constructing mobile services that addresses such challenges. In getting started, we have taken a middleware-based approach that mediates interaction between clients and mobile services. This work offers a foundation upon which we intend to build using techniques that fall within the domain of automated software engineering."
84.url:http://dx.doi.org/10.1109/ASE.2009.69
84.opinion:exclude

85.title:"title":"Secure and Usable Requirements Engineering"
85.abstract:"abstract":"Software security is an increasingly important aspect of computing; however, it is still addressed as an after thought in too many development efforts. While a variety of approaches have been proposed for security requirements engineering, we find many still lacking with respect to their usability. In this proposal I describe my work in the area of security requirements engineering. SURE, Secure and Usable Requirements Engineering, is a new approach that supports non-security experts in order to specify security requirements from which testing artifacts can be derived. In addition, ASSURE, Automated Support for Secure and Usable Requirements Engineering, a system that implements the SURE technique is presented."
85.url:http://dx.doi.org/10.1109/ASE.2009.81
85.opinion:exclude

86.title:"title":"Goal-Based Testing of Semantic Web Services"
86.abstract:"abstract":"Web services, the reusable software components, have brought automation to Internet computing. However, since they are currently described syntactically using XML standards, i.e., SOAP, WSDL and UDDI, the automation of web services tasks, e.g., web service discovery, selection, composition and execution, is still a challenge. In order to make the most of automation in Service Oriented Architecture (SOA), the concept of the semantic web services (SWS), which are described semantically using an ontology language, have been introduced. The research related to testing and quality assurance aspects of web services is not mature~\\cite{Tsai2008}. This is especially true for semantic web services, since research to-date has mainly focused the automation of WS tasks. Furthermore, some semantic web service frameworks promote the client-oriented SOA, by formally specifying user requirements, called ``goal specification'', and automatically resolve it by appropriate web service detection. Keeping this in mind, we propose a novel approach for testing semantic web services based on user goal specifications. We believe this type of testing would add real value to web service users, unlike tradition web service testing."
86.url:http://dx.doi.org/10.1109/ASE.2009.74
86.opinion:exclude

87.title:"title":"Migration from Procedural Programming to Aspect Oriented Paradigm"
87.abstract:"abstract":"&#x02018;Separation of Concerns&#x02019; has long been a key issue in the field of Software Engineering. While developing a large complex and scalable Software, it can be observed that certain concerns have a tendency to get interleaved with the Core-Functionalities in such a way that they become inseparable. As a result of which the coder, who is supposed to be responsible only with the Core-Functionalities, is bound to take extra burden or botheration regarding the proper and accurate handling of these scattered and crosscutting concerns called Aspects. In our work we propose to devise a complete process of migrating a procedural form source-code to an aspect oriented program. We propose to devise a methodology to separate the scattered concerns from source-code through Code-Mining cascaded with a Traceability-Framework also to be framed by us. Thereafter we propose to devise a Design-Level Aspect Oriented Model for refactoring these separated code fragments in the Aspect Oriented Paradigm. Lastly, we propose to verify and validate the complete migration process."
87.url:http://dx.doi.org/10.1109/ASE.2009.41
87.opinion:exclude

88.title:"title":"Improving Component Dependency Resolution with Soft Constraints, Validation and Verification"
88.abstract:"abstract":"Software components are encapsulated units of execution which express dependencies through explicitly stated requirements and capabilities. These are used within component repositories to resolve sub-systems of deployable components. The main problem with this dependency resolution is not the lack of available solutions but the excess of them. Within current repositories the returned solution may not be optimal for the composers requirements and context. Improving dependency resolved solutions from repositories using soft constraints with validation and verification is the topic of our Ph.D. and discussed within this paper."
88.url:http://dx.doi.org/10.1109/ASE.2009.28
88.opinion:exclude

