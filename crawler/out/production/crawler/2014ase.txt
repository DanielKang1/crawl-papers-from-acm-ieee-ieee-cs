1.title:Keynote talk: the logic of information design
1.abstract:At least since the sixties, there have been many attempts to understand the logic of design, when the latter is broadly understood as a purposeful way of realizing an artefact. In this talk, I shall explore current methodologies to see how they may be adapted to cases in which what is being designed is information, in the sense of both a semantic artefact (e.g. a train timetable) and a communication process (e.g. the announcement that a specific train is leaving from particular platform).
1.url:http://doi.acm.org/10.1145/2642937.2643086
1.opinion:exclude

2.title:Keynote talk: EasyChair
2.abstract:The design and architecture of every very large Web service is unique, and EasyChair is not an exception. This talk overviews design features of EasyChair, which may be interesting for the software engineering community.
2.url:http://doi.acm.org/10.1145/2642937.2643085
2.opinion:exclude

3.title:Keynote talk: experiences from developing industrial software systems with long lifecycles
3.abstract:This talk highlights experiences from the research and development of industrial systems. Especially we look at automation of software engineering tasks to enhance developer productivity and how quality can be assured. The talk will also highlight the importance of understanding the domain and customer in order to make the best possible industrial software.
3.url:http://doi.acm.org/10.1145/2642937.2643087
3.opinion:exclude

4.title:Automated analysis of multithreaded programs for performance modeling
4.abstract:The behavior of multithreaded programs is often difficult to understand and predict. Synchronization operations and limited computational resources combine to produce complex non-linear dependencies between a program's configuration parameters and its performance. Performance models are used to understand these dependencies. Such models are complex, and constructing them requires a solid understanding of the program's behavior. As a result, building models of complex applications manually is extremely time-consuming and error-prone. In this paper we demonstrate that such models can be built automatically. This paper presents our approach for automatically modeling multithreaded programs. Our framework uses a combination of static and dynamic analyses of a single representative run of a system to build a model that can then be explored under a variety of configurations. We show how the models are constructed and show they accurately predict the performance of various multithreaded programs, including complex industrial applications.
4.url:http://doi.acm.org/10.1145/2642937.2642979
4.opinion:exclude

5.title:Behavioral resource-aware model inference
5.abstract:Software bugs often arise because of differences between what developers think their system does and what the system actually does. These differences frustrate debugging and comprehension efforts. We describe Perfume, an automated approach for inferring behavioral, resource-aware models of software systems from logs of their executions. These finite state machine models ease understanding of system behavior and resource use. Perfume improves on the state of the art in model inference by differentiating behaviorally similar executions that differ in resource consumption. For example, Perfume separates otherwise identical requests that hit a cache from those that miss it, which can aid understanding how the cache affects system behavior and removing cache-related bugs. A small user study demonstrates that using Perfume is more effective than using logs and another model inference tool for system comprehension. A case study on the TCP protocol demonstrates that Perfume models can help understand non-trivial protocol behavior. Perfume models capture key system properties and improve system comprehension, while being reasonably robust to noise likely to occur in real-world executions.
5.url:http://doi.acm.org/10.1145/2642937.2642988
5.opinion:accept

6.title:Targeted test input generation using symbolic-concrete backward execution
6.abstract:Knowing inputs that cover a specific branch or statement in a program is useful for debugging and regression testing. Symbolic backward execution (SBE) is a natural approach to find such targeted inputs. However, SBE struggles with complicated arithmetic, external method calls, and data-dependent loops that occur in many real-world programs. We propose symcretic execution, a novel combination of SBE and concrete forward execution that can efficiently find targeted inputs despite these challenges. An evaluation of our approach on a range of test cases shows that symcretic execution finds inputs in more cases than concolic testing tools while exploring fewer path segments. Integration of our approach will allow test generation tools to fill coverage gaps and static bug detectors to verify candidate bugs with concrete test cases.
6.url:http://doi.acm.org/10.1145/2642937.2642951
6.opinion:exclude

7.title:Discriminating influences among instructions in a dynamic slice
7.abstract:Dynamic slicing is an analysis that operates on program execution models (e.g., dynamic dependence graphs) to support the interpreation of program-execution traces. Given an execution event of interest (i.e., the slicing criterion), it solves for all instruction-execution events that either affect or are affected by that slicing criterion, and thereby reduces the search space to find influences within execution traces. Unfortunately, the resulting dynamic slices are still often prohibitively large for many uses. Despite this reduction search space, the dynamic slices are often still prohibitively large for many uses, and moreover, are provided without guidance of which and to what degree those influences are exerted. In this work, we present a novel approach to quantify the relevance of each instruction-execution event within a dynamic slice by its degree of relative influence on the slicing criterion. As such, we augment the dynamic slice with dynamic-relevance measures for each event in the slice, which can be used to guide and prioritize inspection of the events in the slice. We conducted an experiment that evaluates the ability of existing dynamic slicing and our approach, using dynamic relevance, to correctly identify sources of execution influence and state propagation. The results of the experiment show that inspections that were guided by traditional dynamic slicing to find the root cause for a failure reduced the search space by, on average, 61.3%. Further, inspections guided with the assistance of the new dynamic relevance reduced the search space by 96.2%.
7.url:http://doi.acm.org/10.1145/2642937.2642962
7.opinion:exclude

8.title:Dompletion: DOM-aware JavaScript code completion
8.abstract:JavaScript is a scripting language that plays a prominent role in modern web applications. It is dynamic in nature and interacts heavily with the Document Object Model (DOM) at runtime. These characteristics make providing code completion support to Java- Script programmers particularly challenging. We propose an auto- mated technique that reasons about existing DOM structures, dynamically analyzes the JavaScript code, and provides code completion suggestions for JavaScript code that interacts with the DOM through its APIs. Our automated code completion scheme is implemented in an open source tool called DOMPLETION. The results of our empirical evaluation indicate that (1) DOM structures exhibit patterns, which can be extracted and reasoned about in the con- text of code completion suggestions; (2) DOMPLETION can pro- vide code completion suggestions with a recall of 89%, precision of 90%, and an average time of 2.8 seconds.
8.url:http://doi.acm.org/10.1145/2642937.2642981
8.opinion:exclude

9.title:Continuous test generation: enhancing continuous integration with automated test generation
9.abstract:In object oriented software development, automated unit test generation tools typically target one class at a time. A class, however, is usually part of a software project consisting of more than one class, and these are subject to changes over time. This context of a class offers significant potential to improve test generation for individual classes. In this paper, we introduce Continuous Test Generation (CTG), which includes automated unit test generation during continuous integration (i.e., infrastructure that regularly builds and tests software projects). CTG offers several benefits: First, it answers the question of how much time to spend on each class in a project. Second, it helps to decide in which order to test them. Finally, it answers the question of which classes should be subjected to test generation in the first place. We have implemented CTG using the EvoSuite unit test generation tool, and performed experiments using eight of the most popular open source projects available on GitHub, ten randomly selected projects from the SF100 corpus, and five industrial projects. Our experiments demonstrate improvements of up to +58% for branch coverage and up to +69% for thrown undeclared exceptions, while reducing the time spent on test generation by up to +83%.
9.url:http://doi.acm.org/10.1145/2642937.2643002
9.opinion:exclude

10.title:Leveraging existing tests in automated test generation for web applications
10.abstract:To test web applications, developers currently write test cases in frameworks such as Selenium. On the other hand, most web test generation techniques rely on a crawler to explore the dynamic states of the application. The first approach requires much manual effort, but benefits from the domain knowledge of the developer writing the test cases. The second one is automated and systematic, but lacks the domain knowledge required to be as effective. We believe combining the two can be advantageous. In this paper, we propose to (1) mine the human knowledge present in the form of input values, event sequences, and assertions, in the human-written test suites, (2) combine that inferred knowledge with the power of automated crawling, and (3) extend the test suite for uncovered/unchecked portions of the web application under test. Our approach is implemented in a tool called Testilizer. An evaluation of our approach indicates that Testilizer (1) outperforms a random test generator, and (2) on average, can generate test suites with improvements of up to 150% in fault detection rate and up to 30% in code coverage, compared to the original test suite.
10.url:http://doi.acm.org/10.1145/2642937.2642991
10.opinion:exclude

11.title:Automated unit test generation for classes with environment dependencies
11.abstract:Automated test generation for object-oriented software typically consists of producing sequences of calls aiming at high code coverage. In practice, the success of this process may be inhibited when classes interact with their environment, such as the file system, network, user-interactions, etc. This leads to two major problems: First, code that depends on the environment can sometimes not be fully covered simply by generating sequences of calls to a class under test, for example when execution of a branch depends on the contents of a file. Second, even if code that is environment-dependent can be covered, the resulting tests may be unstable, i.e., they would pass when first generated, but then may fail when executed in a different environment. For example, tests on classes that make use of the system time may have failing assertions if the tests are executed at a different time than when they were generated. In this paper, we apply bytecode instrumentation to automatically separate code from its environmental dependencies, and extend the EvoSuite Java test generation tool such that it can explicitly set the state of the environment as part of the sequences of calls it generates. Using a prototype implementation, which handles a wide range of environmental interactions such as the file system, console inputs and many non-deterministic functions of the Java virtual machine (JVM), we performed experiments on 100 Java projects randomly selected from SourceForge (the SF100 corpus). The results show significantly improved code coverage - in some cases even in the order of +80%/+90%. Furthermore, our techniques reduce the number of unstable tests by more than 50%.
11.url:http://doi.acm.org/10.1145/2642937.2642986
11.opinion:exclude

12.title:Finding HTML presentation failures using image comparison techniques
12.abstract:Presentation failures in web applications can negatively affect an application's usability and user experience. To find such failures, testers must visually inspect the output of a web application or exhaustively specify invariants to automatically check a page's correctness. This makes finding presentation failures labor intensive and error prone. In this paper, we present a new automated approach for detecting and localizing presentation failures in web pages. To detect presentation failures, our approach uses image processing techniques to compare a web page and its oracle. Then, to localize the failures, our approach analyzes the page with respect to its visual layout and identifies the HTML elements likely to be responsible for the failure. We evaluated our approach on a set of real-world web applications and found that the approach was able to accurately detect failures and identify the faulty HTML elements.
12.url:http://doi.acm.org/10.1145/2642937.2642966
12.opinion:exclude

13.title:Accelerated test execution using GPUs
13.abstract:As product life-cycles become shorter and the scale and complexity of systems increase, accelerating the execution of large test suites gains importance. Existing research has primarily focussed on techniques that reduce the size of the test suite. By contrast, we propose a technique that accelerates test execution, allowing test suites to run in a fraction of the original time, by parallel execution with a Graphics Processing Unit (GPU). Program testing, which is in essence execution of the same program withmmultiple sets of test data, naturally exhibits the kind of data parallelism that can be exploited with GPUs. Our approach simultaneously executes the program with one test case per GPU thread. GPUs have severe limitations, and we discuss these in the context of our approach and define the scope of our applications. We observe speed-ups up to a factor of 27 compared to single-core execution on conventional CPUs with embedded systems benchmark programs.
13.url:http://doi.acm.org/10.1145/2642937.2642957
13.opinion:exclude

14.title:Seeking the user interface
14.abstract:User interface design and coding can be complex and messy. We describe a system that uses code search to simplify and automate the generation of such code. We start with a simple sketch of the desired interface along with a set of keywords describing the application context. We then use existing code search engines to find results based on the keywords. We look for potential Java-based user interface solutions within those results and apply a series of code transformations to the solutions to generate derivative solutions, aiming to get solutions that constitute only the user interface and that will compile and run. We run the resultant solutions and compare the generated interfaces to the user's sketches. Finally, we let programmers interact with the matched solutions and return the running code for the solutions they choose. The system can be used not only for generating initial user interface code for an application, but also for exploring alternative interfaces and for looking at the user interfaces in a code repository.
14.url:http://doi.acm.org/10.1145/2642937.2642976
14.opinion:exclude

15.title:Interrogative-guided re-ranking for question-oriented software text retrieval
15.abstract:In many software engineering tasks, question-oriented text retrieval is often used to help developers search for software artifacts. In this paper, we propose an interrogative-guided re-ranking approach for question-oriented software text retrieval. Since different interrogatives usually indicate users' different search focuses, we firstly label 9 kinds of question-answer pairs according to the common interrogatives. Then, we train document classifiers by using 1,826 questions along with 2,460 answers from StackOverflow, apply the classifiers to our document repository and present a re-ranking approach to improve the retrieval precision. In software document classification, our classifiers achieve the average precision, recall and F-measure of 56.2%, 90.9% and 69.4% respectively. Our re-ranking approach presents 9.6% improvement in nDCG@1 upon the baseline, and we also obtain 8.1% improvement in nDCG@10 when more candidates are included.
15.url:http://doi.acm.org/10.1145/2642937.2642953
15.opinion:exclude

16.title:An empirical study on reducing omission errors in practice
16.abstract:Since studies based on mining software repositories sparked interests in the field of guiding software changes, many change recommendation techniques have been proposed to reduce omission errors. While these techniques only used existing software commit data sets to evaluate their effectiveness, we use the data set of supplementary patches which correct initial incomplete patches to investigate how much actual omission errors could be prevented in practice. We find that while a single trait is inadequate, combining multiple traits is limited as well for predicting supplementary change locations. Neither does a boosting approach improve accuracy significantly, nor filtering based on developer or package specific information necessarily improves the accuracy. Developers rarely repeat the same mistakes, making the potential value of history-based change prediction less promising. We share our skepticism that omission errors are hard to prevent in practice based on a systematic evaluation of a supplementary patch data set.
16.url:http://doi.acm.org/10.1145/2642937.2642956
16.opinion:exclude

17.title:Fusion fault localizers
17.abstract:Many spectrum-based fault localization techniques have been proposed to measure how likely each program element is the root cause of a program failure. For various bugs, the best technique to localize the bugs may differ due to the characteristics of the buggy programs and their program spectra. In this paper, we leverage the diversity of existing spectrum-based fault localization techniques to better localize bugs using data fusion methods. Our proposed approach consists of three steps: score normalization, technique selection, and data fusion. We investigate two score normalization methods, two technique selection methods, and five data fusion methods resulting in twenty variants of Fusion Localizer. Our approach is bug specific in which the set of techniques to be fused are adaptively selected for each buggy program based on its spectra. Also, it requires no training data, i.e., execution traces of the past buggy programs. We evaluate our approach on a common benchmark dataset and a dataset consisting of real bugs from three medium to large programs. Our evaluation demonstrates that our approach can significantly improve the effectiveness of existing state-of-the-art fault localization techniques. Compared to these state-of-the-art techniques, the best variants of Fusion Localizer can statistically significantly reduce the amount of code to be inspected to find all bugs. Our best variants can increase the proportion of bugs localized when developers only inspect the top 10% most suspicious program elements by more than 10% and increase the number of bugs that can be successfully localized when developers only inspect up to 10 program blocks by more than 20%.
17.url:http://doi.acm.org/10.1145/2642937.2642983
17.opinion:exclude

18.title:Automated variability analysis and testing of an E-commerce site.: an experience report
18.abstract:In this paper, we report on our experience on the development of La Hilandera, an e-commerce site selling haberdashery products and craft supplies in Europe. The store has a huge input space where customers can place almost three millions of different orders which made testing an extremely difficult task. To address the challenge, we explored the applicability of some of the practices for variability management in software product lines. First, we used a feature model to represent the store input space which provided us with a variability view easy to understand, share and discuss with all the stakeholders. Second, we used techniques for the automated analysis of feature models for the detection and repair of inconsistent and missing configuration settings. Finally, we used test selection and prioritization techniques for the generation of a manageable and effective set of test cases. Our findings, summarized in a set of lessons learnt, suggest that variability techniques could successfully address many of the challenges found when developing e-commerce sites.
18.url:http://doi.acm.org/10.1145/2642937.2642939
18.opinion:exclude

19.title:PrefFinder: getting the right preference in configurable software systems
19.abstract:Highly configurable software, such as web browsers,databases or office applications, have a large number of preferences that the user can customize, but documentation of them may be scarce or distributed. A user, tester or service technician may have to search through hundreds or thousands of choices in multiple documents when trying to identify which preference will modify a particular system behavior. In this paper we present PrefFinder, a natural language framework that finds (and changes) user preferences. It is tied into an application's preference system and static documentation. We have instantiated PrefFinder as a plugin on two open source applications, and as a stand-alone GUI for an industrial application. PrefFinder finds thecorrect answer between 76-96% of the time on more than 175 queries. When compared to asking questions on a help forum or through the company's service center, we can potentially save days or even weeks of time.
19.url:http://doi.acm.org/10.1145/2642937.2643009
19.opinion:exclude

20.title:MiL testing of highly configurable continuous controllers: scalable search using surrogate models
20.abstract:Continuous controllers have been widely used in automotive domain to monitor and control physical components. These controllers are subject to three rounds of testing: Model-in-the-Loop (MiL), Software-in-the-Loop and Hardware-in-the-Loop. In our earlier work, we used meta-heuristic search to automate MiL testing of fixed configurations of continuous controllers. In this paper, we extend our work to support MiL testing of all feasible configurations of continuous controllers. Specifically, we use a combination of dimensionality reduction and surrogate modeling techniques to scale our earlier MiL testing approach to large, multi-dimensional input spaces formed by configuration parameters. We evaluated our approach by applying it to a complex, industrial continuous controller. Our experiment shows that our approach identifies test cases indicating requirements violations. Further, we demonstrate that dimensionally reduction helps generate surrogate models with higher prediction accuracy. Finally, we show that combining our search algorithm with surrogate modelling improves its efficiency for two out of three requirements.
20.url:http://doi.acm.org/10.1145/2642937.2642978
20.opinion:exclude

21.title:Assertion guided abstraction: a cooperative optimization for dynamic partial order reduction
21.abstract:We propose a new method for reducing the interleaving space during stateless model checking of multithreaded C/C++ programs. The problem is challenging because of the exponential growth of possible interleavings between threads. We have developed a new method, called assertion guided abstraction, which leverages both static and dynamic program analyses in a cooperative framework to reduce the interleaving space. Unlike existing methods that consider all interleavings of all conflicting memory accesses in a program, our new method relies on a new notion of predicate dependence based on which we can soundly abstract the interleaving space to only those conflicting memory accesses that may cause assertion violations and/or deadlocks. Our experimental evaluation of assertion guided abstraction on open source benchmarks shows that it is capable of achieving a significant reduction, thereby allowing for the verification of programs that were previously too complex for existing algorithms to handle.
21.url:http://doi.acm.org/10.1145/2642937.2642998
21.opinion:exclude

22.title:Symbolic state validation through runtime data
22.abstract:Real world programs are typically built on top of many library functions. Symbolic analysis of these programs generally requires precise models of these functions? Application Programming Interfaces (APIs), which are mostly unavailable because these models are costly to construct. A variant approach of symbolic analysis is to over-approximate the return values of those APIs that have not been modeled. However, such approximation can induce many unreachable symbolic states, which are expensive to validate manually. In this paper, we propose a static approach to automatically validating the reported anomalous symbolic states. The validation makes use of the available runtime data of the un-modeled APIs collected from previous program executions. We show that the symbolic state validation problem can be cast as a MAX-SAT problem and solved by existing constraint solvers. Our approach is motivated by two observations. We may bind the symbolic parameters in un-modeled APIs based on observations made in former executions by other programs. The binding enables us to use the corresponding observed concrete return values of APIs to validate the symbolic states arising from the over-approximated return values of the un-modeled APIs. Second, some symbolic constraints can be accurately evaluated despite the imprecision of the over-approximated symbolic values. Our technique found 80 unreported bugs when it was applied to 10 popular programs with a total of 1.5 million lines of code. All of them can be confirmed by test cases. Our technique presents a promising way to apply the big data paradigm to software engineering. It provides a mechanism to validate the symbolic states of a project by leveraging the many concrete input-output values of APIs collected from other projects.
22.url:http://doi.acm.org/10.1145/2642937.2642973
22.opinion:exclude

23.title:Verifying self-adaptive applications suffering uncertainty
23.abstract:Self-adaptive applications address environmental dynamics systematically. They can be faulty and exhibit runtime errors when environmental dynamics are not considered adequately. It becomes more severe when uncertainty exists in their sensing and adaptation to environments. Existing work verifies self-adaptive applications, but does not explicitly consider environmental constraints or uncertainty. This gives rise to inaccurate verification results. In this paper, we address this problem by proposing a novel approach to verifying self-adaptive applications suffering uncertainty in their environmental interactions. It builds Interactive State Machine (ISM) models for such applications and verifies them with explicit consideration of environmental constraints and uncertainty. It then refines verification results by prioritizing counterexamples according to their probabilities. We experimentally evaluated our approach with real-life self-adaptive applications, and the experimental results confirmed its effectiveness. Our approach reported 200-660% more counterexamples than not considering uncertainty, and eliminated all false counterexamples caused by ignoring environmental constraints.
23.url:http://doi.acm.org/10.1145/2642937.2642999
23.opinion:exclude

24.title:Automated synthesis and deployment of cloud applications
24.abstract:Complex networked applications are assembled by connecting software components distributed across multiple machines. Building and deploying such systems is a challenging problem which requires a significant amount of expertise: the system architect must ensure that all component dependencies are satisfied, avoid conflicting components, and add the right amount of component replicas to account for quality of service and fault-tolerance. In a cloud environment, one also needs to minimize the virtual resources provisioned upfront, to reduce the cost of operation. Once the full architecture is designed, it is necessary to correctly orchestrate the deployment phase, to ensure all components are started and connected in the right order. We present a toolchain that automates the assembly and deployment of such complex distributed applications. Given as input a high-level specification of the desired system, the set of available components together with their requirements, and the maximal amount of virtual resources to be committed, it synthesizes the full architecture of the system, placing components in an optimal manner using the minimal number of available machines, and automatically deploys the complete system in a cloud environment.
24.url:http://doi.acm.org/10.1145/2642937.2642980
24.opinion:exclude

25.title:Concurrent transformation components using contention context sensors
25.abstract:Sometimes components are conservatively implemented as thread-safe, while during the actual execution they are only accessed from one thread. In these scenarios, overly conservative assumptions lead to suboptimal performance. The contribution of this paper is a component architecture that combines the benefits of different synchronization mechanisms to implement thread-safe concurrent components. Based on the thread contention monitored at runtime, context-aware composition and optimization select the appropriate mechanism. On changing contention, it revises this decision automatically and transforms the components accordingly. We implemented this architecture for concurrent queues, sets, and ordered sets. In all three cases, experimental evaluation shows close to optimal performance regardless of the actual contention. As a consequence, programmers can focus on the semantics of their systems and, e.g., conservatively use thread-safe components to assure consistency of their data, while deferring implementation and optimization decisions to contention-context-aware composition at runtime.
25.url:http://doi.acm.org/10.1145/2642937.2642995
25.opinion:exclude

26.title:Assigning time budgets to component functions in the design of time-critical automotive systems
26.abstract:The adoption of AUTOSAR and Model Driven Engineering (MDE) for the design of automotive software architectures allows an early analysis of system properties and the automatic synthesis of architecture and software implementation. To select and configure the architecture with respect to timing constraints, knowledge about the worst case execution times (WCET) of functions is required. An accurate evaluation of the WCET is only possible when reusing legacy functionality or very late in the development and procurement process. To drive the integration of SW components belonging to systems with timing constraints, automotive methodologies propose to assign WCET budgets to functions. This paper presents two solutions to assign budgets, while considering at the same time the problem of SW/HW synthesis. The first solution is a one-step algorithm. The second is an iterative improvement procedure with a staged approach that scales better to very large size systems. Both methods are evaluated on industrial systems to study their effectiveness and scalability.
26.url:http://doi.acm.org/10.1145/2642937.2643015
26.opinion:exclude

27.title:symMMU: symbolically executed runtime libraries for symbolic memory access
27.abstract:Symbolic execution calls for specialized address translation. Unlike a pointer on a traditional machine model, which corresponds to a single address, a symbolic pointer may represent multiple feasible addresses. A symbolic pointer dereference manipulates symbolic state, potentially submitting many theorem prover requests in the process. Hence, design and management of symbolic accesses critically affects symbolic executor performance, complexity, and completeness. We demonstrate a symbolic execution extension, the symMMU, which separates access dispatch mechanism from policy by servicing memory accesses with symbolically executed runtime software handlers. This runtime code concisely represents access policies including pointer concretization, address forking, and symbolic indexing. These policies are competitive with a baseline hard-coded memory policy. Furthermore, the symMMU cleanly supports handlers for profiling, heap analysis, and demand allocated symbolic buffers. In practice, the symMMU flags hardware-validated bugs for over a thousand Linux program binaries.
27.url:http://doi.acm.org/10.1145/2642937.2642974
27.opinion:exclude

28.title:Evaluation of string constraint solvers in the context of symbolic execution
28.abstract:Symbolic execution tools query constraint solvers for tasks such as determining the feasibility of program paths. Therefore, the effectiveness of such tools depends on their constraint solvers. Most modern constraint solvers for primitive types are efficient and accurate. However, research on constraint solvers for complex types, such as strings, is less converged. In this paper, we introduce two new solver adequacy criteria, modeling cost and accuracy, to help the user identify an adequate solver. Using these metrics and performance criterion, we evaluate four distinct string constraint solvers in the context of symbolic execution. Our results show that, depending on the needs of the user and composition of the program, one solver might be more appropriate than another. Yet, none of the solvers exhibit the best results for all programs. Hence, if resources permit, the user will benefit the most from executing all solvers in parallel and enabling communication between solvers.
28.url:http://doi.acm.org/10.1145/2642937.2643003
28.opinion:exclude

29.title:Incremental symbolic execution for automated test suite maintenance
29.abstract:Scaling software analysis techniques based on source-code, such as symbolic execution and data flow analyses, remains a challenging problem for systematically checking software systems. In this work, we aim to efficiently apply symbolic execution in increments based on versions of code. Our technique is based entirely on dynamic analysis and patches completely automated test suites based on the code changes. Our key insight is that we can eliminate constraint solving for unchanged code by checking constraints using the test suite of a previous version. Checking constraints is orders of magnitude faster than solving them. This is in contrast to previous techniques that rely on inexact static analysis or cache of previously solved constraints. Our technique identifies ranges of paths, each bounded by two concrete tests from the previous test suite. Exploring these path ranges covers all paths affected by code changes up to a given depth bound. Our experiments show that incremental symbolic execution based on dynamic analysis is an order of magnitude faster than running complete standard symbolic execution on the new version of code.
29.url:http://doi.acm.org/10.1145/2642937.2642961
29.opinion:exclude

30.title:Program analysis for secure big data processing
30.abstract:The ubiquitous nature of computers is driving a massive increase in the amount of data generated by humans and machines. Two natural consequences of this are the increased efforts to (a) derive meaningful information from accumulated data and (b) ensure that data is not used for unintended purposes. In the direction of analyzing massive amounts of data (a.), tools like MapReduce, Spark, Dryad and higher level scripting languages like Pig Latin and DryadLINQ have significantly improved corresponding tasks for software developers. The second, but equally important aspect of ensuring confidentiality (b.), has seen little support emerge for programmers: while advances in cryptographic techniques allow us to process directly on encrypted data, programmer-friendly and efficient ways of programming such data analysis jobs are still missing. This paper presents novel data flow analyses and program transformations for Pig Latin, that automatically enable the execution of corresponding scripts on encrypted data. We avoid fully homomorphic encryption because of its prohibitively high cost; instead, in some cases, we rely on a minimal set of operations performed by the client. We present the algorithms used for this translation, and empirically demonstrate the practical performance of our approach as well as improvements for programmers in terms of the effort required to preserve data confidentiality.
30.url:http://doi.acm.org/10.1145/2642937.2643006
30.opinion:exclude

31.title:Multi-objective optimization in rule-based design space exploration
31.abstract:Design space exploration (DSE) aims to find optimal design candidates of a domain with respect to different objectives where design candidates are constrained by complex structural and numerical restrictions. Rule-based DSE aims to find such candidates that are reachable from an initial model by applying a sequence of exploration rules. Solving a rule-based DSE problem is a difficult challenge due to the inherently dynamic nature of the problem. In the current paper, we propose to integrate multi-objective optimization techniques by using Non-dominated Sorting Genetic Algorithms (NSGA) to drive rule-based design space exploration. For this purpose, finite populations of the most promising design candidates are maintained wrt. different optimization criteria. In our context, individuals of a generation are defined as a sequence of rule applications leading from an initial model to a candidate model. Populations evolve by mutation and crossover operations which manipulate (change, extend or combine) rule execution sequences to yield new individuals. Our multi-objective optimization approach for rule-based DSE is domain independent and it is automated by tooling built on the Eclipse framework. The main added value is to seamlessly lift multi-objective optimization techniques to the exploration process preserving both domain independence and a high-level of abstraction. Design candidates will still be represented as models and the evolution of these models as rule execution sequences. Constraints are captured by model queries while objectives can be derived both from models or rule applications.
31.url:http://doi.acm.org/10.1145/2642937.2643005
31.opinion:exclude

32.title:Understanding performance stairs: elucidating heuristics
32.abstract:How do experts navigate the huge space of implementations for a given specification to find an efficient choice with minimal searching? Answer: They use "heuristics" -- rules of thumb that are more street wisdom than scientific fact. We provide a scientific justification for Dense Linear Algebra (DLA) heuristics by showing that only a few decisions (out of many possible) are critical to performance; once these decisions are made, the die is cast and only relatively minor performance improvements are possible. The (implementation x performance) space of DLA is stair-stepped. Each stair is a set of implementations with very similar performance and (surprisingly) share key design decision(s). High-performance stairs align with heuristics that prescribe certain decisions in a particular context. Stairs also tell us how to tailor the search engine of a DLA code generator to reduce the time it needs to find implementations that are as good or better than those crafted by experts.
32.url:http://doi.acm.org/10.1145/2642937.2642975
32.opinion:exclude

33.title:Fine-grained and accurate source code differencing
33.abstract:At the heart of software evolution is a sequence of edit actions, called an edit script, made to a source code file. Since software systems are stored version by version, the edit script has to be computed from these versions, which is known as a complex task. Existing approaches usually compute edit scripts at the text granularity with only add line and delete line actions. However, inferring syntactic changes from such an edit script is hard. Since moving code is a frequent action performed when editing code, it should also be taken into account. In this paper, we tackle these issues by introducing an algorithm computing edit scripts at the abstract syntax tree granularity including move actions. Our objective is to compute edit scripts that are short and close to the original developer intent. Our algorithm is implemented in a freely-available and extensible tool that has been intensively validated.
33.url:http://doi.acm.org/10.1145/2642937.2642982
33.opinion:exclude

34.title:Combining rule-based and information retrieval techniques to assign software change requests
34.abstract:Change Requests (CRs) are key elements to software maintenance and evolution. Finding the appropriate developer to a CR is crucial for obtaining the lowest, economically feasible, fixing time. Nevertheless, assigning CRs is a labor-intensive and time consuming task. In this paper, we present a semi-automated CR assignment approach which combine rule-based and information retrieval techniques. The approach emphasizes the use of contextual information, essential to effective assignments, and puts the development team in control of the assignment rules, toward making its adoption easier. Results of an empirical evaluation showed that the approach is up to 46,5% more accurate than approaches which rely solely on machine learning techniques.
34.url:http://doi.acm.org/10.1145/2642937.2642964
34.opinion:exclude

35.title:Recommendation system for software refactoring using innovization and interactive dynamic optimization
35.abstract:We propose a novel recommendation tool for software refactoring that dynamically adapts and suggests refactorings to developers interactively based on their feedback and introduced code changes. Our approach starts by finding upfront a set of non-dominated refactoring solutions using NSGA-II to improve software quality, reduce the number of refactorings and increase semantic coherence. The generated non-dominated refactoring solutions are analyzed using our innovization component to extract some interesting common features between them. Based on this analysis, the suggested refactorings are ranked and suggested to the developer one by one. The developer can approve, modify or reject each suggested refactoring, and this feedback is used to update the ranking of the suggested refactorings. After a number of introduced code changes, a local search is performed to update and adapt the set of refactoring solutions suggested by NSGA-II. We evaluated this tool on four large open source systems and one industrial project provided by our partner. Statistical analysis of our experiments over 31 runs shows that the dynamic refactoring approach performed significantly better than three other search-based refactoring techniques, manual refactorings, and one refactoring tool not based on heuristic search.
35.url:http://doi.acm.org/10.1145/2642937.2642965
35.opinion:exclude

36.title:Recommending refactorings based on team co-maintenance patterns
36.abstract:Refactoring aims at restructuring existing source code when undisciplined development activities have deteriorated its comprehensibility and maintainability. There exist various approaches for suggesting refactoring opportunities, based on different sources of information, e.g., structural, semantic, and historical. In this paper we claim that an additional source of information for identifying refactoring opportunities, sometimes orthogonal to the ones mentioned above, is team development activity. When the activity of a team working on common modules is not aligned with the current design structure of a system, it would be possible to recommend appropriate refactoring operations---e.g., extract class/method/package---to adjust the design according to the teams' activity patterns. Results of a preliminary study---conducted in the context of extract class refactoring---show the feasibility of the approach, and also suggest that this new refactoring dimension can be complemented with others to build better refactoring recommendation tools.
36.url:http://doi.acm.org/10.1145/2642937.2642948
36.opinion:exclude

37.title:Diver: precise dynamic impact analysis using dependence-based trace pruning
37.abstract:Impact analysis determines the effects that the behavior of program entities, or changes to them, can have on the rest of the system. Dynamic impact analysis is one practical form that computes smaller impact sets than static alternatives for concrete sets of executions. However, existing dynamic approaches can still produce impact sets that are too large to be useful. To address this problem, we present a novel dynamic impact analysis called DIVER that exploits static dependencies to identify runtime impacts much more precisely without reducing safety and at acceptable costs. Our preliminary empirical evaluation shows that DIVER can significantly increase the precision of dynamic impact analysis.
37.url:http://doi.acm.org/10.1145/2642937.2642950
37.opinion:exclude

38.title:Automating regression verification
38.abstract:Regression verification is an approach complementing regression testing with formal verification. The goal is to formally prove that two versions of a program behave either equally or differently in a precisely specified way. In this paper, we present a novel automatic approach for regression verification that reduces the equivalence of two related imperative integer programs to Horn constraints over uninterpreted predicates. Subsequently, state-of-the-art SMT solvers are used to solve the constraints. We have implemented the approach, and our experiments show non-trivial integer programs that can now be proved equivalent without further user input.
38.url:http://doi.acm.org/10.1145/2642937.2642987
38.opinion:exclude

39.title:An empirical evaluation and comparison of manual and automated test selection
39.abstract:Regression test selection speeds up regression testing by re-running only the tests that can be affected by the most recent code changes. Much progress has been made on research in automated test selection over the last three decades, but it has not translated into practical tools that are widely adopted. Therefore, developers either re-run all tests after each change or perform manual test selection. Re-running all tests is expensive, while manual test selection is tedious and error-prone. Despite such a big trade-off, no study assessed how developers perform manual test selection and compared it to automated test selection. This paper reports on our study of manual test selection in practice and our comparison of manual and automated test selection. We are the first to conduct a study that (1) analyzes data from manual test selection, collected in real time from 14 developers during a three-month study and (2) compares manual test selection with an automated state-of-the-research test-selection tool for 450 test sessions. Almost all developers in our study performed manual test selection, and they did so in mostly ad-hoc ways. Comparing manual and automated test selection, we found the two approaches to select different tests in each and every one of the 450 test sessions investigated. Manual selection chose more tests than automated selection 73% of the time (potentially wasting time) and chose fewer tests 27% of the time (potentially missing bugs). These results show the need for better automated test-selection techniques that integrate well with developers' programming environments.
39.url:http://doi.acm.org/10.1145/2642937.2643019
39.opinion:exclude

40.title:Taming test inputs for separation assurance
40.abstract:The Next Generation Air Transportation System (NextGen) advocates the use of innovative algorithms and software to address the increasing load on air-traffic control. AutoResolver [12] is a large, complex NextGen component that provides separation assurance between multiple airplanes up to 20 minutes ahead of time. Our work targets the development of a light-weight, automated testing environment for AutoResolver. The input space of AutoResolver consists of airplane trajectories, each trajectory being a sequence of hundreds of points in the three-dimensional space. Generating meaningful test cases for AutoResolver that cover its behavioral space to a satisfactory degree is a major challenge. We discuss how we tamed this input space to make it amenable to test case generation techniques, as well as how we developed and validated an extensible testing environment around AutoResolver.
40.url:http://doi.acm.org/10.1145/2642937.2642940
40.opinion:exclude

41.title:Transferring an automated test generation tool to practice: from pex to fakes and code digger
41.abstract:Producing industry impacts has been an important, yet challenging task for the research community. In this paper, we report experiences on successful technology transfer of Pex and its relatives (tools derived from or associated with Pex) from Microsoft Research and lessons learned from more than eight years of research efforts by the Pex team in collaboration with academia. Moles, a tool associated with Pex, was shipped as Fakes with Visual Studio since August 2012, benefiting a huge user base of Visual Studio around the world. The number of download counts of Pex and its lightweight version called Code Digger has reached tens of thousands within one or two years. Pex4Fun (derived from Pex), an educational gaming website released since June 2010, has achieved high educational impacts, reflected by the number of clicks of the "Ask Pex!" button (indicating the attempts made by users to solve games in Pex4Fun) as over 1.5 million till July 2014. Evolved from Pex4Fun, the Code Hunt website has been used in a very large programming competition. In this paper, we discuss the technology background, tool overview, impacts, project timeline, and lessons learned from the project. We hope that our reported experiences can inspire more high-impact technology-transfer research from the research community.
41.url:http://doi.acm.org/10.1145/2642937.2642941
41.opinion:exclude

42.title:Angels and monsters: an empirical investigation of potential test effectiveness and efficiency improvement from strongly subsuming higher order mutation
42.abstract:We study the simultaneous test effectiveness and efficiency improvement achievable by Strongly Subsuming Higher Order Mutants (SSHOMs), constructed from 15,792 first order mutants in four Java programs. Using SSHOMs in place of the first order mutants they subsume yielded a 35%-45% reduction in the number of mutants required, while simultaneously improving test efficiency by 15% and effectiveness by between 5.6% and 12%. Trivial first order faults often combine to form exceptionally non-trivial higher order faults; apparently innocuous angels can combine to breed monsters. Nevertheless, these same monsters can be recruited to improve automated test effectiveness and efficiency.
42.url:http://doi.acm.org/10.1145/2642937.2643008
42.opinion:exclude

43.title:Scaling exact multi-objective combinatorial optimization by parallelization
43.abstract:Multi-Objective Combinatorial Optimization (MOCO) is fundamental to the development and optimization of software systems. We propose five novel parallel algorithms for solving MOCO problems exactly and efficiently. Our algorithms rely on off-the-shelf solvers to search for exact Pareto-optimal solutions, and they parallelize the search via collaborative communication, divide-and-conquer, or both. We demonstrate the feasibility and performance of our algorithms by experiments on three case studies of software-system designs. A key finding is that one algorithm, which we call FS-GIA, achieves substantial (even super-linear) speedups that scale well up to 64 cores. Furthermore, we analyze the performance bottlenecks and opportunities of our parallel algorithms, which facilitates further research on exact, parallel MOCO.
43.url:http://doi.acm.org/10.1145/2642937.2642971
43.opinion:exclude

44.title:42 variability bugs in the linux kernel: a qualitative analysis
44.abstract:Feature-sensitive verification pursues effective analysis of the exponentially many variants of a program family. However, researchers lack examples of concrete bugs induced by variability, occurring in real large-scale systems. Such a collection of bugs is a requirement for goal-oriented research, serving to evaluate tool implementations of feature-sensitive analyses by testing them on real bugs. We present a qualitative study of 42 variability bugs collected from bug-fixing commits to the Linux kernel repository. We analyze each of the bugs, and record the results in a database. In addition, we provide self-contained simplified C99 versions of the bugs, facilitating understanding and tool evaluation. Our study provides insights into the nature and occurrence of variability bugs in a large C software system, and shows in what ways variability affects and increases the complexity of software bugs.
44.url:http://doi.acm.org/10.1145/2642937.2642990
44.opinion:exclude

45.title:Automating the formalization of product comparison matrices
45.abstract:Product Comparison Matrices (PCMs) form a rich source of data for comparing a set of related and competing products over numerous features. Despite their apparent simplicity, PCMs contain heterogeneous, ambiguous, uncontrolled and partial information that hinders their efficient exploitations. In this paper, we formalize PCMs through model-based automated techniques and develop additional tooling to support the edition and re-engineering of PCMs. 20 participants used our editor to evaluate the PCM metamodel and automated transformations. The results over 75 PCMs from Wikipedia show that (1) a significant proportion of the formalization of PCMs can be automated -- 93.11% of the 30061 cells are correctly formalized; (2) the rest of the formalization can be realized by using the editor and mapping cells to existing concepts of the metamodel. The automated approach opens avenues for engaging a community in the mining, re-engineering, edition, and exploitation of PCMs that now abound on the Internet.
45.url:http://doi.acm.org/10.1145/2642937.2643000
45.opinion:exclude

46.title:Tracking load-time configuration options
46.abstract:Highly-configurable software systems are pervasive, although configuration options and their interactions raise complexity of the program and increase maintenance effort. Especially load-time configuration options, such as parameters from command-line options or configuration files, are used with standard programming constructs such as variables and if statements intermixed with the program's implementation; manually tracking configuration options from the time they are loaded to the point where they may influence control-flow decisions is tedious and error prone. We design and implement Lotrack, an extended static taint analysis to automatically track configuration options. Lotrack derives a configuration map that explains for each code fragment under which configurations it may be executed. An evaluation on Android applications shows that Lotrack yields high accuracy with reasonable performance. We use Lotrack to empirically characterize how much of the implementation of Android apps depends on the platform's configuration options or interactions of these options.
46.url:http://doi.acm.org/10.1145/2642937.2643001
46.opinion:exclude

47.title:Statistical learning approach for mining API usage mappings for code migration
47.abstract:The same software product nowadays could appear in multiple platforms and devices. To address business needs, software companies develop a software product in a programming language and then migrate it to another one. To support that process, semi-automatic migration tools have been proposed. However, they require users to manually define the mappings between the respective APIs of the libraries used in two languages. To reduce such manual effort, we introduce StaMiner, a novel data-driven approach that statistically learns the mappings between APIs from the corpus of the corresponding client code of the APIs in two languages Java and C#. Instead of using heuristics on the textual or structural similarity between APIs in two languages to map API methods and classes as in existing mining approaches, StaMiner is based on a statistical model that learns the mappings in such a corpus and provides mappings for APIs with all possible arities. Our empirical evaluation on several projects shows that StaMiner can detect API usage mappings with higher accuracy than a state-of-the-art approach. With the resulting API mappings mined by StaMiner, Java2CSharp, an existing migration tool, could achieve a higher level of accuracy.
47.url:http://doi.acm.org/10.1145/2642937.2643010
47.opinion:exclude

48.title:Compatibility testing using patterns-based trace comparison
48.abstract:When composing a system from components, we need to ensure that the components are compatible. This is commonly achieved by components interacting only via published and well-defined interfaces. Even so, it is possible for client components to learn about and depend on unpublished yet observable behaviors of components. To identify and support these situations, compatibility testing should uncover such observable behaviors. As a solution, we propose a patterns-based approach to test compatibility between programs in terms of their observable behaviors. The approach compares traces of behaviors observed at identical published interfaces of programs and detects incompatibilities stemming from both the presence of previously unobserved behaviors and the absence of previously observed behaviors. The traces are compared by transforming them into sets of structural and binary linear temporal patterns. During Windows 8 development cycle, we applied this approach to test compatibility between USB 2.0 and USB 3.0 bus drivers. Upon testing 14 USB 2.0 devices that were functioning without errors with both USB bus drivers, we uncovered 25 previously unknown incompatibilities between the bus drivers.
48.url:http://doi.acm.org/10.1145/2642937.2642942
48.opinion:exclude

49.title:Personas in the middle: automated support for creating personas as focal points in feature gathering forums
49.abstract:Many software systems utilize forums to allow a broad set of stakeholders to request features. However the resulting mass of ideas and comments can make prioritization and management of feature requests challenging. In this paper we propose a novel approach for partially automating the creation of personas from a set of feature requests in open forums. Our approach utilizes topic clustering, classification, and association rules to identify meaningful groupings of feature requests and then uses them to guide the construction of personas. Once created, these personas are leveraged to coordinate feature requests, track changes, and to provide stakeholder communication mechanisms. We illustrate our approach with examples taken from the health-insurance domain and then evaluate it against feature requests in the SugarCRM project.
49.url:http://doi.acm.org/10.1145/2642937.2642958
49.opinion:exclude

50.title:Constructing adaptive configuration dialogs using crowd data
50.abstract:As modern software systems grow in size and complexity so do their configuration possibilities. Users are easy to be confused and overwhelmed by the amount of choices they need to make in order to fit their systems to their exact needs. We propose a method to construct adaptive configuration elicitation dialogs through utilizing crowd wisdom. A set of configuration preferences in the form of association rules is first mined from a crowd configuration data set. Possible configuration elicitation dialogs are then modeled through a Markov Decision Process (MDP). Association rules are used to inform the model about configuration decisions that can be automatically inferred from knowledge already elicited earlier in the dialog. This way, an MDP solver can search for elicitation strategies which maximize the expected amount of automated decisions, reducing thereby elicitation effort and increasing user confidence of the result. The method is applied to the privacy configuration of Facebook.
50.url:http://doi.acm.org/10.1145/2642937.2642960
50.opinion:exclude

51.title:Validating ajax applications using a delay-based mutation technique
51.abstract:The challenge of validating Asynchronous JavaScript and XML (Ajax) applications lies in actual errors exposed in a user environment. Several studies have proposed effective and efficient testing techniques to identify executable faults. However, the applications might have faults that are not executed during testing, but might cause actual errors in a user environment. Although we have investigated static methods for finding ``potential faults'' that seem to cause actual errors if executed, developers need to confirm whether or not the potential faults are actually executable. Herein, we propose a mutation-based testing method implemented in a tool called JSPreventer. Even if the potential faults are not easily executable in a given environment, our method mutates the applications until they are executable using two delay-based mutation operators to manipulate the timing of the applications handling interactions. Thus, JSPreventer provides executable evidences of the not-easily-executable faults for developers, if it reveals actual errors by testing the mutated applications. We applied our method to real-world applications and found actual errors that developers could debug to improve their reliability. Therefore, JSPreventer can help developers validate reliable real-world Ajax applications.
51.url:http://doi.acm.org/10.1145/2642937.2642996
51.opinion:exclude

52.title:Static, lightweight includes resolution for PHP
52.abstract:Dynamic languages include a number of features that are challenging to model properly in static analysis tools. In PHP, one of these features is the include expression, where an arbitrary expression provides the path of the file to include at runtime. In this paper we present two complementary analyses for statically resolving PHP includes, one that works at the level of individual PHP files, and one targeting PHP programs possibly consisting of multiple scripts. To evaluate the effectiveness of these analyses we have applied the first to a corpus of 20 open-source systems, totaling more than 4.5 million lines of PHP, and the second to a number of programs from a subset of these systems. Our results show that, in many cases, includes can be resolved to a specific file or a small subset of possible files, enabling better IDE features and more advanced program analysis tools for PHP.
52.url:http://doi.acm.org/10.1145/2642937.2643017
52.opinion:exclude

53.title:Information flows as a permission mechanism
53.abstract:This paper proposes Flow Permissions, an extension to the Android permission mechanism. Unlike the existing permission mechanism, our permission mechanism contains semantic information based on information flows. Flow Permissions allow users to examine and grant per-app information flows within an application e.g., a permission for reading the phone number and sending it over the network) as well as cross-app information flows across multiple applications e.g., a permission for reading the phone number and sending it to another application already installed on the user's phone). Our goal with Flow Permissions is to provide visibility into the holistic behavior of the applications installed on a user's phone. In order to support Flow Permissions on Android, we have developed a static analysis engine that detects flows within an Android application. We have also modified Android's existing permission mechanism and installation procedure to support Flow Permissions. We evaluate our prototype with 2,992 popular applications and 1,047 malicious applications and show that our design is practical and effective in deriving Flow Permissions. We validate our cross-app flow generation and installation procedure on a Galaxy Nexus smartphone.
53.url:http://doi.acm.org/10.1145/2642937.2643018
53.opinion:exclude

54.title:Improving the accuracy of oracle verdicts through automated model steering
54.abstract:The oracle - a judge of the correctness of the system under test (SUT) - is a major component of the testing process. Specifying test oracles is challenging for some domains, such as real-time embedded systems, where small changes in timing or sensory input may cause large behavioral differences. Models of such systems, often built for analysis and simulation, are appealing for reuse as oracles. These models, however, typically represent an idealized system, abstracting away certain issues such as non-deterministic timing behavior and sensor noise. Thus, even with the same inputs, the model's behavior may fail to match an acceptable behavior of the SUT, leading to many false positives reported by the oracle. We propose an automated steering framework that can adjust the behavior of the model to better match the behavior of the SUT to reduce the rate of false positives. This model steering is limited by a set of constraints (defining acceptable differences in behavior) and is based on a search process attempting to minimize a dissimilarity metric. This framework allows non-deterministic, but bounded, behavior differences, while preventing future mismatches, by guiding the oracle-within limits-to match the execution of the SUT. Results show that steering significantly increases SUT-oracle conformance with minimal masking of real faults and, thus, has significant potential for reducing false positives and, consequently, development costs.
54.url:http://doi.acm.org/10.1145/2642937.2642989
54.opinion:exclude

55.title:Automated domain-specific C verification with mbeddr
55.abstract:When verifying C code, two major problems must be addressed. One is the specification of the verified systems properties, the other one is the construction of the verification environment. Neither C itself, nor existing C verification tools, offer the means to efficiently specify application domain-level properties and environments for verification. These two shortcomings hamper the usability of C verification, and limit its adoption in practice. In this paper we introduce an approach that addresses both problems and results in user-friendly and practically usable C verification. The novelty of the approach is the combination of domain-specific language engineering and C verification. We apply the approach in the domain of state-based software, using mbeddr and CBMC. We validate the implementation with an example from the Pacemaker Challenge, developing a functionally verified, lightweight, and deployable cardiac pulse generator. The approach itself is domain-independent.
55.url:http://doi.acm.org/10.1145/2642937.2642938
55.opinion:exclude

56.title:Pattern-based auto-completion of UML modeling activities
56.abstract:Auto-completion of textual inputs when using IDEs benefits software development experts and novices. Researchers demonstrated that auto-completion is beneficial for graphical modeling tasks as well. However, supporting software development by auto-completing UML modeling activities remains largely unexplored by research and unsupported by modeling tools. By matching editing operations to activity patterns, partly performed modeling activities can be recognized and automatically completed. This paper proposes an approach that computes auto-completions for partly performed modeling activities while a developer is creating or evolving UML models. Selected auto-completions can be previewed and adjusted before being executed. We claim, that this approach can improve developers' modeling efficiency. Therefore, we assessed our approach based on a catalog of common modeling activities for structural UML models and found that effort for conducting defined modeling activities can be reduced significantly.
56.url:http://doi.acm.org/10.1145/2642937.2642949
56.opinion:exclude

57.title:Abstraction-aware verifying compiler for yet another MDD
57.abstract:This paper rethinks both modularity and compilation in the light of abstraction between design and implementation. We propose a new compilation approach called abstraction-aware verifying compiler, in which abstraction is the target of compilation. Both a design model and its code are inputted as the first-class software modules to the compiler.
57.url:http://doi.acm.org/10.1145/2642937.2642952
57.opinion:exclude

58.title:Docovery: toward generic automatic document recovery
58.abstract:Application crashes and errors that occur while loading a document are one of the most visible defects of consumer software. While documents become corrupted in various ways---from storage media failures to incompatibility across applications to malicious modifications---the underlying reason they fail to load in a certain application is that their contents cause the application logic to exercise an uncommon execution path which the software was not designed to handle, or which was not properly tested. We present Docovery, a novel document recovery technique based on symbolic execution that makes it possible to fix broken documents without any prior knowledge of the file format. Starting from the code path executed when opening a broken document, Docovery explores alternative paths that avoid the error, and makes small changes to the document in order to force the application to follow one of these alternative paths. We implemented our approach in a prototype tool based on the symbolic execution engine KLEE. We present a preliminary case study, which shows that Docovery can successfully recover broken documents processed by several popular applications such as the e-mail client pine, the pagination tool pr and the binary file utilities dwarfdump and readelf.
58.url:http://doi.acm.org/10.1145/2642937.2643004
58.opinion:exclude

59.title:Exact and approximate probabilistic symbolic execution for nondeterministic programs
59.abstract:Probabilistic software analysis seeks to quantify the likelihood of reaching a target event under uncertain environments. Recent approaches compute probabilities of execution paths using symbolic execution, but do not support nondeterminism. Nondeterminism arises naturally when no suitable probabilistic model can capture a program behavior, e.g., for multithreading or distributed systems. In this work, we propose a technique, based on symbolic execution, to synthesize schedulers that resolve nondeterminism to maximize the probability of reaching a target event. To scale to large systems, we also introduce approximate algorithms to search for good schedulers, speeding up established random sampling and reinforcement learning results through the quantification of path probabilities based on symbolic execution. We implemented the techniques in Symbolic PathFinder and evaluated them on nondeterministic Java programs. We show that our algorithms significantly improve upon a state-of-the-art statistical model checking algorithm, originally developed for Markov Decision Processes.
59.url:http://doi.acm.org/10.1145/2642937.2643011
59.opinion:exclude

60.title:Derailer: interactive security analysis for web applications
60.abstract:Derailer is an interactive tool for finding security bugs in web applications. Using symbolic execution, it enumerates the ways in which application data might be exposed. The user is asked to examine these exposures and classify the conditions under which they occur as security-related or not; in so doing, the user effectively constructs a specification of the application's security policy. The tool then highlights exposures missing security checks, which tend to be security bugs. We have tested Derailer's scalability on several large open-source Ruby on Rails applications. We have also applied it to a large number of student projects (designed with different security policies in mind), exposing a variety of security bugs that eluded human reviewers.
60.url:http://doi.acm.org/10.1145/2642937.2643012
60.opinion:exclude

61.title:Tracking down root causes of defects in simulink models
61.abstract:Being confronted with a defect in software leads to the well known task: correcting the software such that the defect does not occur anymore. Here, the location of the defect and the corresponding root cause do not have to be identical. Thus, before any correction can be done, the reviewer has to detect the root cause for the defect. In order to reduce the reviewing effort, this paper presents a method to automatically narrow down possible root causes for defects found in Simulink models. Starting at a defect location, a backward search is applied to detect all paths leading to that defect. Each path is weighted by previously determined weights depending on the block types contained in the respective path. This weighting correlates with the probability of a path containing the root cause.
61.url:http://doi.acm.org/10.1145/2642937.2642943
61.opinion:exclude

62.title:The confidence in our k-tails
62.abstract:k-Tails is a popular algorithm for extracting a candidate behavioral model from a log of execution traces. The usefulness of k-Tails depends on the quality of its input log, which may include too few traces to build a representative model, or too many traces, whose analysis is a waste of resources. Given a set of traces, how can one be confident that it includes enough, but not too many, traces? While many have used the k-Tails algorithm, no previous work has yet investigated this question. In this paper we address this question by proposing a novel notion of log completeness. Roughly, a log of traces, extracted from a given system, is k-complete, iff adding any new trace to the log will not change the resulting model k-Tails would build for it. Since the system and its full set of traces is unknown, we cannot know whether a given log is k-complete. However, we can estimate its k-completeness. We call this estimation k-confidence. We formalize the notion of k-confidence and implement its computation. Preliminary experiments show that k-confidence can be efficiently computed and is a highly reliable estimator for k-completeness.
62.url:http://doi.acm.org/10.1145/2642937.2642944
62.opinion:accept

63.title:Localization of concurrency bugs using shared memory access pairs
63.abstract:We propose an effective approach to automatically localize buggy shared memory accesses that trigger concurrency bugs. Compared to existing approaches, our approach has two advantages. First, as long as enough successful runs of a concurrent program are collected, our approach can localize buggy shared memory accesses even with only one single failed run captured, as opposed to the requirement of capturing multiple failed runs in existing approaches. This is a significant advantage because it is more difficult to capture the elusive failed runs than the successful runs in practice. Second, our approach exhibits more precise bug localization results because it also captures buggy shared memory accesses in those failed runs that terminate prematurely, which are often neglected in existing approaches. Based on this proposed approach, we also implement a prototype, named LOCON. Evaluation results on 16 common concurrency bugs show that all buggy shared memory accesses that trigger these bugs can be precisely localized by LOCON with only one failed run captured.
63.url:http://doi.acm.org/10.1145/2642937.2642972
63.opinion:exclude

64.title:Towards self-healing smartphone software via automated patching
64.abstract:Frequent app bugs and low tolerance for loss of functionality create an impetus for self-healing smartphone software. We take a step towards this via on-the-fly error detection and automated patching. Specifically, we add failure detection and recovery to Android by detecting crashes and ``sealing off'' the crashing part of the app to avoid future crashes. In the detection stage, our system dynamically analyzes app execution to detect certain exceptional situations. In the recovery stage, we use bytecode rewriting to alter app behavior as to avoid such situations in the future. When using our implementation, apps can resume operation (albeit with limited functionality) instead of repeatedly crashing. Our approach does not require access to app source code or any system (e.g., kernel-level) modification. Experiments on several real-world, popular Android apps and bugs show that our approach manages to recover the apps from crashes effectively, timely, and without introducing overhead.
64.url:http://doi.acm.org/10.1145/2642937.2642955
64.opinion:exclude

65.title:Minimal strongly unsatisfiable subsets of reactive system specifications
65.abstract:Verifying realizability in the specification phase is expected to reduce the development costs of safety-critical reactive systems. If a specification is not realizable, we must correct the specification. However, it is not always obvious what part of a specification should be modified. In this paper, we propose a method for obtaining the location of flaws. Rather than realizability, we use strong satisfiability, due to the fact that many practical unrealizable specifications are also strongly unsatisfiable. Using strong satisfiability, the process of analyzing realizability becomes less complex. We define minimal strongly unsatisfiable subsets (MSUSs) to locate flaws, and construct a procedure to compute them. We also show correctness properties of our method, and clarify the time complexity of our method. Furthermore, we implement the procedure, and confirm that MSUSs are computable for specifications of reactive systems at non-trivial scales.
65.url:http://doi.acm.org/10.1145/2642937.2642968
65.opinion:exclude

66.title:Droidmarking: resilient software watermarking for impeding android application repackaging
66.abstract:Software plagiarism in Android markets (app repackaging) is raising serious concerns about the health of the Android ecosystem. Existing app repackaging detection techniques fall short in detection efficiency and in resilience to circumventing attacks; this allows repackaged apps to be widely propagated and causes extensive damages before being detected. To overcome these difficulties and instantly thwart app repackaging threats, we devise a new dynamic software watermarking technique - Droidmarking - for Android apps that combines the efforts of all stakeholders and achieves the following three goals: (1) copyright ownership assertion for developers, (2) real-time app repackaging detection on user devices, and (3) resilience to evading attacks. Distinct from existing watermarking techniques, the watermarks in Droidmarking are non-stealthy, which means that watermark locations are not intentionally concealed, yet still are impervious to evading attacks. This property effectively enables normal users to recover and verify watermark copyright information without requiring a confidential watermark recognizer. Droidmarking is based on a primitive called self-decrypting code (SDC). Our evaluations show that Droidmarking is a feasible and robust technique to effectively impede app repackaging with relatively small performance overhead.
66.url:http://doi.acm.org/10.1145/2642937.2642977
66.opinion:exclude

67.title:From out-place transformation evolution to in-place model patching
67.abstract:Model transformation is a key technique to automate software engineering tasks. Like any other software, transformations are not resilient to change. As changes to transformations can invalidate previously produced models, these changes need to be reflected on existing models. Currently, revised out-place transformations are re-executed entirely to achieve this co-evolution task. However, this induces an unnecessary overhead, particularly when computation-intensive transformations are marginally revised, and if existing models have undergone updates prior the re-execution, these updates get discarded in the newly produced models. To overcome this co-evolution challenge, our idea is to infer from evolved out-place transformations patch transformations that propagate changes to existing models by re-executing solely the affected parts based on an in-place execution strategy. Thereby, existing models are only updated by a patch instead of newly produced. In this paper, we present the conceptual foundation of our approach and report on its evaluation in a real-world case study.
67.url:http://doi.acm.org/10.1145/2642937.2642946
67.opinion:exclude

68.title:Using visual dataflow programming for interactive model comparison
68.abstract:In software engineering the comparison of graph-based models is a well-known problem. Although different comparison metrics have been proposed, there are situations in which automatic or pre-configured approaches do not provide reasonable results. Especially when models contain semantic similarities or differences, additional human knowledge is often required. However, only few approaches tackle the problem of how to support humans when comparing models. In this paper, we propose a tool for interactive model comparison. Its design was informed by a set of guidelines that we identified in previous work. In particular, our prototype leverages visual dataflow programming to allow users to implement custom comparison strategies. To this end, they can combine metrics and graph operations to compute similarities and differences, and use color coding to visualize the gained results. Additionally, a qualitative user study allowed to assess whether and how our tool facilitates iterative exploration of similarities and differences between models.
68.url:http://doi.acm.org/10.1145/2642937.2642984
68.opinion:exclude

69.title:NeedFeed: taming change notifications by modeling code relevance
69.abstract:Most software development tools allow developers to subscribe to notifications about code checked-in by their team members in order to review changes to artifacts that they are responsible for. However, past user studies have indicated that this mechanism is counter-productive, as developers spend a significant amount of effort sifting through such feeds looking for items that are relevant to them. We present NeedFeed, a system that models code relevance by mining a project's software repository and highlights changes that a developer may need to review. We evaluate several techniques to model code relevance, from a naive TOUCH-based approach to generic HISTORY-based classifiers using temporal code metrics at file and method-level granularities, which are then improved by building developer-specific models using TEXT-based features from commit messages. NeedFeed reduces notification clutter by more than 90%, on average, with the best strategy giving an average precision and recall of more than 75%.
69.url:http://doi.acm.org/10.1145/2642937.2642985
69.opinion:exclude

70.title:Active code search: incorporating user feedback to improve code search relevance
70.abstract:Code search techniques return relevant code fragments given a user query. They typically work in a passive mode: given a user query, a static list of code fragments sorted by the relevance scores decided by a code search technique is returned to the user. A user will go through the sorted list of returned code fragments from top to bottom. As the user checks each code fragment one by one, he or she will naturally form an opinion about the true relevance of the code fragment. In an active model, those opinions will be taken as feedbacks to the search engine for refining result lists. In this work, we incorporate users' opinion on the results from a code search engine to refine result lists: as a user forms an opinion about one result, our technique takes this opinion as feedback and leverages it to re-order the results to make truly relevant results appear earlier in the list. The refinement results can also be cached to potentially improve future code search tasks. We have built our active refinement technique on top of a state-of-the-art code search engine---Portfolio. Our technique improves Portfolio in terms of Normalized Discounted Cumulative Gain (NDCG) by more than 11.3%, from 0.738 to 0.821.
70.url:http://doi.acm.org/10.1145/2642937.2642947
70.opinion:exclude

71.title:Resilient user interface level tests
71.abstract:About 60% of the software development cost for online applications is related to developing user interfaces commonly used by the end users to interact with those applications. Frequent small changes to user interfaces (UIs) however easily break about 70% of the test cases intended to mimic users' interactions. Fixing broken tests results in extra costs of maintenance while lessening the benefits of test automation. One of the biggest challenges in creating resilient UI level test cases is to identify and locate the elements of the UI in a way that small UI changes do not break the way in which an element was originally located. In this paper, we present our early efforts in creating a test development framework that makes test cases independent of the internal structure of the UIs, so that a change in the structure does not break any test as long as the functionalities validated in the test are not changed. Our approach is inspired by the way human interact with the UIs of online applications and how those interactions are described and communicated in natural language to others. Visual landmarks such as texts (via a perceptual activity) help end users to locate their points of interest during their interaction with the application in a way that is independent of the internal structure of application.
71.url:http://doi.acm.org/10.1145/2642937.2642954
71.opinion:exclude

72.title:Studying task allocation decisions of novice agile teams with data from agile project management tools
72.abstract:Task allocation decisions are critical for the success of Agile teams yet not well understood. Traditional survey/interview based methods limit the scale and level of details of data collection. As agile project management (APM) tools are increasingly adopted, they offer a mechanism for unobtrusively collecting behavior data to support research. In this paper, we demonstrate the advantage of APM tool based method of studying task allocation decision-making in agile software engineering compared to survey/interview based methods. Through the proposed HASE online APM tool, we conducted a study involving 20 novice Agile teams consisting of 119 undergraduate software engineering students over an 8 week period. Analysis of the collected data provides insight into the effects of novice Agile team members' competence and the difficulty of the tasks assigned on them on their workload variation, their confidence, and the timeliness of completion of these tasks. These findings can be useful in designing situation-aware task allocation decision support systems to serve Agile teams.
72.url:http://doi.acm.org/10.1145/2642937.2642959
72.opinion:exclude

73.title:PiE: programming in eliza
73.abstract:Eliza, a primitive example of natural language processing, adopts a rule-based method to conduct simple conversations with people. In this paper, we extend Eliza for a novel application. We propose a system to assist with program synthesis called Programming in Eliza (PiE). According to a set of rules, PiE can automatically synthesize programs from natural language conversations between Eliza and users. PiE is useful for programming in domain-specific languages. We have implemented PiE to synthesize programs in the LOGO programming language, and our experimental results show that, on average, the success ratio is 88.4% for synthesizing LOGO programs from simple conversations with Eliza. PiE also enables end-users with no experience to program in LOGO with a smoother learning curve.
73.url:http://doi.acm.org/10.1145/2642937.2642967
73.opinion:exclude

74.title:Search-based inference of polynomial metamorphic relations
74.abstract:Metamorphic testing (MT) is an effective methodology for testing those so-called ``non-testable'' programs (e.g., scientific programs), where it is sometimes very difficult for testers to know whether the outputs are correct. In metamorphic testing, metamorphic relations (MRs) (which specify how particular changes to the input of the program under test would change the output) play an essential role. However, testers may typically have to obtain MRs manually. In this paper, we propose a search-based approach to automatic inference of polynomial MRs for a program under test. In particular, we use a set of parameters to represent a particular class of MRs, which we refer to as polynomial MRs, and turn the problem of inferring MRs into a problem of searching for suitable values of the parameters. We then dynamically analyze multiple executions of the program, and use particle swarm optimization to solve the search problem. To improve the quality of inferred MRs, we further use MR filtering to remove some inferred MRs. We also conducted three empirical studies to evaluate our approach using four scientific libraries (including 189 scientific functions). From our empirical results, our approach is able to infer many high-quality MRs in acceptable time (i.e., from 9.87 seconds to 1231.16 seconds), which are effective in detecting faults with no false detection.
74.url:http://doi.acm.org/10.1145/2642937.2642994
74.opinion:exclude

75.title:A dynamic analysis to support object-sharing code refactorings
75.abstract:Creation of large numbers of co-existing long-lived isomorphic objects increases the memory footprint of applications significantly. In this paper we propose a dynamic-analysis based approach that detects allocation sites that create large numbers of long-lived isomorphic objects, estimates quantitatively the memory savings to be obtained by sharing isomorphic objects created at these sites, and also checks whether certain necessary conditions for safely employing object sharing hold. We have implemented our approach as a tool, and have conducted experiments on several real-life Java benchmarks. The results from our experiments indicate that in real benchmarks a significant amount of heap memory, ranging up to 37% in some benchmarks, can be saved by employing object sharing. We have also validated the precision of estimates from our tool by comparing these with actual savings obtained upon introducing object-sharing at selected sites in the real benchmarks.
75.url:http://doi.acm.org/10.1145/2642937.2642992
75.opinion:exclude

76.title:Language fuzzing using constraint logic programming
76.abstract:Fuzz testing builds confidence in compilers and interpreters. It is desirable for fuzzers to allow targeted generation of programs that showcase specific language features and behaviors. However, the predominant program generation technique used by most language fuzzers, stochastic context-free grammars, does not have this property. We propose the use of constraint logic programming (CLP) for program generation. Using CLP, testers can write declarative predicates specifying interesting programs, including syntactic features and semantic behaviors. CLP subsumes and generalizes the stochastic grammar approach.
76.url:http://doi.acm.org/10.1145/2642937.2642963
76.opinion:exclude

77.title:Tracing software build processes to uncover license compliance inconsistencies
77.abstract:Open Source Software (OSS) components form the basis for many software systems. While the use of OSS components accelerates development, client systems must comply with the license terms of the OSS components that they use. Failure to do so exposes client system distributors to possible litigation from copyright holders. Yet despite the importance of license compliance, tool support for license compliance assessment is lacking. In this paper, we propose an approach to construct and analyze the Concrete Build Dependency Graph (CBDG) of a software system by tracing system calls that occur at build-time. Through a case study of seven open source systems, we show that the constructed CBDGs: (1) accurately classify sources as included in or excluded from deliverables with 88%-100% precision and 98%-100% recall, and (2) can uncover license compliance inconsistencies in real software systems -- two of which prompted code fixes in the CUPS and FFmpeg systems.
77.url:http://doi.acm.org/10.1145/2642937.2643013
77.opinion:exclude

78.title:Automatic verification of interactions in asynchronous systems with unbounded buffers
78.abstract:Asynchronous communication requires message queues to store the messages that are yet to be consumed. Verification of interactions in asynchronously communicating systems is challenging since the sizes of these queues can grow arbitrarily large during execution. In fact, behavioral models for asynchronously communicating systems typically have infinite state spaces, which makes many analysis and verification problems undecidable. In this paper, we show that, focusing only on the interaction behavior (modeled as the global sequence of messages that are sent, recorded in the order they are sent) results in decidable verification for a class of asynchronously communicating systems. In particular, we present the necessary and sufficient condition under which asynchronously communicating systems with unbounded queues exhibit interaction behavior that is equivalent to their interactions over finitely bounded queues. We show that this condition can be automatically checked, ensuring existence of a finite bound on the queue sizes, and, we show that, the finite bound on the queue sizes can be automatically computed.
78.url:http://doi.acm.org/10.1145/2642937.2643016
78.opinion:exclude

79.title:Towards an intelligent domain-specific traceability solution
79.abstract:State-of-the-art software trace retrieval techniques are unable to perform the complex reasoning that a human analyst follows in order to create accurate trace links between artifacts such as regulatory codes and requirements. As a result, current algorithms often generate imprecise links. To address this problem, we present the Domain-Contextualized Intelligent Traceability Solution (DoCIT), designed to mimic some of the higher level reasoning that a human trace analyst performs. We focus our efforts on the complex domain of communication and control in a transportation system. DoCIT includes rules for extracting ``action units'' from software artifacts, a domain-specific knowledge base for relating semantically similar concepts across action units, and a set of link-creation heuristics which utilize the action units to establish meaningful trace links across pairs of artifacts. Our approach significantly improves the quality of the generated trace links. We illustrate and evaluate DoCIT with examples and experiments from the control and communication sector of a transportation domain.
79.url:http://doi.acm.org/10.1145/2642937.2642970
79.opinion:exclude

80.title:Automated requirements analysis for a molecular watchdog timer
80.abstract:Dynamic systems in DNA nanotechnology are often programmed using a chemical reaction network (CRN) model as an intermediate level of abstraction. In this paper, we design and analyze a CRN model of a watchdog timer, a device commonly used to monitor the health of a safety critical system. Our process uses incremental design practices with goal-oriented requirements engineering, software verification tools, and custom software to help automate the software engineering process. The watchdog timer is comprised of three components: an absence detector, a threshold filter, and a signal amplifier. These components are separately designed and verified, and only then composed to create the molecular watchdog timer. During the requirements-design iterations, simulation, model checking, and analysis are used to verify the system. Using this methodology several incomplete requirements and design flaws were found, and the final verified model helped determine specific parameters for biological experiments.
80.url:http://doi.acm.org/10.1145/2642937.2643007
80.opinion:exclude

81.title:Formalisation of the integration of behavior trees
81.abstract:In this paper, we present a formal definition of the integration of the requirements modeling language Behavior Trees (BTs). We first provide the semantic integration of two interrelated BTs using an extended version of Communicating Sequential Processes. We then use a Semantic Network Model to capture a set of interrelated BTs, and develop algorithm to integrate them all into one BT. This formalisation facilitates developing (semi-)automated tools for modeling the requirements of large-scale software intensive systems.
81.url:http://doi.acm.org/10.1145/2642937.2642945
81.opinion:exclude

82.title:Automatic early defects detection in use case documents
82.abstract:Use cases, as the primary techniques in the user requirement analysis, have been widely adopted in the requirement engineering practice. As developed early, use cases also serve as the basis for function requirement development, system design and testing. Errors in the use cases could potentially lead to problems in the system design or implementation. It is thus highly desirable to detect errors in use cases. Automatically analyzing use case documents is challenging primarily because they are written in natural languages. In this work, we aim to achieve automatic defect detection in use case documents by leveraging on advanced parsing techniques. In our approach, we first parse the use case document using dependency parsing techniques. The parsing results of each use case are further processed to form an activity diagram. Lastly, we perform defect detection on the activity diagrams. To evaluate our approach, we have conducted experiments on 200+ real-world as well as academic use cases. The results show the effectiveness of our method.
82.url:http://doi.acm.org/10.1145/2642937.2642969
82.opinion:exclude

83.title:SymCrash: selective recording for reproducing crashes
83.abstract:Software often crashes despite tremendous effort on software quality assurance. Once developers receive a crash report, they need to reproduce the crash in order to understand the problem and locate the fault. However, limited information from crash reports often makes crash reproduction difficult. Many "capture-and-replay" techniques have been proposed to automatically capture program execution data from the failing code, and help developers replay the crash scenarios based on the captured data. However, such techniques often suffer from heavy overhead and introduce privacy concerns. Recently, methods such as BugRedux were proposed to generate test input that leads to crash through symbolic execution. However, such methods have inherent limitations because they rely on conventional symbolic execution techniques. In this paper, we propose a dynamic symbolic execution method called SymCon, which addresses the limitation of conventional symbolic execution by selecting functions that are hard to be resolved by a constraint solver and using their concrete runtime values to replace the symbols. We then propose SymCrash, a selective recording approach that only instruments and monitors the hard-to-solve functions. SymCrash can generate test input for crashes through SymCon. We have applied our approach to successfully reproduce 13 failures of 6 real-world programs. Our results confirm that the proposed approach is suitable for reproducing crashes, in terms of effectiveness, overhead, and privacy. It also outperforms the related methods.
83.url:http://doi.acm.org/10.1145/2642937.2642993
83.opinion:exclude

84.title:Potential biases in bug localization: do they matter?
84.abstract:Issue tracking systems are valuable resources during software maintenance activities and contain information about the issues faced during the development of a project as well as after its release. Many projects receive many reports of bugs and it is challenging for developers to manually debug and fix them. To mitigate this problem, past studies have proposed information retrieval (IR)-based bug localization techniques, which takes as input a textual description of a bug stored in an issue tracking system, and returns a list of potentially buggy source code files. These studies often evaluate their effectiveness on issue reports marked as bugs in issue tracking systems, using as ground truth the set of files that are modified in commits that fix each bug. However, there are a number of potential biases that can impact the validity of the results reported in these studies. First, issue reports marked as bugs might not be reports of bugs due to error in the reporting and classification process. Many issue reports are about documentation update, request for improvement, refactoring, code cleanups, etc. Second, bug reports might already explicitly specify the buggy program files and for these reports bug localization techniques are not needed. Third, files that get modified in commits that fix the bugs might not contain the bug. This study investigates the extent these potential biases affect the results of a bug localization technique and whether bug localization researchers need to consider these potential biases when evaluating their solutions. In this paper, we analyse issue reports from three different projects: HTTPClient, Jackrabbit, and Lucene-Java to examine the impact of above three biases on bug localization. Our results show that one of these biases significantly and substantially impacts bug localization results, while the other two biases have negligible or minor impact.
84.url:http://doi.acm.org/10.1145/2642937.2642997
84.opinion:exclude

85.title:MIMIC: locating and understanding bugs by analyzing mimicked executions
85.abstract:Automated debugging techniques aim to help developers locate and understand the cause of a failure, an extremely challenging yet fundamental task. Most state-of-the-art approaches suffer from two problems: they require a large number of passing and failing tests and report possible faulty code with no explanation. To mitigate these issues, we present MIMIC, a novel automated debugging technique that combines and extends our previous input generation and anomaly detection techniques. MIMIC (1) synthesizes multiple passing and failing executions similar to an observed failure and (2) uses these executions to detect anomalies in behavior that may explain the failure. We evaluated MIMIC on six failures of real-world programs with promising results: for five of these failures, MIMIC identified their root causes while producing a limited number of false positives. Most importantly, the anomalies identified by MIMIC provided information that may help developers understand (and ultimately eliminate) such root causes.
85.url:http://doi.acm.org/10.1145/2642937.2643014
85.opinion:exclude

86.title:Workspace updates of visual models
86.abstract:In MDE, large models must be collaboratively developed in teams. Collaboration is usually supported by optimistic versioning based on workspaces and a repository. Workspace copies of a model are synchronize with the repository version by an update function. Most update functions currently available compromise the consistency of a model. We present a new approach which guarantees updated models to be consistent and processable by standard visual editors without post-processing. Our approach assumes changes in the repository to be represented as a consistency-preserving edit script. This edit script is executed in a controlled, interactive manner on the workspace copy. Unlike 3-way merging, our approach needs no conflict analysis. Conflicts are detected and resolved interactively. Our approach offers improved usability and requires less implementation effort than existing approaches.
86.url:http://doi.acm.org/10.1145/2642937.2648623
86.opinion:exclude

87.title:Meta-Model validation and verification with MetaBest
87.abstract:Meta-models play a cornerstone role in Model-Driven Engineering as they are used to define the abstract syntax of Domain-Specific Modelling Languages, and so models and all sorts of model transformations depend on them. However, there are scarce tools and methods supporting their validation and verification, which are essential activities for the proper engineering of meta-models. In this paper we present an Eclipse-based tool that aims to fill this gap by providing two complementary meta-model testing languages. The first one has similar philosophy to the xUnit framework, enabling the definition of meta-model unit test suites comprising model fragments and assertions on their (in-)correctness. The second one is directed to verify expected properties of the meta-model, including domain and design properties, quality criteria and platform-specific requirements. Both tools are integrated within a framework for example-based, incremental meta-model development.
87.url:http://doi.acm.org/10.1145/2642937.2648617
87.opinion:exclude

88.title:A web based UML modeling tool with touch screens
88.abstract:With the popularity of pads, large touch screens, notebooks and smart mobiles, it is a reasonable requirement that modelers use such devices to models. However, there are few modeling tools that take full advantage of the devices. This paper discusses a Web based UML modeling tool with touch screens, based on our full analysis of human-machine interaction modes for software modeling. The tool provides more input means, i.e. combining gesture input and traditional keyboard and mouse input, and supports cross-platforms modeling by using HTML 5.
88.url:http://doi.acm.org/10.1145/2642937.2648619
88.opinion:exclude

89.title:SeSAME: modeling and analyzing high-quality service compositions
89.abstract:Today, software components are traded on markets in form of services. These services can be service compositions consisting of several services. If a software architect wants to provide such a service composition in the market for trade, she needs to perform several tasks: she needs to model the composition, to discover existing services to be part of that composition, and to analyze the composition's functional correctness as well as its quality, e.g., performance. Up to now, the architect needed to find and use different tools for these tasks. Typically, these tools are not interoperable with each other. We provide the tool SeSAME that supports a software architect in all of these tasks. SeSAME is an integrated Eclipse-based tool-suite providing a comprehensive service specification language to model service compositions and existing services. Furthermore, it includes modules for service matching, functional analysis, and non-functional analysis. SeSAME is the first tool that integrates all these tasks into one tool-suite and, thereby, provides holistic support for trading software services. Thus, it contributes to the acceptance and success of a service market.
89.url:http://doi.acm.org/10.1145/2642937.2648621
89.opinion:exclude

90.title:MPAnalyzer: a tool for finding unintended inconsistencies in program source code
90.abstract:Unintended inconsistencies are caused by missing a modification task that requires code changes on multiple locations in program source code. In order to identify such inconsistencies efficiently, we proposed a new technique. It firstly learns how code fragments were changed in the past modification tasks, and then, it identifies where inconsistencies exist at the latest version. In this paper, we focus on an aspect of the tool that we developed and shows a case study that we conducted with the tool. A video of the tool is available at http://youtu.be/a7_PVVZ4-vo.
90.url:http://doi.acm.org/10.1145/2642937.2648616
90.opinion:exclude

91.title:Firecrow: a tool for web application analysis and reus
91.abstract:This paper presents Firecrow - a tool for Web application analysis and reuse. The tool's primary function is to support reuse of client-side features, but it can also be used for feature identification, web application slicing, and generation of usage scenarios, i.e. sequences of user actions that cause the manifestation of application behaviors. The tool is in prototype stage and is accessible through a plug-in to the Firefox browser, but it can also be used as a library from other browsers (e.g. Chrome, Safari, and PhantomJs).
91.url:http://doi.acm.org/10.1145/2642937.2648620
91.opinion:exclude

92.title:HUSACCT: architecture compliance checking with rich sets of module and rule types
92.abstract:Architecture Compliance Checking (ACC) is an approach to verify the conformance of implemented program code to high-level models of architectural design. Static ACC focuses on the module views of architecture and especially on rules constraining the modular elements. This paper presents HUSACCT, a static ACC tool that adds extensive support for semantically rich modular architectures (SRMAs) to the current practice of static ACC tools. An SRMA contains modules of semantically different types, like layers and components, which are constrained by rules of different types. HUSACCT provides support for five commonly used types of modules and eleven types of rules. We describe and illustrate how basic and extensive support of these types is provided and how the support can be configured. In addition, we discuss the internal architecture of the tool.
92.url:http://doi.acm.org/10.1145/2642937.2648624
92.opinion:exclude

93.title:CoCoTest: a tool for model-in-the-loop testing of continuous controllers
93.abstract:We present CoCoTest, a tool for automated testing of continuous controllers at the Model-in-the-Loop stage. CoCoTest combines explorative and exploitative search algorithms to identify scenarios in the controller input space that violate or are likely to violate the controller requirements. This enables a scalable and systematic way to test continuous properties of such controllers. Our experiments show that CoCoTest identifies critical flaws in the controller design that are rarely found by manual testing and go unnoticed until late stages of embedded software system development.
93.url:http://doi.acm.org/10.1145/2642937.2648625
93.opinion:exclude

94.title:8Cage: lightweight fault-based test generation for simulink
94.abstract:Matlab Simulink models, mainly used for the specification of continuous embedded systems, employ a data flow-driven notation well understood by engineers. This notation abstracts from the underlying computational model, hiding run time failures such as over-/underflows and divisions by zero. They are often detected late in the development process by the use of static analysis tools on the completely developed system. The responsible underlying faults are sometimes attributable to a single operation in a model. 8Cage is an automated test case generator for the early detection of such single operation related faults. It is configurable to detect these faults and runs automatically in the background. It tries to find potentially failure-causing operations and generates a test case to gather evidence for an actual fault. 8Cage is usable by developing/testing engineers with knowledge of Matlab. It does not require an expert to perform result validation or fault localization.
94.url:http://doi.acm.org/10.1145/2642937.2648622
94.opinion:exclude

95.title:PBGT tool: an integrated modeling and testing environment for pattern-based GUI testing
95.abstract:Pattern Based GUI Testing (PBGT) is a new methodology that aims at systematizing and automating the GUI testing process. It is supported by a Tool (PBGT Tool) which provides an integrated modeling and testing environment that supports the crafting of test models based on UI Test Patterns, using a GUI modeling DSL called PARADIGM. The tool is freely available as an Eclipse plugin, developed on top of the Eclipse Modeling Framework. This paper presents PBGT Tool, which has been successfully used in several projects, and more recently at industry level.
95.url:http://doi.acm.org/10.1145/2642937.2648618
95.opinion:exclude

96.title:A tool chain for generating the description files of highly available software
96.abstract:Service availability is a key non-functional requirement that system architects and integrators seek to achieve. High Availability (HA) can be attained using a dedicated distributed HA middleware that can manage clustered redundant resources to maintain the continuous service delivery even in the presence of failures. Nonetheless, employing such middleware based solutions requires deep knowledge of the domain and substantial configuration effort, which can be a tedious and error prone task. In this paper, we demonstrate an automated approach that mitigates the efforts of configuring HA systems, and allows non-domain experts to easily use specialized HA solutions to increase the reliability of the services provided by their systems.
96.url:http://doi.acm.org/10.1145/2642937.2648626
96.opinion:exclude

97.title:DupFinder: integrated tool support for duplicate bug report detection
97.abstract:To track bugs that appear in a software, developers often make use of a bug tracking system. Users can report bugs that they encounter in such a system. Bug reporting is inherently an uncoordinated distributed process though and thus when a user submits a new bug report, there might be cases when another bug report describing exactly the same problem is already present in the system. Such bug reports are duplicate of each other and these duplicate bug reports need to be identified. A number of past studies have proposed a number of automated approaches to detect duplicate bug reports. However, these approaches are not integrated to existing bug tracking systems. In this paper, we propose a tool named DupFinder, which implements the state-of-the-art unsupervised duplicate bug report approach by Runeson et al., as a Bugzilla extension. DupFinder does not require any training data and thus can easily be deployed to any project. DupFinder extracts texts from summary and description fields of a new bug report and recent bug reports present in a bug tracking system, uses vector space model to measure similarity of bug reports, and provides developers with a list of potential duplicate bug reports based on the similarity of these reports with the new bug report. We have released DupFinder as an open source tool in GitHub, which is available at: https://github.com/smagsmu/dupfinder.
97.url:http://doi.acm.org/10.1145/2642937.2648627
97.opinion:exclude

98.title:Determining the most probable root causes of run-time errors in simulink models
98.abstract:Being confronted with a defect in software leads to the well known task: correcting the software such that the defect does not occur anymore. Here, the location of the defect and the corresponding root cause do not have to be identical. Thus, before any correction can be done, the reviewer has to detect the root cause for the defect. In order to reduce the reviewing effort, this approach presents a method to automatically narrow down possible root causes for defects found in Simulink models. Starting at a defect location, a backward search is applied to detect all paths leading to that defect. Each path is weighted by previously determined weights depending on the block types contained in the respective path. This weighting correlates with the probability of a path containing the root cause.
98.url:http://doi.acm.org/10.1145/2642937.2653465
98.opinion:exclude

99.title:Utilizing feature location techniques for feature addition and feature enhancement
99.abstract:Additions and enhancements to software are prevalent issues within software development. When developers want to make enhancements to a piece of software they first have to build up an understanding of the relevant parts of the system. There are a number of challenges within this: where is the best place to add changes, what to reuse and identifying possible ripple effects. This research aims to investigate the utility of feature location techniques to address these challenges. Feature location can help in automating what traditionally is seen as a manual activity, i.e., searching within code. Feature location techniques may lead to improved software evolution by reducing the amount of noise the developer needs to look through to find code of interest for the three challenges above. However these challenges do not appear to be directly addressed within the feature location literature. This research examines how we can leverage the potential of feature location to create feature addition and enhancement techniques to address the above challenges uncovering techniques that help with positioning, reusing and evolving code.
99.url:http://doi.acm.org/10.1145/2642937.2653466
99.opinion:exclude

100.title:AugIR: the conceptual design and evaluation of an augmented interaction room
100.abstract:It is often difficult for stakeholders with heterogeneous backgrounds to maintain a common understanding of the most important risk and value drivers of a complex software project. Therefore, I propose the concept of a so-called Augmented Interaction Room, i.e. a persistent physical team room whose walls are outfitted with large interactive displays. It provides a dedicated project space for supporting collaborative system design and the communication among members of a software development project. In this paper, I describe the specific challenges regarding visualization and navigation on these large displays and present my approach on how to support stakeholders by automatically identifying inconsistencies, contradictions and potential project risks.
100.url:http://doi.acm.org/10.1145/2642937.2653467
100.opinion:exclude

101.title:A requirements monitoring infrastructure for systems of systems
101.abstract:An increasing number of software systems today are systems of systems (SoS) that have been developed by diverse teams over many years. Such systems emerge gradually and it is hard to analyze or predict their behavior due to their scale, complexity, and heterogeneity. In particular, certain behavior only emerges at runtime due to complex interactions between the involved systems and their environment. Requirements monitoring has been proposed as a solution for checking at runtime whether systems adhere to their requirements. However, existing requirements monitoring approaches have been designed for single systems and therefore do not adequately consider the characteristics of SoS. More specifically, requirements in SoS exist at different levels, across different systems, and are owned by diverse stakeholders. Furthermore, requirements monitoring for SoS has to be flexible with respect to technologies and architectural patterns. This thesis will identify the capabilities required for requirements monitoring of SoS. It will further provide a flexible and tailorable infrastructure to support engineers and maintenance staff in observing and analyzing the behavior of a SoS at runtime. We plan to evaluate our work by assessing its usefulness in the context of an industrial SoS.
101.url:http://doi.acm.org/10.1145/2642937.2653468
101.opinion:exclude

102.title:Embrace your issues: compassing the software engineering landscape using bug reports
102.abstract:Software developers in large projects work in complex information landscapes, and staying on top of all relevant software artifacts is challenging. As software systems often evolve for years, a high number of issue reports is typically managed during the lifetime of a system. Efficient management of incoming issue requires successful navigation of the information landscape. In our work, we address two important work tasks involved in issue management: Issue Assignment (IA) and Change Impact Analysis (CIA). IA is the early task of allocating an issue report to a development team. CIA deals with identifying how source code changes affect the software system, a fundamental activity in safety-critical development. Our solution approach is to support navigation, both among development teams and software artifacts, based on information available in historical issue reports. We present how we apply techniques from machine learning and information retrieval to develop recommendation systems. Finally, we report intermediate results from two controlled experiments and an industrial case study.
102.url:http://doi.acm.org/10.1145/2642937.2653469
102.opinion:exclude

103.title:Issues of automated software composition in AI planning
103.abstract:Automated programming aims at automatically assembling a new software artifact from existing software modules. Although automated programming was revitalized through automated software composition in the last decade, the problem cannot be considered solved. Automated software composition is widely accepted as being a planning task, but the problem is that it has very special properties that other planning problems do not have and that are commonly overseen. These properties usually imply that the composition problem cannot be solved with standard planning tools. This paper gives a brief and intuitive description of the planning problem that most approaches are based on. It points out special properties of this problem and explains why it is not adequate to solve the problem with classical planning tools as done by most existing approaches.
103.url:http://doi.acm.org/10.1145/2642937.2653470
103.opinion:exclude

104.title:A proposal for revisiting coverage testing metrics
104.abstract:Test coverage information can be very useful for guiding testers in enhancing their test suites to exercise possible uncovered entities and in deciding when to stop testing. Since the concept of test criterion was born, several contributions have been made by both academia and industry in the definition and adaptation of adequacy criteria aiming at ensuring the discovery of more failures. Numerous contributions have also been done in the development of coverage tools. However, for complex applications that are reused in different contexts and for emerging paradigms (e.g., component-based development, service-oriented architecture, and cloud computing), traditional coverage metrics may no longer provide meaningful information to help testers on these tasks. Inspired by the idea of relative coverage this research focuses on the introduction of meaningful coverage metrics to cope with the challenges imposed by the current programming paradigms as well as on the definition of a theoretical framework for the development of relative coverage metrics.
104.url:http://doi.acm.org/10.1145/2642937.2653471
104.opinion:exclude

105.title:Variability-aware change impact analysis of multi-language product lines
105.abstract:Change impact analysis (CIA) techniques have been applied successfully to determine the effects of modifications when evolving software systems. However, many software systems today use multiple programming languages and they are organized as software product lines (SPLs) to ease their customization to different customer and market needs. Due to the limitations of current CIA approaches regarding variability and cross-language support assessing the impact of changes becomes difficult and arduous. This work aims at developing a CIA approach for multi language SPLs. The approach is based on a multiple conditional system dependency graphs. We will evaluate the approach based on the software product lines of an industry partner in the domain of industrial automation.
105.url:http://doi.acm.org/10.1145/2642937.2653472
105.opinion:exclude

106.title:Improvement of applications' stability through robust APIs
106.abstract:Modern programs require useful and robust APIs to guarantee applications' responsiveness. Given that large APIs can be used by novice developers and that not all experts are infallible, a well-designed API should inform developers about all the possible exceptions that an application can throw when it calls specific API methods. This research aims to identify automatically which exceptions should be included in an API reference. For this, there will be an evaluation of the Android API with an emphasis on its Java error-handling mechanism. First goal will be the automatic identification of as many as possible critical exceptions that each API method of the system can throw.Second goal will be the recommendation of undocumented exceptions that can lead client applications to execution failures (crashes). Consequently, the contribution of this research would be the decrease of possible application crashes that are associated with undocumented exceptions.
106.url:http://doi.acm.org/10.1145/2642937.2653473
106.opinion:exclude

107.title:Model-driven development of content management systems based on Joomla
107.abstract:Today's web consists of dynamic web sites with a wide range of functionality, whereby most of the sites are based on Content Management Systems (CMS). CMSs became a de facto standard as basic frameworks for web applications, because they provide lots of existing features and a high usability. Even though typical CMSs come along with lots of core functions, the most of them are extensible through different extension types. Through these extensions CMS-based applications become fully adequate web applications. One of the most popular CMSs is Joomla, which is extensible through external functionality. During the development and maintenance of both external functionality and concrete Joomla-based sites, some obstacles occur. One of the most substantial obstacle occurs when a new version of the core CMS platform is released. A further migration process is always an extremely time-consuming process. Through the use of model-driven development (MDD) a lot of the invested development and maintenance effort can be reduced, especially when new platform versions appear frequently. This paper proposes an MDD approach to the Joomla CMS, and contains several use cases for both forward engineering and reverse engineering of Joomla extensions and Joomla-based web sites with concrete data which shall be used for a faster and simpler migration process.
107.url:http://doi.acm.org/10.1145/2642937.2653474
107.opinion:exclude

