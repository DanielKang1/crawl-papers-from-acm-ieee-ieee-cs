1.title:"Assumption generation for software component verification"
1.abstract:"Model checking is an automated technique that can be used to determine whether a system satisfies certain required properties. The typical approach to verifying properties of software components is to check them for all possible environments. In reality, however, a component is only required to satisfy properties in specific environments. Unless these environments are formally characterized and used during verification (assume-guarantee paradigm), the results returned by verification can be overly pessimistic. This work defines a framework that brings a new dimension to model checking of software components. When checking a component against a property, our model checking algorithms return one of the following three results: the component satisfies a property for any environment; the component violates the property for any environment; or finally, our algorithms generate an assumption that characterizes exactly those environments in which the component satisfies its required property. Our approach has been implemented in the LTSA tool and has been applied to the analysis of a NASA application."
1.url:http://dx.doi.org/10.1109/ASE.2002.1114984
1.opinion:exclude

2.title:"An approach to rapid prototyping of large multi-agent systems"
2.abstract:"Engineering individual components of a multi-agent system and their interactions is a complex and error-prone task in urgent need of methods and tools. Prototyping is a valuable technique to help software engineers explore the design space while gaining insight and a \"feel\" for the dynamics of the system; prototyping also allows engineers to learn more about the relationships among design features and the desired computational behaviour. In this paper we describe an approach to building prototypes of large multi-agent systems with which we can experiment and analyse results. We have implemented an environment embodying our approach. This environment is supported by a distributed platform that helps us achieve controlled simulations."
2.url:http://dx.doi.org/10.1109/ASE.2002.1114987
2.opinion:exclude

3.title:"Generative design patterns"
3.abstract:"A design pattern encapsulates the knowledge of object-oriented designers into re-usable artifacts. A design pattern is a descriptive device that fosters software design re-use. There are several reasons why design patterns are not used as generative constructs that support code re-use. The first reason is that design patterns describe a set of solutions to a family of related design problems and it is difficult to generate a single body of code that adequately solves each problem in the family. A second reason is that it is difficult to construct and edit generative design patterns. A third major impediment is the lack of a tool-independent representation. A common representation could lead to a shared repository to make more patterns available. We describe a new approach to generative design patterns that solves these three difficult problems. We illustrate this approach using tools called CO/sub 2/P/sub 2/S and Meta-CO/sub 2/P/sub 2/S but our approach is tool-independent."
3.url:http://dx.doi.org/10.1109/ASE.2002.1114991
3.opinion:exclude

4.title:"Deviation analysis through model checking"
4.abstract:"Inaccuracies, or deviations, in the measurements of monitored variables in a control system are facts of life that control software must accommodate $the software is expected to continue functioning correctly in the face of an expected range of deviations in the inputs. Deviation analysis can be used to determine how a software specification will behave in the face of such deviations in data from the environment. The idea is to describe the correct values of an environmental quantity; along with a range of potential deviations, and then determine the effects on the outputs of the system. The analyst can then check whether the behavior of the software is acceptable with respect to these deviations. In this report we wish to propose a new approach to deviation analysis using model checking techniques. This approach allows for more precise analysis than previous techniques, and refocuses deviation analysis from an exploratory analysis to a verification task, allowing us to investigate a different range of questions regarding a system's response to deviations."
4.url:http://dx.doi.org/10.1109/ASE.2002.1114992
4.opinion:exclude

5.title:"Automatic validation of deployed J2EE components using aspects"
5.abstract:"Validating that software components meet their requirements under a particular deployment scenario is very challenging. We describe a new approach that uses component aspects, describing functional and nonfunctional cross-cutting concerns impacting components, to perform automated deployed component validation. Aspect information associated with J2EE component implementations is inspected after component deployment by validation agents. These agents run automated tests to determine if the deployed components meet their aspect-described requirements. We describe the way component aspects are encoded, the automated agent-based testing process we employ, and our validation agent architecture and implementation."
5.url:http://dx.doi.org/10.1109/ASE.2002.1114993
5.opinion:exclude

6.title:"On CASE tool usage at Nokia"
6.abstract:"We present the results of a research work targeted to understanding CASE tools usage in Nokia. By means of a survey questionnaire, we collected data aimed to identify what features are most useful and best implemented in current CASE tools according to senior developers and managers. With the aid of both descriptive and inferential statistical data analysis methods, we found out that the features that are rated most useful belong to the graphical editing, version management and document generation categories. The statistical methods we use allow us to extend the results to the whole population with a certain degree of confidence. The analysis of the data seems to give the indication that there is a general level of dissatisfaction on the quality of currently available CASE tools. Also, there is evidence that some of the most advanced features (reverse engineering, code generation) are not deemed as useful as others. Further research should focus on extending the survey to other types of industries, and attempt generalization of the results. This may constitute precious feedback for the software tools industry in order to develop products that correspond more to industry needs."
6.url:http://dx.doi.org/10.1109/ASE.2002.1114995
6.opinion:exclude

7.title:"Experience report on automated procedure construction for deductive synthesis"
7.abstract:"Deductive program synthesis systems based on automated theorem proving offer the promise of \"correct by construction\" software. However, the difficulty encountered in constructing usable deductive synthesis systems has prevented their widespread use. Amphion is a real-world, domain-independent program synthesis system. It is specialized to specific applications through the creation of an operational domain theory and a specialized deductive engine. This paper describes an experiment aimed at making the construction of usable Amphion applications easier. The software system Theory Operationalization for Program Synthesis (TOPS) has a library of decision procedures with a theory template for each procedure. TOPS identifies axioms in the domain theory that are an instance of a library of procedure and uses partial deduction to augment the procedure with the capability to construct ground terms for deductive synthesis. Synthesized procedures are interfaced to a resolution theorem prover. Axioms in the original domain theory that are implied by the synthesized procedures are removed. During deductive synthesis, each procedure is invoked to test conjunctions of literals in the language of the theory of that procedure. When possible, the procedure generates ground terms and binds them to variables in a problem specification. These terms are program fragments. Experiments show that the procedures synthesized by TOPS can reduce theorem proving search at least as much as hand tuning of the deductive synthesis system."
7.url:http://dx.doi.org/10.1109/ASE.2002.1114996
7.opinion:exclude

8.title:"Generating product-lines of product-families"
8.abstract:"GenVoca is a methodology and technology for generating product-lines, i.e. building variants of a program. The primitive components from which applications are constructed are refinements or layers, which are modules that implement a feature that many programs of a product-line can share. Unlike conventional components (e.g., COM, CORBA, EJB), a layer encapsulates fragments of multiple classes. Sets of fully formed classes can be produced by composing layers. Layers are modular, albeit unconventional, building blocks of programs. But what are the building blocks of layers? We argue that facets is an answer. A facet encapsulates fragments of multiple layers, and compositions of facets yields sets of fully formed layers. Facets arise when refinements scale from producing variants of individual programs to producing variants of multiple integrated programs, as typified byproduct families (e.g., MS Office). We present a mathematical model that explains relationships between layers and facets. We use the model to develop a generator for tools (i.e., product family) that are used in language-extensible Integrated Development Environments (IDEs)."
8.url:http://dx.doi.org/10.1109/ASE.2002.1114997
8.opinion:exclude

9.title:"Knowledge-based synthesis of numerical programs for simulation of rigid-body systems in physics-based animation"
9.abstract:"Physics-based animation programs are important in a variety of contexts, including education, science and entertainment among others. Manual construction of such programs is expensive, time consuming and prone to error. We have developed a system for automatically synthesizing physics-based animation programs for a significant class of problems: constrained systems of rigid bodies, subject to driving and dissipative forces. Our system includes a graphical interface for specifying a physical scenario, including objects, geometry, dynamical variables and coordinate systems, along with a symbolic interface for specifying forces and constraints operating in the scenario. The entities defined in the graphical interface serve as the underlying vocabulary for specifications constructed in the symbolic interface. We use an algorithmically controlled rewrite system to construct a numerical simulation program that drives a real-time animation of the specified scenario. The algorithm operates by partitioning the constraints and dynamic variables into classes, assigning each class to be implemented in a different component of a general simulation program scheme. Our approach provides many of the benefits of formal deductive methods of program synthesis, while keeping the computational costs of program synthesis more in line with conventional program generator technology. We have successfully tested our system on numerous examples."
9.url:http://dx.doi.org/10.1109/ASE.2002.1114998
9.opinion:exclude

10.title:"CpprofJ: aspect-capable call path profiling of multi-threaded Java applications"
10.abstract:"A primary goal of program performance understanding tools is to focus the user's attention directly on optimization opportunities where significant cost savings may be found. Optimization opportunities fall into (at least) three broad categories: the call context of a general component may obviate the need for some of its generality; cross-cutting program aspects may be implemented suboptimally for the particular context of use; and thread dependencies may cause unintended delays. This paper enhances prior work in call path profiling in several ways. First, it provides two different call path oriented views on program performance, a server view and a thread view. The former helps one optimize for throughput, while the latter is useful for optimizing thread latency. The views incorporate a typed time notation for representing different program activities, such as monitor wait and thread preemption times. Second, the new framework allows aspect-oriented program profiling, even when the original program was not designed in an aspect oriented fashion. Finally, the approach is implemented in a tool, CPPROFJ, an aspect-capable call path profiler for Java. It exploits recent developments in the Java APIs to achieve accurate and portable sampling-based profiling. Three case studies illustrate its use."
10.url:http://dx.doi.org/10.1109/ASE.2002.1114999
10.opinion:exclude

11.title:"No Java without caffeine: A tool for dynamic analysis of Java programs"
11.abstract:"To understand the behavior of a program, a maintainer reads some code, asks a question about this code, conjectures an answer, and searches the code and the documentation for confirmation of her conjecture. However, the confirmation of the conjecture can be error-prone and time-consuming because the maintainer has only static information at her disposal. She would benefit from dynamic information. In this paper, we present Caffeine, an assistant that helps the maintainer in checking her conjecture about the behavior of a Java program. Our assistant is a dynamic analysis tool that uses the Java platform debug architecture to generate a trace, i.e., an execution history, and a Prolog engine to perform queries over the trace. We present a usage scenario based on the n-queens problem, and two real-life examples based on the Singleton design pattern and on the composition relationship."
11.url:http://dx.doi.org/10.1109/ASE.2002.1115000
11.opinion:exclude

12.title:"Constructing CORBA-supported oracles for testing: a case study in automated software testing"
12.abstract:"As the complexity of applications and therefore of their testing process grows, the importance of automating the testing activity increases. The testing process includes test case generation, test sequencing, oracle construction, test execution and result interpretation. Automatic generation of test cases from formal specifications has received considerable attention. Relatively little work has been reported, however, on constructing oracles for supporting efficient and automatic execution of such test cases. We present a technique for constructing a CORBA-supported VDM oracle for black-box testing starting from a VDM-SL specification. This specification is used to automatically verify the results of operations implemented in a high-level programming language. We present a case study of the technique applied to a Java application for generic access control. The technique is applicable to any CORBA-compliant programming language."
12.url:http://dx.doi.org/10.1109/ASE.2002.1115003
12.opinion:exclude

13.title:"Generating expected results for automated black-box testing"
13.abstract:"In this paper we describe a technique for generating expected results for automated black-box testing. Generating expected results allows larger automated test suites to be created, moving us toward continuous product testing. Our technique uses a program's Input-Output (IO) relationships to identify unique combinations of program inputs that influence program outputs. With this information, a small set of test cases is executed and checked for correctness. Given the correctness of this set, the expected results for the larger combinatorial test set can be generated automatically. Included in the paper is an experimental study in which checking the results of 384 test cases allows us to generate expected results and fully automate nearly 600,000 test cases."
13.url:http://dx.doi.org/10.1109/ASE.2002.1115005
13.opinion:exclude

14.title:"Generating test data for functions with pointer inputs"
14.abstract:"Generating test inputs for a path in a function with integer and real parameters is an important but difficult problem. The problem becomes more difficult when pointers are passed as inputs to a function. In this case, the shape of the input data structure as well as the data values in the fields of this data structure need to be determined for traversal of the given path. The existing techniques to address this problem are inefficient since they use backtracking to simultaneously satisfy the constraints on the pointer variables and the data values used along the path. In this paper, we develop a novel approach that allows the generation of the shape of an input data structure to be done independently of the generation of its data values so as to force the control flow of a function along a given path. We also present a new technique that generates the shape of the input data structure by solving a set of pointer constraints derived in a single pass of the statements along the path. Although simple, our approach is powerful in handling pointer aliasing. It is efficient and provides a practical solution to generating test data for functions with pointer inputs."
14.url:http://dx.doi.org/10.1109/ASE.2002.1115007
14.opinion:exclude

15.title:"Automating requirements traceability: Beyond the record &amp; replay paradigm"
15.abstract:"Requirements traceability (RT) aims at defining relationships between stakeholder requirements and artifacts produced during the software development life-cycle. Although techniques for generating and validating RT are available, RT in practice often suffers from the enormous effort and complexity of creating and maintaining traces or from incomplete trace information that cannot assist engineers in real-world problems. In this paper we will present a tool-supported technique easing trace acquisition by generating trace information automatically. We will explain the approach using a video-on-demand system and show that the generated traces can be used in various engineering scenarios to solve RT-related problems."
15.url:http://dx.doi.org/10.1109/ASE.2002.1115010
15.opinion:exclude

16.title:"Enabling iterative software architecture derivation using early non-functional property evaluation"
16.abstract:"The structure of a software architecture strongly influences the architecture's ability to prescribe systems satisfying functional requirements, non functional requirements, and overall qualities such as maintainability, reusability, and performance. Achieving an acceptable architecture requires an iterative derivation and evaluation process that allows refinement based on a series of tradeoffs. Researchers at the University of Texas at Austin are developing a suite of processes and supporting tools to guide architecture derivation from requirements acquisition through system design. The various types of decisions needed for concurrent derivation and evaluation demand a synthesis of evaluation techniques, because no single technique is suitable for all concerns of interest. Two tools in this suite, RARE and ARCADE, cooperate to enable iterative architecture derivation and architecture property evaluation. RARE guides derivation by employing a heuristics knowledge base, and evaluates the resulting architecture by applying static property evaluation based on structural metrics. ARCADE provides dynamic property evaluation leveraging simulation and model-checking. This paper presents a study whereby RARE and ARCADE were employed in the early stages of an industrial project to derive a Domain Reference Architecture (DRA), a high-level architecture capturing domain functionality, data, and timing. The discussion emphasizes early evaluation of performance qualities, and illustrates how ARCADE and RARE cooperate to enable iterative derivation and evaluation. These evaluations influenced DRA refinement as well as subsequent design decisions involving application implementation and computing platform selection."
16.url:http://dx.doi.org/10.1109/ASE.2002.1115011
16.opinion:exclude

17.title:"Model-based tests of truisms"
17.abstract:"Software engineering (SE) truisms capture broadly-applicable principles of software construction. The trouble with truisms is that such general principles may not apply in specific cases. This paper tests the specificity of two SE truisms: (a) increasing software process level is a desirable goal; and (b) it is best to remove errors during the early parts of a software lifecycle. Our tests are based on two well-established SE models: (1) Boehm et.al.'s COCOMO II cost estimation model; and (2) Raffo's discrete event software process model of a software project life cycle. After extensive simulations of these models, the TAR2 treatment learner was applied to find the model parameters that most improved the potential performance of the real-world systems being modelled. The case studies presented here showed that these truisms are clearly sub-optimal for certain projects since other factors proved to be far more critical. Hence, we advise against truism-based process improvement. This paper offers a general alternative framework for model-based assessment of methods to improve software quality: modelling + validation + simulation + sensitivity. That is, after recording what is known in a model, that model should be validated, explored using simulations, then summarized to find the key factors that most improve model behavior."
17.url:http://dx.doi.org/10.1109/ASE.2002.1115012
17.opinion:exclude

18.title:"Interfaces for modular feature verification"
18.abstract:"Feature-oriented programming organizes programs around features rather than objects, thus better supporting extensible, product-line architectures. Programming languages increasingly support this style of programming, but programmers get little support from verification tools. Ideally, programmers should be able to verify features independently of each other and use automated compositional reasoning techniques to infer properties of a system from properties of its features. Achieving this requires carefully designed interfaces: they must hold sufficient information to enable compositional verification, yet tools should be able to generate this information automatically because experience indicates programmers cannot or will not provide it manually. We present a model of interfaces that supports automated, compositional, feature-oriented model checking. To demonstrate their utility, we automatically detect the feature-interaction problems originally found manually by R. Hall in an email suite case study."
18.url:http://dx.doi.org/10.1109/ASE.2002.1115013
18.opinion:exclude

19.title:"Automated validation of class invariants in C++ applications"
19.abstract:"In this paper, we describe a non-invasive approach for validation of class invariants in C++ applications. Our approach is fully automated so that the user need only supply the class invariants for each class hierarchy to be checked and our validator constructs an InvariantVisitor, a variation of the Visitor Pattern, and an InvariantFacilitator. Instantiations of the InvariantVisitor and InvariantFacilitator classes encapsulate the invariants in C++ statements and facilitate the validation of the invariants. We describe both our approach and our results of validating invariants in keystone, a well tested parser front-end for C++."
19.url:http://dx.doi.org/10.1109/ASE.2002.1115014
19.opinion:exclude

20.title:"A framework for automatic debugging"
20.abstract:"This paper presents an application framework in which declarative specifications of debugging actions are translated into execution monitors that can automatically detect bugs. The approach is non-intrusive with respect to program source code and provides a high level of abstraction for debugging activities."
20.url:http://dx.doi.org/10.1109/ASE.2002.1115015
20.opinion:exclude

21.title:"Towards usable and relevant model checking techniques for the analysis of dependable interactive systems"
21.abstract:"Model checking is a formal technique for the automated analysis of system models against formal requirements. Once a suitable model and property have been specified, no further interaction by the analyst is required. However, this does not make the method necessarily user friendly since the checker must be provided with appropriate and complex input data. Furthermore, counter-examples generated by the system are often difficult to interpret. Because of this complexity, model checking is not commonly used, and exhaustive exploration of system models based on finite state descriptions is not exploited within industrial dependable systems design. The paper describes the development of an integrated collection of tools around SMV, intended to make it more accessible to practicing software engineers and in particular those concerned with the human interface issues in complex safety critical systems."
21.url:http://dx.doi.org/10.1109/ASE.2002.1115016
21.opinion:exclude

22.title:"Automatic verification of any number of concurrent, communicating processes"
22.abstract:"The automatic verification of concurrent systems by model-checking is limited due to the inability to generalise results to systems consisting of any number of processes. We use abstraction to prove general results, by model-checking, about feature interaction analysis of a telecommunications service involving any number of processes. The key idea is to model-check a system of constant number (m) of concurrent processes, in parallel with an \"abstract\" process which represents the product of any number of other processes. The system, for any specified set of selected features, is generated automatically using Perl scripts."
22.url:http://dx.doi.org/10.1109/ASE.2002.1115017
22.opinion:exclude

23.title:"System testing for object-oriented frameworks using hook technology"
23.abstract:"An application framework provides a reusable design and implementation for a family of software systems. If the framework contains defects, the defects will be passed on to the applications developed from the framework. Framework defects are hard to discover at the time the framework is instantiated. Therefore, it is important to remove all defects before instantiating the framework. The problem addressed in this paper is developing an automated state-based test suite generator technique that uses hook technology to produce test suites to test frameworks at the system level. A case study is reported and its results show that the proposed technique is reasonably effective at detecting faults. A supporting tool that automatically produces framework test cases, executes them, and evaluates the results is presented."
23.url:http://dx.doi.org/10.1109/ASE.2002.1115018
23.opinion:exclude

24.title:"What makes finite-state models more (or less) testable?"
24.abstract:"This paper studies how details of a particular model can effect the efficacy of a search for detects. We find that if the test method is fixed, we can identity classes of software that are more or less testable. Using a combination of model mutators and machine learning, we find that we can isolate topological features that significantly change the effectiveness of a defect detection tool. More specifically, we show that for one defect detection tool (a stochastic search engine) applied to a certain representation (finite state machines), we can increase the average odds of finding a defect from 69% to 91%. The method used to change those odds is quite general and should apply to other defect detection tools being applied to other representations."
24.url:http://dx.doi.org/10.1109/ASE.2002.1115019
24.opinion:exclude

25.title:"Analyzing dependencies in large component-based systems"
25.abstract:"Component-based development has become an important area in the software engineering field. In spite of this, there has been little effort to understand and to manage the different forms of dependencies that can occur in systems built from components. Dependencies reflect the potential for one component to affect or be affected by the elements (e.g., other components) that compose the system. Understanding dependencies is an essential requirement to perform important tasks, such as evolution and testing, during a component-based system's life cycle. In this paper, we present a technique to analyze dependencies in large component-based systems."
25.url:http://dx.doi.org/10.1109/ASE.2002.1115020
25.opinion:exclude

26.title:"Identifying cause and effect relations between events in concurrent event-based components"
26.abstract:"Concurrent event-based components present characteristics that impose difficulties in understanding their dynamic behavior, mainly for interpreting the cause and effect relations between input and output events in component interactions. In this paper, we propose a technique to help in the process of understanding the dynamic behavior of concurrent event-based components. It checks the event trace (generated by monitoring the component execution) against a specification of the component communication protocol (even with a possibly incomplete or incorrect specification). The technique identifies and presents the more probable cause and effect relations between the component events, providing also a measurement related to this probability."
26.url:http://dx.doi.org/10.1109/ASE.2002.1115021
26.opinion:exclude

27.title:"Automatic test case optimization using a bacteriological adaptation model: application to .NET components"
27.abstract:"In this paper, we present several complementary computational intelligence techniques that we explored in the field of .Net component testing. Mutation testing serves as the common backbone for applying classical and new artificial intelligence (AI) algorithms. With mutation tools, we know how to estimate the revealing power of test cases. With AI, we aim at automatically improving test case efficiency. We therefore looked first at genetic algorithms (GA) to solve the problem of test. The aim of the selection process is to generate test cases able to kill as many mutants as possible. We then propose a new AI algorithm that fits better to the test optimization problem, called bacteriological algorithm (BA): BAs behave better that GAs for this problem. However, between GAs and BAs, a family of intermediate algorithms exists: we explore the whole spectrum of these intermediate algorithms to determine whether an algorithm exists that would be more efficient than BAs.: the approaches are compared on a .Net system."
27.url:http://dx.doi.org/10.1109/ASE.2002.1115023
27.opinion:exclude

28.title:"From early requirements to user interface prototyping: a methodological approach"
28.abstract:"The objective of this paper is to define a software production process which represents the correspondence between the primitive elements of a business model (represented in the framework i*) and the user interface of the software system. The representation of the user interface is compliant with the Unified Model Language (UML). We use a use case model as an intermediary between the business requirements and the application software. By doing this, we go a step further in the process of properly embedding early requirements engineering into the software production process, because organizational users can validate their requirements as early as possible. This is done through the validation of the user interfaces which are generated as a software representation of these requirements. These interfaces can also be reused for further refinement as a useful starting point in the software development process."
28.url:http://dx.doi.org/10.1109/ASE.2002.1115025
28.opinion:exclude

29.title:"SeDiTeC-testing based on sequence diagrams"
29.abstract:"In this paper we present a concept for automated testing of object-oriented applications and a tool called SeDiTeC that implements these concepts for Java applications. SeDiTeC uses UML sequence diagrams, that are complemented by test case data sets consisting of parameters and return values for the method calls, as test specification and therefore can easily be integrated into the development process as soon as the design phase starts. SeDiTeC supports specification of several test case data sets for each sequence diagram as well as to combine several sequence diagrams to so-called combined sequence diagrams thus reducing the number of diagrams needed. For classes and their methods whose behavior is specified in sequence diagrams and the corresponding test case data sets SeDiTeC can automatically generate test stubs thus enabling testing right from the beginning of the implementation phase. Validation is not restricted to comparing the test case data sets with the observed data, but can also include validation of pre- and postconditions."
29.url:http://dx.doi.org/10.1109/ASE.2002.1115026
29.opinion:exclude

30.title:"VIATRA - visual automated transformations for formal verification and validation of UML models"
30.abstract:"The VIATRA (visual automated model transformations) framework is the core of a transformation-based verification and validation environment for improving the quality of systems designed using the Unified Modeling Language by automatically checking consistency, completeness, and dependability requirements. In the current paper, we present an overview of (i) the major design goals and decisions, (ii) the underlying formal methodology based on metamodeling and graph transformation, (iii) the software architecture based upon the XMI standard, and (iv) several benchmark applications of the VIATRA framework."
30.url:http://dx.doi.org/10.1109/ASE.2002.1115027
30.opinion:exclude

31.title:"A temporal logic approach to the specification of reconfigurable component-based systems"
31.abstract:"We propose a formal specification language for dynamically reconfigurable component-based systems, based on temporal logic. The main aim of the language is to allow one to specify behaviours of component-based systems declaratively, with special emphasis on behaviours in which the architectural structure of the system changes dynamically. Due to the semantics and organisation of our language, it is straightforward to hierarchically build reconfigurable systems in terms of subsystems and basic component parts, and reason about them within the language. Despite its expressive power, the language is rather simple."
31.url:http://dx.doi.org/10.1109/ASE.2002.1115028
31.opinion:exclude

32.title:"Dependence management for dynamic reconfiguration of component-based distributed systems"
32.abstract:"The growing popularity of wired and wireless Internet requires distributed systems to be more flexible, adaptive and easily extensible. Dynamic reconfiguration of component-based distributed systems is one possible solution to meet these demands. However, there are some challenges for building dynamically reconfigurable distributed systems. Managing dependencies among components is one of the most crucial problems we have to solve before a system can be dynamically reconfigured at runtime. This paper describes a dependence management for dynamic reconfiguration of distributed systems. The dependence management analyzes not only the static dependencies among components, but also the dynamic dependencies that take place at runtime, in order to support an efficient consistent reconfiguration of distributed systems. In addition, the dependence management can deal with nested dependencies during a dynamic reconfiguration."
32.url:http://dx.doi.org/10.1109/ASE.2002.1115030
32.opinion:exclude

33.title:"Combining and adapting software quality predictive models by genetic algorithms"
33.abstract:"The goal of quality models is to predict a quality factor starting from a set of direct measures. Selecting an appropriate quality model for a particular software is a difficult, non-trivial decision. In this paper, we propose an approach to combine and/or adapt existing models (experts) in such way that the combined/adapted model works well on the particular system. Test results indicate that the models perform significantly better than individual experts in the pool."
33.url:http://dx.doi.org/10.1109/ASE.2002.1115031
33.opinion:exclude

34.title:"Towards certifying domain-specific properties of synthesized code"
34.abstract:"We present a technique for certifying domain-specific properties of code generated using program synthesis technology. Program synthesis is a maturing technology that generates code from high-level specifications in particular domains. For acceptance in safety-critical applications, the generated code must be thoroughly tested which is a costly process. We show how the program synthesis system AUTOFILTER can be extended to generate not only code but also proofs that properties hold in the code. This technique has the potential to reduce the costs of testing generated code."
34.url:http://dx.doi.org/10.1109/ASE.2002.1115032
34.opinion:exclude

35.title:"Predicting software stability using case-based reasoning"
35.abstract:"Predicting stability in object-oriented (OO) software, i.e., the ease with which a software item can evolve while preserving its design, is a key feature for software maintenance. We present a novel approach which relies on the case-based reasoning (CBR) paradigm. Thus, to predict the chances of an OO software item breaking downward compatibility, our method uses knowledge of past evolution extracted from different software versions. A comparison of our similarity-based approach to a classical inductive method such as decision trees, is presented which includes various tests on large datasets from existing software."
35.url:http://dx.doi.org/10.1109/ASE.2002.1115033
35.opinion:exclude

36.title:"A model of planning and enactment support in complex software development projects"
36.abstract:"Summary form only given. We propose an approach to facilitating not only project enactment but also project planning and monitoring, by tracking all decisions made during project planning and enactment and managing dependencies between these decisions. In order to identify those dependencies relevant for a decision, we established an extendable model of planning and plan enactment, which explicitly describes the activities likely to occur while planning and enacting a software development project, as well as standard dependencies between these activities. We formalized this model by adapting an existing dependency management system, the Redux Model of Design, to record planning and plan enactment decisions and their dependencies. Furthermore, we are currently identifying heuristics to automatically capture typical dependencies, and defining rules to provide automatic planning support where possible. This approach allows to interleave planning and plan enactment, and to feed enactment data back into the plan, either by automatically reacting to enactment events and plan changes, or by notifying the appropriate person(s). Thus, we provide extensive support for the process of planning and enacting software development projects, in the form of dependency management, user notifications, and, where possible, automation of selective process steps."
36.url:http://dx.doi.org/10.1109/ASE.2002.1114984
36.opinion:exclude

37.title:"Distributed modular model checking"
37.abstract:"Summary form only given. Model checking is a formal method that verifies whether a finite state model of a system satisfies a specification given as a temporal logic formula. The most severe problem model checking suffers from is the so called state explosion problem. Distribution is one of the techniques that combat the state explosion. The aim is to distribute the state space among a number of computers so as to be able to verify larger systems. Another approach that deals with the state explosion problem is modularity, i.e. exploiting the structure of the system. We propose to employ modular techniques to the distributed model checking problem. This can be useful especially for software, as the software model checking algorithms suffer from state explosion more severely than the hardware model checking techniques even when the system consists of one sequential finite-state component. Moreover, software programs have typically richer syntactic structure that can be exploited. Besides elaborating a theoretical background for distributed model checking based on the modular approach, we also intend to develop modular approaches to partitioning the state space, in particular to define partition functions that reduce the necessary communication in the distributed environment."
37.url:http://dx.doi.org/10.1109/ASE.2002.1114987
37.opinion:exclude

38.title:"Process support for tools interoperability"
38.abstract:"Summary form only given. Our work seeks to build a platform that makes entities of various types (component, COTS, tools, etc.) interoperate in order to build a new application. We call this new kind of application a federation. Our federations use workflow as a support for application integration and interoperability. In this approach, the process is not defined in term of tools and their parameters; instead, the process is high level and describes only abstract steps without knowledge on how these steps will be carried out. Therefore, the federation offers a means to describe and control the synchronization between the abstract and executable process, and a set of concrete tools. The federation ensures that the execution of the abstract level involves a compatible real execution at the concrete level. Indeed, the real execution requires the collaboration of several tools. The description on how the abstract level is refined into the real execution satisfies consistency rules and interoperability paradigms. We think our work contributes by providing a high level view in which the application can be described, independently from the real tools specificities, and by providing the means to describe the application behavior and the tools can be used and modified flexibly and dynamically."
38.url:http://dx.doi.org/10.1109/ASE.2002.1114991
38.opinion:exclude

39.title:"Automatic synthesis of distributed systems"
39.abstract:"Summary form only given. Our research aims towards a new method of synthesis for distributed systems using Mazurkiewicz traces for specification and asynchronous automata for models. Mazurkiewicz trace languages are languages closed under an explicit independence relation between actions and therefore they are suitable to describe concurrent behaviour. The main objectives of this work are: (a) to develop a specification language based on a distributed version of temporal logic on traces that is able to express properties about the independence of actions; (b) to design a synthesis procedure based on improvements and heuristics of the algorithms for asynchronous automata; (c) to implement the new procedure efficiently (and so to turn the theory into a reliable tool that can be used in practice); (d) to apply it to case studies in areas like small distributed algorithms (e.g. mutual exclusion, communication protocols) and asynchronous circuit design. The idea used for the core of the synthesis procedure is that of unfoldings, a successful technique based on branching time partial order semantics. Promising preliminary results were obtained: we were able to automatically synthesize mutual exclusion algorithms from regular trace specifications."
39.url:http://dx.doi.org/10.1109/ASE.2002.1114992
39.opinion:exclude

40.title:"Automatic inter-procedural test case generation"
40.abstract:"Summary form only given. Our work is based on a new approach of the automatic structural test case generation problem defined previously. It uses constraint logic programming (CLP) to try and solve the problem of generating test cases in order to attain the structural covering of a procedure. A test tool prototype, named Inka has been developed by Thales Systemes Aeroportes. Inka. is designed for automatic structural test case generation for C programs. The operating cycle of Inka is cut in three parts. Our work in the Inka project is to find a way to treat large programs. Our approach of this problem is to find an alternative between stubs and complete unfolding."
40.url:http://dx.doi.org/10.1109/ASE.2002.1114993
40.opinion:exclude

41.title:"Semantic links and co-evolution in object-oriented software development"
41.abstract:"Summary form only given. This research focuses on the problem of the semantic linking and co-evolution of the different design diagrams and models of an object-oriented software application. The blueprint of an object-oriented software application consists mainly of models drawn in a modeling language. The state-of-the-art modeling language in object-oriented software development is the Unified Modeling Language (UML). Our research hypothesis is that using decidable fragments of first order logic to express the different UML diagrams enables the semantic linking of the different diagrams and models and enables the support of co-evolution which can be semi-automated, enhancing the reusability, maintainability and understandability of the design of the software application and of the software application in general. We propose to develop a formal framework to support the linking of the different diagrams and models within the software development life cycle (SDLC). The advantages of such a framework are: reasoning capabilities are provided, co-evolution is more guaranteed, adaptability of the design is improved and reuse and understandability of the software design increases. To support co-evolution of the design models in a semi-automatic way we investigate the query capabilities of these logic families."
41.url:http://dx.doi.org/10.1109/ASE.2002.1114995
41.opinion:exclude

