1.title:Toward automated software development
1.abstract:The ASE conference series is rooted in the 1981 Knowledge-Based Software Assistant (KBSA) document [1] and the ensuing US Air Force Rome Labs research program. KBSA addressed automated tool support for the entire software lifecycle, including project management, requirements, specifications, code generation, and evolution. A core goal of KBSA and ASE is the automated generation of code from requirement-level specifications, which promises benefits in three directions: (1) High Assurance - generation of correct code, as well as certification information as a by-product, (2) Productivity through automation, and (3) High Performance through machine application of best-practice design knowledge. Benefits should also include lowering of the cost of lifecycle ownership, due to automated treatment of evolving requirements. This talk lays out a more modern unifying framework for automated software development that is consistent with these goals [2]. Recent progress in automated support for algorithm design and system development is highlighted
1.url:http://doi.acm.org/10.1145/1321631.1321632
1.opinion:exclude

2.title:Specifying and verifying software
2.abstract:Software verification presents many challenges. One of these isproviding programmers with automated tool support for verification, another is providing specification support that captures common programming idioms. In this talk, I will discuss these two challenges, drawing from experience with building program verifiersfor Spec# and C. I will also give a demo of the Spec# programming system, which includes the automatic static program verifier Boogie.
2.url:http://doi.acm.org/10.1145/1321631.1321633
2.opinion:exclude

3.title:The embarrassing truth about software automation and what should be done about it
3.abstract:Automation is the most successful technological means for achieving dramatic improvements in both productivity and quality. The displacement of manual manufacturing methods by automated processes is at the core of the Industrial Revolution and accounts for much of what we usually refer to "progress" over the past several centuries. The introduction of computers and computer software more than a half century ago create unprecedented opportunities in this regard: they can not only help us automate the physical processes involved in production but also many of the highly mechanistic and error-prone mental processes involved in design. The opportunity provided by software and computers in this regard is without precedent. Yet, when one reviews the history of computer-based automation in the process of software design, we find that, following the invention of compilers almost fifty years ago, the use of computer-based automation has made little difference in the lives and methods of the vast majority of software developers. This is not due to lack of opportunity - software is an eminently flexible medium limited mostly by imagination - but mostly due to often unrecognized social and cultural factors. In this talk, we first examine the history of the use of computer-based automation in software development and speculate on the reasons why the software development community has spurned the opportunities that it offers. Next, we identify the scope of opportunities offered by current computer-based technology and what could be done to exploit this potential to its fullest.
3.url:http://doi.acm.org/10.1145/1321631.1321634
3.opinion:exclude

4.title:Scalable automatic test data generation from modeling diagrams
4.abstract:We explore the automatic generation of test data that respect constraints expressed in the Object-Role Modeling(ORM) language. ORM is a popular conceptual modelinglanguage, primarily targeting database applications, withsignificant uses in practice. The general problem of evenchecking whether an ORM diagram is satisfiable is quitehard: restricted forms are easily NP-hard and the problemis undecidable for some expressive formulations of ORM.Brute-force mapping to input for constraint and SAT solversdoes not scale: state-of-the-art solvers fail to find data to satisfy uniqueness and mandatory constraints in realistic time even for small examples. We instead define a restricted subset of ORM that allows efficient reasoning yet contains most constraints overwhelmingly used in practice. We show that the problem of deciding whether these constraints are consistent (i.e., whether we can generate appropriate test data) is solvable in polynomial time, and we produce a highly efficient (interactive speed) checker. Additionally, we analyze over 160 ORM diagrams that capture data models from industrial practice and demonstrate that our subset of ORM is expressive enough to handle their vast majority
4.url:http://doi.acm.org/10.1145/1321631.1321635
4.opinion:exclude

5.title:Exploring the neighborhood with dora to expedite software maintenance
5.abstract:Completing software maintenance and evolution tasks for todayâs large, complex software systems can be difficult, often requiring considerable time to understand the system well enough to make correct changes. Despite evidence that successful programmers use program structure as well as identifier names to explore software, most existing program exploration techniques use either structural or lexical identifier information. By using only one type of information, automated tools ignore valuable clues about a developer's intentions - clues critical to the human program comprehension process. In this paper, we present and evaluate a technique that exploits both program structure and lexical information to help programmers more effectively explore programs. Our approach uses structural information to focus automated program exploration and lexical information to prune irrelevant structure edges from consideration. For the important program exploration step of expanding from a seed, our experimental results demonstrate that an integrated lexical-and structural-based approach is significantly more effective than a state-of-the-art structural program exploration technique
5.url:http://doi.acm.org/10.1145/1321631.1321637
5.opinion:exclude

6.title:Netstub: a framework for verification of distributed java applications
6.abstract:Automated verification of distributed programs is a challenging problem. Since the behavior of a distributed program encompasses the behavior of the network, possible configurations of the network have to be investigated during verification. This leads to very large state spaces, and automated verification becomes infeasible. We present a framework that addresses this problem by decoupling the behavior of distributed programs from the behavior of the network. Our framework is based on a set of stub classes that replace native methods used in network communication and enables verification of distributed Java applications by isolating their behavior from the network. The framework supports two modes of verification: unit verification and integration verification. Integration verification checks multiple interacting distributed application components by running them in a single JVM and simulating the behavior of the network within the same JVM via stub classes. Unit verification targets a single component of a distributed application and requires that the user write an event generator class that utilizes the API exported by the framework. While unit verification only checks a single application component, it benefits from a greatly reduced state space compared do that of integration verification
6.url:http://doi.acm.org/10.1145/1321631.1321638
6.opinion:exclude

7.title:Modeling bug report quality
7.abstract:Software developers spend a significant portion of their resources handling user-submitted bug reports. For software that is widely deployed, the number of bug reports typically outstrips the resources available to triage them. As a result, some reports may be dealt with too slowly or not at all. We present a descriptive model of bug report quality based on a statistical analysis of surface features of over 27,000 publicly available bug reports for the Mozilla Firefox project. The model predicts whether a bug report is triaged within a given amount of time. Our analysis of this model has implications for bug reporting systems and suggests features that should be emphasized when composing bug reports. We evaluate our model empirically based on its hypothetical performance as an automatic filter of incoming bug reports. Our results show that our model performs significantly better than chance in terms of precision and recall. In addition, we show that our modelcan reduce the overall cost of software maintenance in a setting where the average cost of addressing a bug report is more than 2% of the cost of ignoring an important bug report.
7.url:http://doi.acm.org/10.1145/1321631.1321639
7.opinion:exclude

8.title:Diconic addition of failsafe fault-tolerance
8.abstract:We present a divide-and-conquer method, called DiConic, for automatic addition of failsafe fault-tolerance to distributed programs, where a failsafe program guarantees to meet its safety specification even when faults occur. Specifically, instead of adding fault-tolerance to a program as a whole, we separately revise program actions so that the entire program becomes failsafe fault-tolerant. Our DiConic algorithm has the potential to utilize the processing power of a large number of machines working in parallel, thereby enabling automatic addition of failsafe fault-tolerance to distributed programs with a large number of processes. We formulate our DiConic synthesis algorithm in terms of the satisfiability problem and demonstrate our approach for the Byzantine Generals problem and an industrial application.
8.url:http://doi.acm.org/10.1145/1321631.1321641
8.opinion:exclude

9.title:Ensuring consistency in long running transactions
9.abstract:Flow composition languages permit the construction of long-running transactions from collections of independent atomic services. Due to environmental limitations, such transactions usually cannot be made to conform to standard ACID semantics. We propose set consistency, a powerful, yet intuitive, notion of consistency for long-running transactions. Set consistency considers the collection of permanent (non-intermittent) changes made by a process, when viewed at the end of its execution. Consistency requirements for such collections of changes are specified as predicates over the atomic actions of a process. Set consistency generalizes self-cancellation, a standard consistency requirement for long-running transactions, where failed processes are responsible for undoing any partially completed work. Set consistency can also express strictly stronger requirements, such as mutual exclusion or dependency. We show that the set consistency verification problem for processes is co-NP complete and present an algorithm for verifying set consistency by reduction to propositional validity. We have implemented this algorithm and demonstrate the value and tractability of our approach on three real-world case studies. In each case, the consistency requirements can be verified within a second, demonstrating the practicality of our approach.
9.url:http://doi.acm.org/10.1145/1321631.1321642
9.opinion:exclude

10.title:Assertion-based repair of complex data structures
10.abstract:Programmers have long used assertions to characterize properties of code. An assertion violation signals a corruption in the programstate. At such a state, it is standard to terminate the program, debug it if possible, and re-execute it. We propose a new view: instead of terminating the program, use the violated assertion as a basis of repairing the state of the program and let it continue. We present a novel algorithm to repair complex data structures. Given a structure that violates an assertion that represents its integrity constraints, our algorithm performs a systematic search based on symbolic execution to repair the structure, i.e., mutate it such that the resulting structure satisfies the given constraints. Heuristics to prune search and minimize mutations enable efficient and effective repair. Experiments using libraries and applications, such as a naming architecture and a database engine, show that our prototype efficiently repairs complex structures while enabling systems to recover from potentially crippling errors.
10.url:http://doi.acm.org/10.1145/1321631.1321643
10.opinion:exclude

11.title:Automatic code stylizing
11.abstract:Coding style is an important aspect of software development. We present a system that uses machine learning to deduce the coding style from a corpus of code and then applies this knowledge to convert arbitrary code to the learned style. We use a broad definition of coding style that includes spacing, indentation, naming, ordering, and equivalent programming constructs. The result provides a more flexible and powerful approach to code stylizing than current techniques.
11.url:http://doi.acm.org/10.1145/1321631.1321645
11.opinion:exclude

12.title:Keyword programming in java
12.abstract:Keyword programming is a novel technique for reducing the need to remember details of programming language syntax and APIs, by translating a small number of keywords provided by the user into a valid expression. Prior work has demonstrated the feasibility and merit of this approach in limited domains. This paper presents a new algorithm that scales to the much larger domain of general-purpose Java programming. We tested the algorithm by extracting keywords from method calls in open source projects, and found that it could accurately reconstruct over 90% of the original expressions. We also conducted a study using keywords generated by users, whose results suggest that users can obtain correct Java code using keyword queries as accurately as they can write the correct Java code themselves
12.url:http://doi.acm.org/10.1145/1321631.1321646
12.opinion:exclude

13.title:Towards supporting awareness of indirect conflicts across software configuration management workspaces
13.abstract:Workspace awareness techniques have been proposed to enhance the effectiveness of software configuration management systems in coordinating parallel work. These techniques share information regarding ongoing changes, so potential conflicts can be detected during development, instead of when changes are completed and committed to a repository. To date, however, workspace awareness techniques only address direct conflicts, which arise due to concurrent changes to the same artifact, but are unable to support indirect conflicts, which arise due to ongoing changes in one artifact affecting concurrent changes in an-other artifact. In this paper, we present a new, cross-workspace awareness technique that supports one particular kind of indirect conflict, namely those indirect conflicts caused by changes to class signatures. We introduce our approach, discuss its implementation in our workspace awareness tool Palantír, illustrate its potential through two pilot studies, and lay out how to generalize the technique to a broader set of indirect conflicts
13.url:http://doi.acm.org/10.1145/1321631.1321647
13.opinion:exclude

14.title:Combined static and dynamic mutability analysis
14.abstract:Knowing which method parameters may be mutated during a method's executionis useful for many software engineering tasks. We present an approach todiscovering parameter reference immutability, in which several lightweight, scalable analyses are combined in stages, with each stage refining the overall result. The resulting analysis is scalable and combines the strengths of its component analyses. As one of the component analyses, we present a novel, dynamic mutability analysis and show how its results can be improved by random input generation. Experimental results on programs of up to 185 kLOC show that, compared to previous approaches, our approach increases both scalability and overall accuracy
14.url:http://doi.acm.org/10.1145/1321631.1321649
14.opinion:exclude

15.title:Sequential circuits for program analysis
15.abstract:A number of researchers have proposed the use of Boolean satisfiability solvers for verifying C programs. They encode correctness checks as Boolean formulas using finitization: loops and recursion are bounded, as is the size of the input instances. The SAT approach has been shown to find subtle bugs with reasonable resources. However, it does not scale well; in particular, it lacks the ability to handle larger bounds. We present SEBAC, which can handle the same class of programs as the SAT approach, and scales to bounds that are orders of magnitude higher. The key difference between SEBAC and SAT techniques is SEBAC's use of imperative Boolean sequential circuits, which are Boolean formulas with memory elements instead of the Boolean formulas which are stateless
15.url:http://doi.acm.org/10.1145/1321631.1321650
15.opinion:exclude

16.title:Residual dynamic typestate analysis exploiting static analysis: results to reformulate and reduce the cost of dynamic analysis
16.abstract:Programmers using complex libraries and frameworks are faced with the difficult task of ensuring that their implementations comply with complex and informally described rules for proper sequencing of API calls. Recent advances in static and dynamic techniques for checking explicit specifications of program typestate properties have shown promise in addressing this challenge. Unfortunately, static typestate analyses are limited in their scalability and dynamic analyses can suffer from significant run-time overhead. In this paper, we present an approach that exploits information calculated by flow-sensitive static typestate analyses to reformulate the original analysis problem as a residual dynamic typestate analysis. We demonstrate that residual analyses retain the error reporting of unoptimized dynamic analysis while offering the potential for significantly reducing analysis cost
16.url:http://doi.acm.org/10.1145/1321631.1321651
16.opinion:exclude

17.title:Directed test generation using symbolic grammars
17.abstract:We present CESE, a tool that combines exhaustive enumeration of test inputs from a structured domain with symbolic execution driven test generation. We target programs whose valid inputs are determined by some context free grammar. We abstract the concrete input syntax with symbolic grammars, where some original tokens are replaced with symbolic constants. This reduces the set of input strings that must be enumerated exhaustively. For each enumerated input string, which may contain symbolic constants, symbolic execution based test generation instantiates the constants based on program execution paths. The "template" generated by enumerating valid strings reduces the burden on the symbolic execution to generate syntactically valid inputs and helps exercise interesting code paths. Together, symbolic grammars provide a link between exhaustive enumeration of valid inputs and execution-directed symbolic test generation Preliminary experiments with CESE show that the combination achieves better coverage than both pure enumerative test generation and pure directed symbolic test generation, in orders of magnitude less time and number of generated inputs. In addition, CESE is able to automatically generate inputs that achieve coverage within 10% of manually constructed tests.
17.url:http://doi.acm.org/10.1145/1321631.1321653
17.opinion:exclude

18.title:Nighthawk: a two-level genetic-random unit test data generator
18.abstract:Randomized testing has been shown to be an effective method fortesting software units. However, the thoroughness of randomized unit testing varies widely according to the settings of certain parameters, such as the relative frequencies with which methods are called. In this paper, we describe a system which uses agenetic algorithm to find parameters for randomized unit testing that optimize test coverage. We compare our coverage results to previous work, and report on case studies and experiments on system options
18.url:http://doi.acm.org/10.1145/1321631.1321654
18.opinion:exclude

19.title:An aspect-oriented weaving mechanism based on component and connector architecture
19.abstract:Aspect-oriented programming (AOP) separates crosscutting concerns from primary concerns. These concerns are woven together by a weaver. Although AOP provides an effective module mechanism, it is not necessarily easy for a programmer to understand the overall behavior of a woven program. To deal with this problem, we propose a new kind of information hiding mechanism called a weaving-interface that encapsulates weaving in class-based AOP in which all kinds of concerns are described according to classes. Weaving-interfaces are completely separated from concerns described in terms of classes. A programmer who designs how to compose concerns does not have to know the details of class definitions, but has only to be aware of weaving-interfaces. A programmer who designs each concern does not have to know how the concern is composed, but has only to be aware of weaving-interfaces. Adopting the weaving-interface mechanism, AO weaving can be realized by the component-and-connector software architecture. This weaving-interface mechanism is effective for software modularity, evolution, and reuse
19.url:http://doi.acm.org/10.1145/1321631.1321656
19.opinion:exclude

20.title:Towards automatic model synchronization from model transformations
20.abstract:The metamodel techniques and model transformation techniques provide a standard way to represent and transform data, especially the software artifacts in software development. However, after a transformation is applied, the source model and the target model usually co-exist and evolve independently. How to propagate modifications across models in different formats still remains as an open problem. In this paper we propose an automatic approach to synchronizing models that are related by model transformations. Given a unidirectional transformation between metamodels, we can automatically synchronize models in the metamodels by propagating modifications across the models. We have implemented a model synchronization system supporting the Atlas Transformation Language (ATL) and have successfully tested our implementation on several ATL transformation examples in the ATL web site.
20.url:http://doi.acm.org/10.1145/1321631.1321657
20.opinion:exclude

21.title:An automated model-based debugging approach
21.abstract:Program debugging is a difficult and time-consuming task. Our ultimate goal in this work is to help developers reduce the space of potential root causes for failures, which can, in turn, improve the turn around time for bug fixes. We propose a novel and very different approach. Rather then focusing on how a program behaves by analyzing its source code and/or execution traces, we concentrate on how it should behave with respect to a given behavioral model. We identify and verify slices of the behavior model, that, once implemented wrong in the program, can potentially lead to failures. Not only do we identify functional differences between the programand its model, but we also provide a ranked list of diagnoses which might explain (or be associated with) these differences. Our experiments suggest that the proposed approach can be quite effective in reducing the search space for potential root causes for failures
21.url:http://doi.acm.org/10.1145/1321631.1321659
21.opinion:exclude

22.title:Context-aware statistical debugging: from bug predictors to faulty control flow paths
22.abstract:Effective bug localization is important for realizing automated debugging. One attractive approach is to apply statistical techniques on a collection of evaluation profiles of program properties to help localize bugs. Previous research has proposed various specialized techniques to isolate certain program predicates as bug predictors. However, because many bugs may not be directly associated with these predicates, these techniques are often ineffective in localizing bugs. Relevant control flow paths that may contain bug locations are more informative than stand-alone predicates for discovering and understanding bugs. In this paper, we propose an approach to automatically generate such faulty control flow paths that link many bug predictors together for revealing bugs. Our approach combines feature selection (to accurately select failure-related predicates as bug predictors), clustering (to group correlated predicates), and control flow graph traversal in a novel way to help generate the paths. We have evaluated our approach on code including the Siemens test suite and rhythmbox (a large music management application for GNOME). Our experiments show that the faulty control flow paths are accurate, useful for localizing many bugs, and helped to discover previously unknown errors in rhythmbox
22.url:http://doi.acm.org/10.1145/1321631.1321660
22.opinion:exclude

23.title:Object ownership profiling: a technique for finding and fixing memory leaks
23.abstract:We introduce object ownership profiling, a technique forfinding and fixing memory leaks in object-oriented programs. Object ownership profiling is the first memory profiling technique that reports both a hierarchy of allocated objects along with size and time information aggregated up that hierarchy. In addition, it reveals the cross-hierarchy interactions that areessential to pinpointing the source of the leak. We identify five memory management anti-patterns, including two that involve rich heap structure and object interactions, and are novel contributions of this work. We apply object ownership profiling to find and fix memory leaks in the Alloy IDE v3 that users had complained about for years, and that had eluded detection by code reviews and type-based commercial memory profilers.
23.url:http://doi.acm.org/10.1145/1321631.1321661
23.opinion:exclude

24.title:Parseweb: a programmer assistant for reusing open source code on the web
24.abstract:Programmers commonly reuse existing frameworks or libraries to reduce software development efforts. One common problem in reusing the existing frameworks or libraries is that the programmers know what type of object that they need, but do not know how to get that object with a specific method sequence. To help programmers to address this issue, we have developed an approach that takes queries of the form "Source object type → Destination object type" as input, and suggests relevant method-invocation sequences that can serve as solutions that yield the destination object from the source object given in the query. Our approach interacts with a code search engine (CSE) to gather relevant code samples and performs static analysis over the gathered samples to extract required sequences. As code samples are collected on demand through CSE, our approach is not limited to queries of any specific set of frameworks or libraries. We have implemented our approach with a tool called PARSEWeb, and conducted four different evaluations to show that our approach is effective in addressing programmer's queries. We also show that PARSEWeb performs better than existing related tools: Prospector and Strathcona
24.url:http://doi.acm.org/10.1145/1321631.1321663
24.opinion:exclude

25.title:Automatic extraction of framework-specific models from framework-based application code
25.abstract:Framework-specific models represent the design of pplicationcode from the framework viewpoint by showing how framework-provided concepts are implemented in the code. In this paper, we describe an experimental study of the static analyses necessary to automatically retrieve such models from application code. We reverse engineer a number of applications based on three open-source frameworks and evaluate the quality of the retrieved models. The models are expressed using framework-specific modeling languages(FSMLs), each designed for one of the open-source frameworks. For reverse engineering, we use prototype implementations of the three FSMLs. Our results show that for the considered frameworks rather simple code analysesare sufficient for automatically retrieving framework-specific models form a large body of application code with high precision and recall
25.url:http://doi.acm.org/10.1145/1321631.1321664
25.opinion:exclude

26.title:Pallino: automation to support regression test selection for cots-based applications
26.abstract:Software products are often built from commercial-off-the-shelf (COTS) components. When new releases of these components are made available for integration and testing, source code is usually not provided by the vendors. Various regression test selection techniques have been developed and have been shown to be cost effective. However, the majority of these test selection techniques rely on source code for change identification and impact analysis. In our research, we have evolved a regression test selection (RTS) process called Integrated - Black-box Approach for Component Change Identification (I-BACCI) for COTS-based applications. I-BACCI reduces the test suite based upon changes in the binary code of the COTS component using the firewall regression test selection method. In this paper, we present the Pallino tool. Pallino statically analyzes binary code to identify the code change and the impact of these changes. Based on the output of Pallino and the original test suit, testers can determine the regression test cases needed to cover the application glue code which is affected by the changed areas in the new version of the COTS component. Three case studies, examining a total of fifteen component releases, were conducted on ABB internal products. With the help of Pallino, RTS via the I-BACCI process can be completed in about one to two person hours for each release of the case studies. The total size of application and component for each release is about 340 830 KLOC. Pallino is extensible and can be modified to support other RTS methods for COTS components. Currently, Pallino works on components in Common Object File Format or Portable Executable formats
26.url:http://doi.acm.org/10.1145/1321631.1321665
26.opinion:exclude

27.title:Feature location via information retrieval based filtering of a single scenario execution trace
27.abstract:The paper presents a semi-automated technique for feature location in source code. The technique is based on combining information from two different sources: an execution trace, on one hand and the comments and identifiers from the source code, on the other hand. Users execute a single partial scenario, which exercises the desired feature and all executed methods are identified based on the collected trace. The source code is indexed using Latent Semantic Indexing, an Information Retrieval method, which allows users to write queries relevant to the desired feature and rank all the executed methods based on their textual similarity to the query. Two case studies on open source software (JEdit and Eclipse) indicate that the new technique has high accuracy, comparable with previously published approaches and it is easy to use as it considerably simplifies the dynamic analysis.
27.url:http://doi.acm.org/10.1145/1321631.1321667
27.opinion:exclude

28.title:Clustering support for automated tracing
28.abstract:Automated trace tools dynamically generate links between various software artifacts such as requirements, design elements, code, test cases, and other less structured supplemental documents. Trace algorithms typically utilize information retrieval methods to compute similarity scores between pairs of artifacts. Results are returned to the user as a ranked set of candidate links, and the user is then required to evaluate the results through performing a top-down search through the list. Although clustering methods have previously been shown to improve the performance of information retrieval algorithms by increasing understandability of the results and minimizing human analysis effort, their usefulness in automated traceability tools has not yet been explored. This paper evaluates and compares the effectiveness of several existing clustering methods to support traceability; describes a technique for incorporating them into the automated traceability process; and proposes new techniques based on the concepts of theme cohesion and coupling to dynamically identify optimal clustering granularity and to detect cross-cutting concerns that would otherwise remain undetected by standard clustering algorithms. The benefits of utilizing clustering in automated trace retrieval are then evaluated through a case study
28.url:http://doi.acm.org/10.1145/1321631.1321668
28.opinion:exclude

29.title:Inferring structural patterns for concern traceability in evolving software
29.abstract:As part of the evolution of software systems, effort is often invested to discover in what parts of the source code a feature (or other concern) is implemented. Unfortunately, knowledge about a concern's implementation can become invalid as the system evolves. We propose to mitigate this problem by automatically inferring structural patterns among the elements identified as relevant to a concern's implementation. We then document the inferred patterns as rules that can be checked as the source code evolves. Checking whether structural patterns hold across different versions of a system enables the automatic identification of new elements related to a documented concern. We implemented our technique for JAVA in an Eclipse plug-in called ISIS and applied it to a number of concerns. With a case study spanning 34 versions of the development history of an open-source system, we show how our approach supports the tracking of a concern's implementation through modifications such as extensions and refactorings
29.url:http://doi.acm.org/10.1145/1321631.1321669
29.opinion:exclude

30.title:Finding errors in components that exchange xml data
30.abstract:Two or more components (e.g., objects, modules, or programs) interoperate when they exchange data, such as XML data. Using Application Programming Interface (API) calls exported by XML parsers remains a primary mode of accessing and manipulating XML, and these API calls lead to various run-time errors in components that exchange XML data. Currently, no tool checks the source code of interoperating components for potential flaws caused by third-party API calls that lead to incorrect XML data exchanges and runtime errors, even when components are located within the same application Our solution combines program abstraction and symbolic execution in order to reengineer the approximate schema of XML data that would be output by a component. This schema is compared using bisimulation with the schema of XML data that is expected by some other components. We describe our approach and give our error checking algorithm. We implemented our approach in a tool that we used on open source and commercial systems and discovered errors that were not detected during their design and testing.
30.url:http://doi.acm.org/10.1145/1321631.1321671
30.opinion:exclude

31.title:A dynamic birthmark for java
31.abstract:Code theft is a threat for companies that consider code asa core asset. A birthmark can help them to prove codetheft by identifying intrinsic properties of a program. Twoprograms with the same birthmark are likely to share a com-mon origin. Birthmarking works in particular for code thatwas not protected by tamper-resistant copyright notices thatotherwise could prove ownership.We propose a dynamic birthmark for Java that observes howa program uses objects provided by the Java Standard API.Such a birthmark is difficult to foil because it captures the observable semantics of a program. In an evaluation, ourAPI Birthmark reliably identified XML parsers and PNGreaders before and after obfuscating them with state-of-the-art obfuscation tools. These rendered existing birthmarksineffective, such as the Whole-Program-Path Birthmark byMyles and Collberg
31.url:http://doi.acm.org/10.1145/1321631.1321672
31.opinion:exclude

32.title:Effective memory protection using dynamic tainting
32.abstract:Programs written in languages that provide direct access tomemory through pointers often contain memory-related faults, which may cause non-deterministic failures and even security vulnerabilities. In this paper, we present a new technique based on dynamic tainting for protecting programs from illegal memory accesses. When memory is allocated, at runtime, our technique taints both the memory and the corresponding pointer using the same taint mark. Taint marks are then suitably propagated while the program executes and are checked every time a memory address m is accessed through a pointer p; if the taint marks associated with mand p differ, the execution is stopped and the illegalaccess is reported. To allow for a low-overhead, hardware-assisted implementation of the approach, we make several key technical and engineering decisions in the definition of our technique. In particular, we use a configurable, low number of reusable taint marks instead of a unique mark for each area of memory allocated, which reduces the overhead of the approach without limiting its flexibility and ability to target most memory-related faults and attacks known to date. We also define the technique at the binary level, which lets us handle the (very) common case of applications that use third-party libraries whose source code is unavailable. To investigate the effectiveness and practicality of our approach, we implemented it for heap-allocated memory and performed a preliminary empirical study on a set of programs. Our results show that (1) our technique can identify a large class of memory-related faults, even when using only two unique taint marks, and (2)a hardware-assisted implementation of the technique could achieve overhead in the single digits
32.url:http://doi.acm.org/10.1145/1321631.1321673
32.opinion:exclude

33.title:An automated approach to monitoring and diagnosing requirements
33.abstract:Monitoring the satisfaction of software requirements and diagnosing what went wrong in case of failure is a hard problem that has received little attention in the Software and Requirement Engineering literature. To address this problem, we propose a framework adapted from artificial intelligence theories of action and diagnosis. Specifically, the framework monitors the satisfaction of software requirements and generates log data at a level of granularity that can be tuned adaptively at runtime depending on monitored feedback. When errors are found, the framework diagnoses the denial of the requirements and identifies problematic components. To support diagnostic reasoning, we transform the diagnostic problem into apropositional satisfiability (SAT) problem that can be solved by existing SAT solvers. We preprocess log data into a compact propositional encoding that better scales with problem size. The proposed theoretical framework has been implemented as a diagnosing component that will return sound and complete diagnoses accounting for observed aberrant system behaviors. Our solution is illustrated with two medium-sized publicly available case studies: a Web-based email client and an ATM simulation. Our experimental results demonstrate the feasibility of scaling our approach to medium-size software systems
33.url:http://doi.acm.org/10.1145/1321631.1321675
33.opinion:accept

34.title:The business case for automated software engineering
34.abstract:Adoption of advanced automated SE (ASE) tools would be favored if a business case could be made that these tools are more valuable than alternate methods. In theory, software prediction models can be used to make that case. In practice, this is complicated by the "local tuning" problem. Normally, predictors for software effort and defects and threat use local data to tune their predictions. Such local tuning data is often unavailable. This paper shows that assessing the relative merits of different SE methods need not require precise local tunings. STAR1 is a simulated annealer plus a Bayesian post-processor that explores the space of possible local tunings within software prediction models. STAR1 ranks project decisions by their effects on effort and defects and threats. In experiments with two NASA systems, STAR1 found that ASE tools were necessary to minimize effort/ defect/ threats.
34.url:http://doi.acm.org/10.1145/1321631.1321676
34.opinion:exclude

35.title:Testing concurrent programs using value schedules
35.abstract:Concurrent programs are difficult to debug and verify because of the nondeterministic nature of concurrent executions. A particular concurrency-related bug may only show up under certain rarely-executed thread interleavings. Therefore, commonly used debugging methodologies, such as inserting print statements, are no longer sufficient for uncovering concurrency-related bugs. However, many existing bug detection methods, such as dynamic analysis and model checking, have a very high computational cost. In this paper, we introduce a new technique for uncovering concurrency-related bugs from multithreaded Java programs. Our technique uncovers concurrency-related bugs by generating and testing read-write assignment sequences, referred to as value schedules, of a multithreaded Java program. Our value-schedule-based technique distinguishes itself in its ability to avoid exploring superfluous program state space caused by speculative permutation on transitions. Therefore, our technique can achieve a higher degree of POR (Partial Order Reduction) than existing methods. We demonstrate our technique using some programs, with an implementation built using an explicit state model checker called JPF
35.url:http://doi.acm.org/10.1145/1321631.1321678
35.opinion:exclude

36.title:Effective random testing of concurrent programs
36.abstract:Multithreaded concurrent programs often exhibit wrong behaviors due to unintended interferences among the concurrent threads. Such errors are often hard to find because they typically manifest under very specific thread schedules. Traditional testing, which pays no attention to thread schedules and non-deterministically exercises a few arbitrary schedules, often misses such bugs. Traditional model checking techniques, which try to systematically explore all thread schedules, give very high confidence in the correctness of the system, but, unfortunately, they suffer from the state explosion problem. Recently, dynamic partial order techniques have been proposed to alleviate the problem. However, such techniques fail for large programs because the state space remains large in spite of reduction. An inexpensive and a simple alternative approach is to perform random testing by choosing thread schedules at random. We show that such a naive approach often explores some states with very high probability compared to the others. We propose a random partial order sampling algorithm (or RAPOS) that partly removes this non-uniformity in sampling the state space. We empirically compare the proposed algorithm with the simple random testing algorithm and show that the former outperforms the latter
36.url:http://doi.acm.org/10.1145/1321631.1321679
36.opinion:exclude

37.title:Automated gui testing guided by usage profiles
37.abstract:Most software developed in recent years has a graphical userinterface (GUI). The only way for the end-user to interact with the software application is through the GUI. Hence, acceptance and system testing of the software requires GUI testing. This paper presents a new technique for testing of GUI applications. Information on the actual usage of the application, in the form of "usage profiles," is used to ensure that a new version of the application will function correctly. Usage profiles, sequences of events that end-users execute on a GUI, are used to develop a probabilistic usage model of the application. An algorithm uses the model to generate test cases that represent events the user is most likely to execute. Reverse engineering methods are used to extract the underlying structure of the application. An empirical study on four open source GUI applications reveals that test suites generated from the probabilistic model are 0.2-22% of the size of test suites produced directly from usage profiles. Furthermore, the test suites generated from the model detect more faults per test case than those detected directly from the usage profiles, and detect faults not detected by the original profiles
37.url:http://doi.acm.org/10.1145/1321631.1321681
37.opinion:exclude

38.title:Efficiently monitoring data-flow test coverage
38.abstract:Structural testing of software requires monitoring the software's execution to determine which program entities are executed by a test suite. Such monitoring can add considerable overhead to the execution of the program, adversely affecting the cost of running a test suite. Thus, minimizing the necessary monitoring activity lets testers reduce testing time or execute more test cases. A basic testing strategy is to cover all statements or branches but a more effective strategy is to cover all definition-use associations (DUAs). In this paper, we present a novel technique to efficiently monitor DUAs, based on branch monitoring. We show how to infer from branch coverage the coverage of many DUAs, while remaining DUAs are predicted with high accuracy by the same information. Based on this analysis, testers can choose branch monitoring to approximate DUA coverage or instrument directly for DUA monitoring, which is precise but more expensive. In this paper, we also present a tool, called DUA-Forensics, that we implemented for this technique along with a set of empirical studies that we performed using the tool
38.url:http://doi.acm.org/10.1145/1321631.1321682
38.opinion:exclude

39.title:Synthesizing client load models for performance engineering via web crawling
39.abstract:Accurate web application performance testing relies on the use of loading tests based on a realistic client behaviour load model. Unfortunately developing such load models and associated test plans and scripts is tedious and error-prone with most existing web performance testing tools providing limited client load modelling capabilities. We describe a new approach and toolset that we have developed, MaramaMTE+, which improves the ability to model realistic web client load behaviour, automatically generates complex web application testing plans and scripts, and integrates load behaviour modelling with a generic performance engineering tool. MaramaMTE+ uses a stochastic form chart as its client loading model. A 3rd party web crawler application extracts structural information from a target web site, aggregating the collected data into a crawler database that is then used for form chart model generation. The performance engineer then augments this synthesized form chart with client loading probabilities. Realistic web loading tests for a 3rd party web load testing tool are then automatically generated from this resultant stochastic form chart client load model. We describe the development of our MaramaMTE+ environment, example usage of the tool, and compare and contrast the results obtained from our generated performance load tests against hand-built 3rd party tool load tests
39.url:http://doi.acm.org/10.1145/1321631.1321684
39.opinion:exclude

40.title:Synthesis of test purpose directed reactive planning tester for nondeterministic systems
40.abstract:We describe a model-based construction of an online tester for black-box testing of the implementation under test(IUT). The external behaviour of the IUT is modelled as an output observable nondeterministic EFSM with the assumption that all transition paths are feasible. A test purpose is attributed to the IUT model by a set of Boolean variables called traps that are used to measure the progress of the test run. These variables are associated with the transitions of the IUT model. The situation where all traps have been reached means that the test purpose has been achieved. We present a way to construct a tester that at runtime selects a suboptimal test path from trap to trap by finding the shortest path to the next unvisited trap. The principles of reactive planning are implemented in the form of the decision rules of selecting the shortest paths at runtime. The decision rules are constructed in advance from the IUT model and the test purpose. Preliminary experimental results confirm that this method clearly out performs random choice and is better than the anti-ant algorithm in terms of resultant test sequence length to achieve the test purpose
40.url:http://doi.acm.org/10.1145/1321631.1321685
40.opinion:exclude

41.title:Towards automated consistency checks of product line requirements specifications
41.abstract:A requirements specification for an individual software system should be consistent, i.e. free of contradictions. In product line engineering, the product line requirements specification comprises all the requirements common to all products of the product line as well as the variable requirements used to derive individual products from the product line. The set of requirements (common and all the variable ones) of a product line is typically inconsistent since variable requirements can contradict each other. This is not a problem as long as contradicting requirements are not included in a product derived from the product line. Thus, the set of requirements realized in each individual product has to be consistent. Employing techniques used in single system development to check the consistency of product line requirements will thus produce false positive results, since there can be contradiction in the product line requirements specification In this paper we first provide a concise definition of consistency for product line requirements specifications. Based on this definition, we define a formal framework for checking consistency of product line requirements specifications. Our framework supports consistency checks in the domain engineering process. In contrast to consistency checks in single system development, it tolerates certain types of inconsistencies caused by the variability of prod-uct line requirements.
41.url:http://doi.acm.org/10.1145/1321631.1321687
41.opinion:exclude

42.title:Automated detection of api refactorings in libraries
42.abstract:Software developers often do not build software from scratch but reuse software libraries. In theory, the APIs of a library should be stable, but in practice they do change and thus require changes in software that reuses the library. Our previous study of five reusable components shows that more than 80% of these API changes are caused by refactorings. If these refactorings could be automatically detected, they could be used to automatically upgrade applications. In this paper, we present a technique and its supporting tool, RefacLib, to automatically detect refactorings in libraries. RefacLib uses syntactic analysis in the first phase to quickly detect refactoring candidates across two versions of a library. In the second phase, RefacLib uses various heuristics to refine the results. We used RefacLib to detect refactorings in five open source libraries and frameworks. The experiments show that RefacLib can process realistic code bases and detects refactorings with practical accuracy
42.url:http://doi.acm.org/10.1145/1321631.1321688
42.opinion:exclude

43.title:Improving evolutionary class testing in the presence of non-public methods
43.abstract:Automating the generation of object-oriented unit tests is a challenging task. This is mainly due to the complexity and peculiarities that the principles of object-orientation imply. One of these principles is the encapsulation of class members which prevents non-public methods and attributes of the class under test from being freely accessed. This paper suggests an improvement of our automated search-based test generation approach which particularly addresses the test of non-public methods. We extend our objective functions by an additional component that accounts for encapsulation. Additionally, we propose a modification of the search space which increases the efficiency of the approach. The value of the improvement in terms of achieved code coverage is demonstrated by a case study with 7 real-world test objects. In contrast to other approaches which break encapsulation in order to test non-public methods, the tests generated by our approach inherently guarantee that class invariants are not violated. At the same time, refactorings of the encapsulated class members will not break the generated tests
43.url:http://doi.acm.org/10.1145/1321631.1321689
43.opinion:exclude

44.title:Behavioral adaptation of component compositions based on process algebra encodings
44.abstract:Software adaptation has been proposed as a solution to mismatch between components through the generation of software pieces called adaptors. We propose a new behavioral adaptation approach for the generation of adaptor protocols. Compared to related work, it is fully automated and addresses the adaptor computation complexity thanks to process algebra encodings and on-the-fly techniques.
44.url:http://doi.acm.org/10.1145/1321631.1321690
44.opinion:exclude

45.title:A buffer overflow benchmark for software model checkers
45.abstract:Software model checking based on abstraction-refinement has recently achieved widespread success in verifying API conformance in device drivers, and we believe this success can be replicated for the problem of buffer overflow detection. This paper presents a publicly-available benchmark suite to help guide and evaluate this research. The benchmark consists of 298 code fragments of varying complexity capturing 22 buffer overflow vulnerabilities in 12 open source applications. We give a preliminary evaluation of the benchmark using the SatAbs model checker
45.url:http://doi.acm.org/10.1145/1321631.1321691
45.opinion:exclude

46.title:Checking threat modeling data flow diagrams for implementation conformance and security
46.abstract:Threat modeling is a lightweight approach to reason about application security and uses Data Flow Diagrams (DFDs) with security annotations. We extended Reflexion Models to check the conformance of an as-designed DFD with an approximation of the as-built DFD obtained from the implementation. We also designed a set of properties and an analysis to help novice designers think about security threats such as spoofing, tampering and information disclosure.
46.url:http://doi.acm.org/10.1145/1321631.1321692
46.opinion:exclude

47.title:Coevolving programs and unit tests from their specification
47.abstract:Writing a formal specification before implementing a program helps to find problems with the system requirements. The requirements might be for example incomplete and ambiguous. Fixing these types of errors is very difficult and expensive during the implementation phase of the software development cycle. Although writing a formal specification is usually easier than implementing the actual code, writing a specification requires time, and often it is preferred, instead, to use this time on the implementation. In this paper we introduce for the first time a framework that might evolve any possible generic program from its specification. We use the Genetic Programming to evolve the programs, and at the same time we exploit the specifications to coevolve sets of unit tests. Programs are rewarded on how many tests they do not fail, whereas the unit tests are rewarded on how many programs they make fail. We present and analyse four different problems on which this novel technique is successfully applied.
47.url:http://doi.acm.org/10.1145/1321631.1321693
47.opinion:exclude

48.title:Combining environment generation and slicing for modular software model checking
48.abstract:To be effective, software model checking needs powerful reduction techniques. In this paper, we present an experimental study that demonstrates effectiveness of environment generation combined with slicing as a model generation and reduction technique. Automatic environments were obtained using the Bandera Environment Generator (BEG) and slicing was performed using the Indus Java slicer. The results show that environment generation implemented in BEG is an aggressive reduction technique. However, it may miss behaviors in the environment and consequently in the module under analysis, making it unsafe. As such, this technique can be effective for detection of errors but not sufficient to prove their absence.Slicing, while a safe technique, may be too approximate and not scalable. Also, slicing implemented in Indus requires a closed system and cannot be safely applied to a module without its environment. In this paper, we show how environment generation and slicing can be combined to detect errors and prove their absence. We applied the combined approach to verify parts of Fujitsu's enterprise software called Interstage Business Process Management (I-BPM)
48.url:http://doi.acm.org/10.1145/1321631.1321694
48.opinion:exclude

49.title:Covering array sampling of input event sequences for automated gui testing
49.abstract:This paper describes a new automated technique to generate test cases for GUIs by using covering arrays (CAs). The key motivation is to generate long GUI event sequences that are systematically sampled at a particular coverage strength. CAs, to date, have not been effectively used in sampling event driven systems such as GUIs which maintain state. We leverage a "stateless" abstraction of GUIs that allows us to use CAs. Once the CAs have been generated, we reuse the abstractions to reinsert ordering relationships between GUI events, thereby creating executable test cases. A feasibility study on a well-studied GUI-based application shows that the new technique is able to detect a large number of previously undetected faults
49.url:http://doi.acm.org/10.1145/1321631.1321695
49.opinion:exclude

50.title:Direct-dependency-based software compatibility testing
50.abstract:Software compatibility testing is an important quality assurance task aimed at ensuring that component-based software systems build and/or execute properly across a broad range of user system configurations. Because each configuration can involve multiple components with different versions, and because there are complex and changing interdependencies between components and their versions, it is generally infeasible to test all potential configurations. Therefore, compatibility testing usually means examining only a handful of default or popular configurations to detect problems, and as a result costly errors can and do escape to the field This paper presents an improved approach to compatibility testing called RACHET. We formally model the configuration space for component-based systems and use the model to generate test plans covering user-specified portion of the space - the example in this paper is covering all it direct dependencies between components. The test plan is executed efficiently in parallel, by distributing work so as to best utilize test resources. We conducted experimentsand simulation studies applying our approach to a large-scale data management middleware system. The results showed that for this system RACHET discovered incompatibilities between components at a small fraction of the cost for exhaustive testing without compromising test quality
50.url:http://doi.acm.org/10.1145/1321631.1321696
50.opinion:exclude

51.title:Driving the selection of cots components on the basis of system requirements
51.abstract:In a component-based development process the selection of components is an activity that takes place over multiple lifecycle phases that span from requirement specifications through design to implementation-integration. Automated tool support for component selection would be very helpful in each phase. In this paper we introduce a framework that supports the selection of COTS components in the requirements phase. The framework lays on a tool that builds and solves an optimization model, whose solution provides the optimal COTS component selection. The selection criterion is based on cost minimization of the whole system while assuring a certain degree of satisfaction of the system requirements. The output of the model solution indicates the optimal combination of single COTS components and assemblies of COTS that satisfy the requirements while minimizing costs
51.url:http://doi.acm.org/10.1145/1321631.1321697
51.opinion:exclude

52.title:Efficient unit test case minimization
52.abstract:Randomized unit test cases can be very effective in detecting defects. In practice, however, failing test cases often comprise long sequences of method calls that are tiresome to reproduce and debug. We present a combination of static slicing and delta debugging that automatically minimizes the sequence of failure-inducing method calls. In a case study on the EiffelBase library, the strategy minimizes failing unit test cases on average by 96%. This approach improves on the state of the art by being far more efficient: in contrast to the approach of Lei and Andrews, who use delta debugging alone, our case study found slicing to be 50 times faster, while providing comparable results. The combination of slicing and delta debugging gives the best results and is 11 times faster.
52.url:http://doi.acm.org/10.1145/1321631.1321698
52.opinion:exclude

53.title:An energy consumption framework for distributed java-based systems
53.abstract:In this paper we define and evaluate a framework for estimating the energy consumption of Java-based software systems. Our primary objective in devising the framework is to enable an engineer to make informed decisions when adapting a system's architecture, such that the energy consumption on hardware devices with a finite battery life is reduced, and the lifetime of the system's key software services increases. Our framework explicitly takes a component-based perspective, which renders it well suited for a large class of today's distributed, embedded, and pervasive applications. The framework allows the engineer to estimate the software system's energy consumption at system construction-time and refine it at runtime. In a large number of distributed application scenarios, the framework showed very good precision on the whole, giving results that were within 5% (and often less) of the actually measured power losses incurred by executing the software. Our work to date has also highlighted a number of possible enhancements
53.url:http://doi.acm.org/10.1145/1321631.1321699
53.opinion:exclude

54.title:Evacon: a framework for integrating evolutionary and concolic testing for object-oriented programs
54.abstract:Achieving high structural coverage such as branch coverage in object oriented programs is an important and yet challenging goal due to two main challenges. First, some branches involve complex program logics and generating tests to cover them requires deep knowledge of the program structure and semantics. Second, covering some branches requires special method sequences to lead the receiver object or non-primitive arguments to specific desirable states. Previous work has developed the concolic testing technique (a combination of concrete and symbolic testing techniques) and the evolutionary testing technique to address these two challenges, respectively. However, neither technique was designed to address both challenges at the same time. To address the respective weaknesses of these two previous techniques, we propose a novel framework called Evacon that integrates evolutionary testing (used to search for desirable method sequences) and concolic testing (used to generate desirable method arguments). We have implemented our framework and applied it on six classes taken from the Java standard library and basic data structures. The experimental results show that the tests generated using our framework can achieve higher branch coverage than evolutionary testing or concolic testing alone
54.url:http://doi.acm.org/10.1145/1321631.1321700
54.opinion:exclude

55.title:Extracting rights and obligations from regulations: toward a tool-supported process
55.abstract:Security, privacy and governance are increasingly the focus of government regulations in the U.S., Europe and elsewhere. This trendhas created a "regulation compliance problem", whereby companiesand developers are required to ensure that their software complies with relevant regulations, either through design or reengineering. We previously proposed a methodology for extracting stakeholder requirements, called rights and obligations, from regulations. In this paper, we examine the challenges of developing tool support for this process. We apply the Cerno framework for textual semantic annotation to propose a tool for semi-automatic semantic annotation of concepts that constitute sources of requirements
55.url:http://doi.acm.org/10.1145/1321631.1321701
55.opinion:exclude

56.title:Extraction of bug localization benchmarks from history
56.abstract:Researchers have proposed a number of tools for automatic bug localization. Given a program and a description of the failure, such tools pinpoint a set of statements that are most likely to contain the bug. Evaluating bug localization tools is a difficult task because existing benchmarks are limited in size of subjects and number of bugs. In this paper we present iBUGS, an approach that semiautomatically extracts benchmarks for bug localization from the history of a project. For ASPECTJ, we extracted 369 bugs, 223 out of these had associated test cases. We demonstrate the relevance of our dataset with a case study on the bug localization tool AMPLE
56.url:http://doi.acm.org/10.1145/1321631.1321702
56.opinion:exclude

57.title:Feature interaction analysis: a maintenance perspective
57.abstract:Software systems have become more complex, with myriad features and multiple functionalities. A major challenge in developing and maintaining such complex software is to identify potential conflicts among its features. Feature interaction analysis becomes progressively more difficult as software's feature combinations and available scenarios increase. Software maintainers need to identify and analyze conflicts that can arise from feature modification requests. Our approach combines Use Case Maps with Formal Concept Analysis to assist maintainers in identifying feature modification impacts at the requirements level, without the need to examine the source code. We demonstrate the applicability of this approach using a teleommunication case study
57.url:http://doi.acm.org/10.1145/1321631.1321703
57.opinion:exclude

58.title:A framework and tool supports for testing modularity of software design
58.abstract:Modularity is one of the most important properties of a software design, with significant impact on changeability and evolvability. However, a formalized and automated approach is lacking to test and verify software design models against their modularity properties, in particular, their ability to accommodate potential changes. In this paper, we propose a novel framework for testing design modularity. The software artifact under test is a software design. A test input is a potential change to the design. The test output is a modularity vector, which precisely captures quantitative capability extents of the design for accommodating the test input (the potential change). Both the design and the test input are represented as formal computable models to enable automatic testing. The modularity vector integrates the net option value analysis with well-known design principles. We have implemented the framework with tool supports and tested aspect-oriented and object-oriented design patterns in terms of their ability to accommodate sequences of possible changes. The results showed that previous informal, implementation-based analysis can be conducted by our framework automatically and quantitatively at the design level. This framework also opens the opportunities of applying testing techniques, such as coverage criteria, on software designs
58.url:http://doi.acm.org/10.1145/1321631.1321704
58.opinion:exclude

59.title:Improving uml profile design practices by leveraging conceptual domain models
59.abstract:The profile extension mechanism has permitted a rapid growth of the use of UML as a domain-specific modeling language. However, designing profiles typically falls into ad-hoc processes that often rely on domain-inappropriate primitives. One of the fundamental reasons is that profiles are specified on the same level of abstraction as the UML abstract syntax and consequently they narrow down the design space to an implementation level. In order to improvethis situation, some profile designers start from a "conceptual domain model" that states the domain ontology, and only then deal with finding out the profile extensions to support it. In spite of this, building truthfulness conceptual domain models and maintaining traceable mapping with the profile view is a bit of an art. In this paper, we propose to systematize the design of UML profiles built-upon conceptual domain models, by adopting a minimal setof framing rules. As these rules are defined on the basis of regularly occurring design patterns, domain models can be afterward checked for self-consistency and interactively transformed in stereotypes, tags and constraints
59.url:http://doi.acm.org/10.1145/1321631.1321705
59.opinion:exclude

60.title:An infrastructure for autonomic system development: the selflet approach
60.abstract:Autonomic computing is an emergent field for the development of large-scale, self-managing, complex distributed computer-based systems. This paper aims to be a practical approach to Autonomic Computing, by defining and implementing a generic model for autonomic systems, allowing software developers to create autonomic applications using a common and comprehensive infrastructure. In particular, this paper defines a simple but complete model andarchitecture of an autonomic computing element called SelfLet, which could be the building components used to create autonomic systems. SelfLets can be defined by specifying their behaviour, the abilities and goals they need to use and/or provide, and a high-level policy guiding their self-management.
60.url:http://doi.acm.org/10.1145/1321631.1321706
60.opinion:exclude

61.title:Iterative model-driven development of adaptable service-based applications
61.abstract:Flexibility and interoperability make web services well suited for designing highly-customizable reactive service-based applications, that is interactive applications that can be rapidly adapted to new requirements and environmental conditions. This is the case, for example of personal data managers that many users tailor to their needs to meet different usage conditions and requests. In this paper, we propose a model-based approach that provides users with the ability of rapidly developing, adapting and reconfiguring reactive service-based applications to meet new requirements and needs. Users specify their needs by describing sample executions that include interactions with web services through an intuitive interface. Interactions are stored in a visual formalism that integrates live sequence charts with graph transformation systems. Models can be visualized, modified, executed and automatically analyzed to identify inconsistencies
61.url:http://doi.acm.org/10.1145/1321631.1321707
61.opinion:exclude

62.title:An approach to mining call-usage patternswith syntactic context
62.abstract:An approach to mine frequently appearing ordered sets of function-call usages, taking into account their proximal control constructs (e.g., if-statements), in the source code is presented. These ordered sets are termed as call-usage patterns. Additionally, variant usages, such as those with missing or out of order calls, are automatically identified along with their specific contextual location. The approach uses lightweight source code analysis and frequent sequential pattern mining. The hypothesis is that these call-usage patterns embody latent programming rules that developers commonly reuse, for example standard usages of API calls. The variants are an indicator of future changes such as the elimination of non-standard usages and/or bugs
62.url:http://doi.acm.org/10.1145/1321631.1321708
62.opinion:exclude

63.title:Mining concepts from code with probabilistic topic models
63.abstract:We develop and apply statistical topic models to software as a means of extracting concepts from source code. The effectiveness of the technique is demonstrated on 1,555 projects from SourceForge and Apache consisting of 113,000 files and 19 million lines of code. In addition to providing an automated, unsupervised, solution to the problem of summarizing program functionality, the approach provides a probabilistic framework with which to analyze and visualize source file similarity. Finally, we introduce an information-theoretic approach for computing tangling and scattering of extracted concepts, and present preliminary results
63.url:http://doi.acm.org/10.1145/1321631.1321709
63.opinion:exclude

64.title:Mining modal scenario-based specifications from execution traces of reactive systems
64.abstract:Specification mining is a dynamic analysis process aimed at automatically inferring suggested specifications of a program from its execution traces. We describe a novel method, framework, and tool, for mining inter-object scenario-based specifications in the form of a UML2-compliant variant of Damm and Harels Live Sequence Charts (LSC). LSC extends the classical partial order semantics of sequence diagrams with temporal liveness and symbolic class level lifelines, in order to generate compact and expressive specifications. The output of our algorithm is a sound and complete set of statistically significant LSCs (i.e., satisfying given thresholds of support and confidence), mined from an input execution trace. We locate statistically significant LSCs by exploring the search space of possible LSCs and checking for their statistical significance. In addition, we use an effective search space pruning strategy, specifically adapted to LSCs, which enables efficient mining of scenarios of arbitrary size. We demonstrate and evaluate the utility of our work in mining informative specifications using a case study on Jeti, a popular, full featured messaging application
64.url:http://doi.acm.org/10.1145/1321631.1321710
64.opinion:exclude

65.title:Model-driven derivation of product architectures
65.abstract:Product Derivation is one of the central activities in Software Product Lines (SPL). One of the main challenges of the process of product derivation is dealing with complexity, which is caused by the large number of artifacts and dependencies between them. Another major challenge is maximizing development efficiency and reducing time-to-market, while at the same time producing high quality products. One approach to overcome these challenges is to automate the derivation process. To this end, this paper focuses on one particular activity of the derivation process; the derivation of the product-specific architecture and describes how this activity can be automated using a model-driven approach. The approach derives the product-specific architecture by selectively copying elements from the product-line architecture. The decision, which elements are included in the derived architecture, is based on a product-specific feature configuration. We present a prototype that implements the derivation as a model transformation described in the Atlas Transformation Language (ATL). We conclude with a short overview of related work and directions for future research
65.url:http://doi.acm.org/10.1145/1321631.1321711
65.opinion:exclude

66.title:Modular and generic programming with interpreterlib
66.abstract:Modular monadic semantics (MMS) is a well-known technique for structuring modular denotational semantic definitions. Families of language constructs are independently defined using syntactic functors and semantic algebras that can be combined in a mix-and-match fashion to create complete language definitions. We introduce InterpreterLib, a Haskell library that implements and extends MMS techniques for writing composable analyses. In addition to modular analyses composition, InterpreterLib provides algebra combinators, explicit algebra semantics, preprocessors for boiler plate generation and generic programming techniques adapted to language analysis. The key benefits of these features are reliability, increased code reuse via modularity and the ability to rapidly retarget component analyses.
66.url:http://doi.acm.org/10.1145/1321631.1321712
66.opinion:exclude

67.title:Reducing irrelevant trace variations
67.abstract:Identifying truly distinct traces is crucial for the performance and practicality of many dynamic analysis activities. For example, given a trace pool resulting from program failures, identifying the set of distinct traces can reduce the debugging effort by more quickly producing a smaller set of candidate fault locations. The process of discriminating valuable traces, however, is subject to the presence of irrelevant variations in the trace constitution, i.e., the sequence of events in a trace, that can make a trace appear unique when it is not, leading to the retention of a trace that adds no value. In this paper we present an approach to address inconsequential and potentially detrimental trace variations. The approach decomposes traces into segments on which irrelevant variations caused by event ordering or repetition can be detected and removed. The approach is illustrated on two well-known client dynamic analyses and is supported by an infrastructure to explore the approach
67.url:http://doi.acm.org/10.1145/1321631.1321713
67.opinion:exclude

68.title:Rteq: modeling and validating infinite-state hard-real-time systems
68.abstract:Complex, interrupt-driven hard real time control software is difficult to design and validate. It does not line up well with traditional state-based, timed-transition approaches to real time system specification, due to the complexity of timers and the pending interrupt queue. The present work takes a new approach to the problem of modeling and tool-supported reasoning about such systems based on infinite-state modeling of the temporal event queue. This approach, RTEQ, can be used in any formalism or tool set supporting event queue modeling. This paper briefly overviews the approach and its application to a novel wireless medium access controller
68.url:http://doi.acm.org/10.1145/1321631.1321714
68.opinion:exclude

69.title:IMP: a meta-tooling platform for creating language-specific ides in eclipse
69.abstract:Programming language design remains a vital field, with interest in languages targeting concurrency, scripting, and aspects, as well as in domain-specific languages. Full-featured integrated development environments (IDEs) have become critical to the adoption of new languages. A key factor in the success of these IDEs is the provision of services specifically tailored to the language. However, modern IDE frameworks are large and complex, and the cost of constructing a language-specific IDE from scratch remains prohibitive IMP is an IDE meta-tooling platform intended to relieve much of the burden of IDE development in Eclipse. IMP combines a language-independent framework, generators for partial implementations of language-specific services, and support for the completion of service implementations by programming at various levels of abstraction. Unlike much of the previous work, IMP permits signigicant customization of IDE appearance and behavior and accommodates incremental elaboration of the IDE; it also makes significant reuse of code and assists during the IDE development process. IMP-based IDEs are in use in research projects in IBM, including within IMP itself. IMP is available as an open-source release from SourceForge.net
69.url:http://doi.acm.org/10.1145/1321631.1321715
69.opinion:exclude

70.title:Scheduling of conflicting refactorings to promote quality improvement
70.abstract:Software refactoring is to restructure object-oriented software to improve its quality, especially extensibility, reusability and maintainability while preserving its external behaviors. For a special software system, there are usually quite a few refactorings available at the same time. But these refactorings may conflict with each other. In other words, carrying out a refactoring may disable other refactorings. Consequently, only a subset of the available refactorings can be applied together, and which refactorings will be applied depends on the schedule (application order) of the refactorings. Furthermore, carrying out different subsets of the refactorings usually leads to different improvement of software quality. As a result, in order to promote the improvement of software quality, refactorings should be scheduled rationally. However, how to schedule refactorings is rarely discussed. Usually, software engineers carry out refactorings immediately when they are found out. They do not wait until all applicable refactorings are found out and scheduled. In other words, the refactorings are not scheduled explicitly, and conflicts among them are not taken into consideration. Though more and more refactorings are formalized and automated by refactoring tools, refactoring tools apply refactorings usually in a nondeterministic fashion (in random). In this paper, we propose a scheduling approach to schedule conflicting refactorings to promote the improvement of software quality achieved by refactorings. Conflicts among refactorings are detected, and then a scheduling model is presented. And then a heuristic algorithm is proposed to solve the scheduling model. Results of experiments suggest that the proposed scheduling approach is effective in promoting the improvement of software quality
70.url:http://doi.acm.org/10.1145/1321631.1321716
70.opinion:exclude

71.title:An evaluation scheme of adaptive configuration techniques
71.abstract:In this paper, we present a set of metrics (scalability, adaptability, overhead, latency, complexity, and effectiveness) to evaluate dynamic configuration techniques. We have applied these metrics to two real systems: remote management system and dynamic security configuration of networked systems. The evaluation results show that our approach can efficiently and dynamically manage system resources and network security with little overhead.
71.url:http://doi.acm.org/10.1145/1321631.1321717
71.opinion:exclude

72.title:Assisting potentially-repetitive small-scale changes via semi-automated heuristic search
72.abstract:When a software system undergoes modification, a given change might need to be repeated throughout the codebase. While the change itself may not be difficult to implement, discovering other locations where this change should be applied (if any exist) can be onerous. Syntactic differences in otherwise semantically similar code can render traditional search techniques ineffective. This paper describes a heuristic search technique to help find the locations required to complete a repetitive small-scale change (RSC). By observing the developer perform a change once, it is possible to infer semantic information about that change and to automatically suggest locations where the same change ought to be made
72.url:http://doi.acm.org/10.1145/1321631.1321718
72.opinion:exclude

73.title:Model checking concurrent linux device drivers
73.abstract:The S<scp>lam</scp> toolkit demonstrates that predicate abstraction enables automated verification of real world Windows device drivers. Our predicate abstraction-based tool DDV<scp>erify</scp>enables the automated verification of Linux device drivers and provides an accurate model of the relevant parts of the kernel. We report on benchmarks based on Linux device drivers, confirming the results that S<scp>lam</scp> established for the Windows world. Furthermore, we take predicate abstraction one step further and introduce a technique to verify concurrent software with shared memory
73.url:http://doi.acm.org/10.1145/1321631.1321719
73.opinion:exclude

74.title:Test automation for kernel code and disk arrays with virtual devices
74.abstract:In this work, we discuss testing automation issues for kernel code on a high-capacity network-attached storage (NAS) in early development phases when the hardware may not be ready. We propose to construct a high-fidelity versatile emulator for hard disks for the fast configuration of virtual disk arrays. To test kernel libraries, we not only have to figure out the proper values for the kernel procedure parameters, but also have to overcome the rigid resource limitations in kernel space. We propose techniques to profile kernel procedure parameters, drive the kernel procedures from the user space, and construct the testing interface between kernel SUTs (software-under-test) and user space test plans. We have experimented to test the multiple devices driver library used in the NAS server by Quanta Computer Inc. Our techniques support rapid NAS configurations and have been proven useful in locating a proven bug in the Linux kernel
74.url:http://doi.acm.org/10.1145/1321631.1321720
74.opinion:exclude

75.title:Towards leveraging model transformation to support model-based testing
75.abstract:The adoption of model-driven development is leading to increased use of models in conjunction with source code in software testing. Model-based testing, however, introduces new challenges for testing activities, which include creation and maintenance of traceability information among test-related artifacts. Traceability is required to support activities such as model-based result evaluation, regression testing and coverage analysis. In this paper, we present an automated approach that leverages model transformation techniques to support test generation. The test generation process includes creation of test-related models and fine-grained relationships among these models. We also motivate our approach with a simple example demonstrating support for model-based regression testing
75.url:http://doi.acm.org/10.1145/1321631.1321721
75.opinion:exclude

76.title:Unit testing concurrent software
76.abstract:There are many difficulties associated with developing correct multithreaded software, and many of the activities that are simple for single threaded software are exceptionally hard for multithreaded software. One such example is constructing unit tests involving multiple threads. Given, for example, a blocking queue implementation, writing a test case to show that it blocks and unblocks appropriately using existing testing frameworks is exceptionally hard. In this paper, we describe the MultithreadedTC framework which allows the construction of deterministic and repeatable unit tests for concurrent abstractions. This framework is not designed to test for synchronization errors that lead to rare probabilistic faults under concurrent stress. Rather, this framework allows us to demonstrate that code does provide specific concurrent functionality (e.g., a thread attempting to acquire a lock is blocked if another thread has the lock). We describe the framework and provide empirical comparisons against hand-coded tests designed for Sun's Java concurrency utilities library and against previous frameworks that addressed this same issue. The source code for this framework is available under an open source license.
76.url:http://doi.acm.org/10.1145/1321631.1321722
76.opinion:exclude

77.title:Validating system properties exhibited in execution traces
77.abstract:Execution traces produced by software systems during their operation can capture important runtime information, and thus are valuable sources for validating software functional properties. Automating the validation of such properties is currently achieved by writing test scripts, where most of the effort focuses on programming operations rather than specifying properties clearly. Improving this practice calls for domain-specific languages that can capture properties exhibited in traces at a higher abstract level. This paper presents a Test Behavior Language (TBL) that uses parameterized patterns as logical predicates to specify and validate trace-based properties abstractly but precisely. TBL has been used to automate the testing of several software systems, including a large telecommunication system. Initial results show that TBL is powerfulfor validating complex properties of these systems in real time, resulting in a big increase in the thoroughness of behavioral analysis and the number of bugs revealed. TBL also reduces the effort of script writing. TBL specifications range from 1/2 to 1/5 the size of their Tcl script counterparts, with a greater benefit realized where thorough trace analysis is needed.
77.url:http://doi.acm.org/10.1145/1321631.1321723
77.opinion:exclude

78.title:Verifying C++ with STL containers via predicate abstraction
78.abstract:This paper describes a flexible and easily extensible predicate abstraction-based approach to the verification of STLusage, and observes the advantages of verifying programsin terms of high-level data structures rather than low-level pointer manipulations. We formalize the semantics of theSTL by means of a Hoare-style axiomatization. The verification requires an operational model conservatively approximating the semantics given by the Standard. Our results show advantages (in terms of errors detected and false positives avoided) over previous attempts to analyze STL usage, due to the power of the abstraction engine and model checker
78.url:http://doi.acm.org/10.1145/1321631.1321724
78.opinion:exclude

79.title:CodeGenie: using test-cases to search and reuse source code
79.abstract:We present CodeGenie, a tool that implements a test-driven approachto search and reuse of code available on large-scale coderepositories. While using CodeGenie developers design test cases fora desired feature first, similar to Test-driven Development (TDD).However, instead of implementing the feature as in TDD, CodeGenieautomatically searches for it based on information available in thetests. To check the suitability of the candidate results in thelocal context, each result is automatically woven into thedeveloper's project and tested using the original tests. Thedeveloper can then reuse the most suitable result. Later, reusedcode can also be unwoven from the project as wished. For the codesearching and wrapping facilities, CodeGenie relies on Sourcerer, anInternet-scale source code infrastructure that we have developed
79.url:http://doi.acm.org/10.1145/1321631.1321726
79.opinion:exclude

80.title:Decor: a tool for the detection of design defects
80.abstract:Software engineers often need to identify design defects, recurring design problems that hinder the development process, to improve and assess the quality of their systems. However, this is diÂ±cult because of the lack of specifications and tools. We propose Decor, a method to specify design defects systematically and to generate automatically detection algorithms. With this method, software engineers analyse and specify design defects at a high-level of abstraction using a unified vocabulary and dedicated language for generating detection algorithms
80.url:http://doi.acm.org/10.1145/1321631.1321727
80.opinion:exclude

81.title:DESERT: a decentralized monitoring tool generator
81.abstract:This paper presents the tool DESERT that allows the generation of decentralized monitoring systems for component based applications.
81.url:http://doi.acm.org/10.1145/1321631.1321728
81.opinion:exclude

82.title:Formal specification generator for KAOS: model transformation approach to generate formal specifications from KAOS requirements models
82.abstract:Formal methods and requirements analysis are techniques for developing complex systems. However, there is little research on reconciling the requirements phase with the formal specification phase. To bridge this gap, we propose a formal specification generator based on model transformation techniques. This tool transforms KAOS models (requirements specifications) into VDM++ formal specifications. Our generator enables consistent and effective software development activities.
82.url:http://doi.acm.org/10.1145/1321631.1321729
82.opinion:exclude

83.title:Integrated tool support for software product line engineering
83.abstract:Product line engineering comprises many heterogeneous activities such as capturing the variability of reusable assets, supporting the derivation of products from the product line, evolving the product line, or tailoring the approach to the specifics of a domain. The inherent complexity of product lines implicates that tool support is inevitable to facilitate smooth performance and to avoid costly errors. Product line engineering tools have to support heterogeneous stakeholders involved in diverse activities. Tool integration therefore is of particular importance to foster their seamless cooperation. However, the integration is difficult to achieve due to the diversity of models and work products. This paper describes the DOPLER tool suite which has been developed to provide such integrated support. The tool suite is flexible and extensible to support domain-specific needs
83.url:http://doi.acm.org/10.1145/1321631.1321730
83.opinion:exclude

84.title:Mastering combinatorial explosion with the tobias-2 test generator
84.abstract:This paper briefly describes the second version of the Tobias combinatorial test generator. This version improves the architecture of the tool to include filtering and test selection mechanisms. These mechanisms, associated with an efficient implementation, allow to generate and filter test suites of up to 1 million test cases.
84.url:http://doi.acm.org/10.1145/1321631.1321731
84.opinion:exclude

85.title:Smart: a tool for application reference testing
85.abstract:Graphical User Interface (GUI) APplications (GAPs) are ubiquitous and provide various services. Since many GAPs are not designed to exchange information (i.e., interoperate), companies replace legacy GAPs with web services, that are designed to interoperate over the Internet. However, it is laborious and inefficient to create unit test cases to test the web services. We propose to demonstrate a SysteM for Application Reference Testing (SMART) novel approach for generating tests for web services from legacy GAPs. During demonstration of Smart we will show how this tool enables nonprogrammers to generate unit test cases for web services by performing drag-and-drop operations on GUI elements of legacy GAPs. We published a research paper that describes our approach and the results of the evaluation of Smart [2].
85.url:http://doi.acm.org/10.1145/1321631.1321732
85.opinion:exclude

86.title:Test suite reduction and prioritization with call trees
86.abstract:This paper presents a tool that (i) constructs tree-based models of a program's behavior during testing and (ii) employs these trees while reordering and reducing a test suite. Using either a dynamic call tree or a calling context tree, the test reduction component identifies a subset of the original tests that covers the same call tree paths. The prioritization technique reorders a test suite so that it covers the call tree paths more rapidly than the initial test ordering. In support of program and test suite understanding, the tool also visualizes the call trees and the coverage relationships. For a chosen case study application, the experimental results show that call tree construction only increases testing time by 13%. In comparison to the original test suite, the experiments show that (i) a prioritized suite achieves coverage much faster and (ii) a reduced test suite contains 45% fewer tests and consumes 82% less time
86.url:http://doi.acm.org/10.1145/1321631.1321733
86.opinion:exclude

87.title:The eureka tool for software model checking
87.abstract:We describe EUREKA, a symbolic model checker for Linear Programs with arrays, i.e. programs where variables and array elements range over a numeric domain and expressions involve linear combinations of variables and array elements. This language fragment easily encodes a large class of programs for which, as demonstrated by our experiments, techniques based on predicate abstraction do not apply successfully.
87.url:http://doi.acm.org/10.1145/1321631.1321734
87.opinion:exclude

88.title:Tool support for the compile-time execution structure of object-oriented programs with ownership annotations
88.abstract:Ownership domain annotations enable obtaining at compile-time the system's execution structure from the annotated program. The execution structure is sound, hierarchical (and thus more scalable) and conveys more design intent than flat object graphs obtained by existing static analyses that do not rely on annotations.
88.url:http://doi.acm.org/10.1145/1321631.1321735
88.opinion:exclude

89.title:Tools for model-based security engineering: models vs. code
89.abstract:We present tools to support model-based security engineering at both the model and the code level. In the approach supported by these tools, one firstly specifies the security-critical part of the system (e.g. a crypto protocol) using the UML security extension UMLsec. The models are automatically verified for security properties using automated theorem provers. These are implemented within a framework that supports implementing verification routines, based on XMI output of the diagrams from UML CASE tools. Advanced users can use this open-source framework to implement verification routines for the constraints of self-defined security requirement In a second step, one verifies that security-critical parts of the model are correctly implemented in the code (which might be a legacy implementation), and applies security hardening transformations where that is not the case. This is supported by tools that (1) establish traceability through refactoring scripts and (2) modularize security hardening advices through aspect-oriented programming. The proposed method has been applied to an open-source implementation of a cryptographic protocol implementation (Jessie) in Java to build up traceability mappings and security aspects. In that application, we found a security weakness which could be fixed using our approach. The resulting refactoring scripts and security aspects have found reusability in the Java Secure Socket Extension (JSSE) library
89.url:http://doi.acm.org/10.1145/1321631.1321736
89.opinion:exclude

90.title:UMLtoCSP: a tool for the formal verification of UML/OCL models using constraint programming
90.abstract:We present UMLtoCSP, a tool for the formal verification of UML/OCL models. Given a UML class diagram annotated with OCL constraints, UMLtoCSP is able to automatically check several correctness properties, such as the strong and weak satisfiability of the model or the lack of redundant constraints. The tool uses Constraint Logic Programming as the underlying formalism and the constraint solver ECLiPSe as the verification engine.
90.url:http://doi.acm.org/10.1145/1321631.1321737
90.opinion:exclude

91.title:Visualizing clone detection results
91.abstract:It has been observed by many practitioners that software applications frequently contain redundant code fragments. This redundancy is often caused by the common programming practice of replicating (or cloning) existing code and then customizing code fragments to handle new demands within an application. An IT organization consequently spends significant amounts of its budget maintaining such code (e.g., a bug in one code fragment is also a bug in all of its replicated clones). To address the challenges of clone detection and maintenance, this demo will introduce a new open source tool that can be used to visualize the results of CloneDR™, which is a commercial clone detection tool (a free version of the tool is available for Java). Several open source projects will be used as case studies to demonstrate the usage of the visualization tool
91.url:http://doi.acm.org/10.1145/1321631.1321738
91.opinion:exclude

92.title:Adaptation hiding modularity
92.abstract:Although self-adaptivity is emerging as an important system property, designers still lack principled and systematic methods for developing, reasoning about, and synthesizing specifications, architectures, and implementations for such systems. I exploit recent work on design structure and design-time adaptive capacity to devise a method for specifying, modularizing and synthesizing mechanisms for runtime self-adaptation. Results of an early evaluation of my method on a widely studied model problem suggest that it contributes to our knowledge of self-adaptive systems and has the potential to improve their design
92.url:http://doi.acm.org/10.1145/1321631.1321740
92.opinion:exclude

93.title:Automating the identification of variability realisation techniques from feature models
93.abstract:In Software Product Line Engineering (SPLE), feature modelling is frequently used to model commonalities and variabilities within a domain. A feature model captures an abstract view of a product line and it can serve as a starting point for software design and component implementation. Handling variability exposed within the feature model is an important problem in this context, and in this paper, we describe ongoing research aimed at automating the identification of variability realisation techniques from feature models
93.url:http://doi.acm.org/10.1145/1321631.1321741
93.opinion:exclude

94.title:Improving change prediction with fine-grained source code mining
94.abstract:The thesis proposes a software-change prediction approach that is based on mining fine-grained evolutionary couplings from source code repositories. Here, fine-grain refers to identifying couplings between source code entities such as methods, control structures, or even comments. This differs from current source code mining techniques that typically only identify couplings between files or fairly high-level entities. Furthermore, the model combines the mined evolutionary couplings with the estimated changes identified by traditional impact analysis techniques (e.g., static analysis of call and program-dependency graphs). The research hypothesis is that software-change prediction using the proposed synergistic approach results in an overall improved expressiveness (i.e., granularity and context given to a developer) and effectiveness (i.e., accuracy of the prediction)
94.url:http://doi.acm.org/10.1145/1321631.1321742
94.opinion:exclude

95.title:Using information retrieval to support design of incremental change of software
95.abstract:The proposed research defines an approach to combine Information Retrieval based analysis of the textual information embedded in software artifacts with program static and dynamic analysis techniques to support key activities of the incremental change of software, such as concept and feature location.
95.url:http://doi.acm.org/10.1145/1321631.1321743
95.opinion:exclude

96.title:Using traceability to support model-based regression testing
96.abstract:Model-driven development is leading to increased use of models in conjunction with source code in software testing. Model-based testing, however, introduces new challenges for testing activities, which include creation and maintenance of traceability information among test-related artifacts. Traceability is required to support activities such as selective regression testing. In fact, most model-based testing automated approaches often concentrate on the test generation and execution activities, while support to other activities is limited (e.g. model-based selective regression testing, coverage analysis and behavioral result evaluation) To address this problem, we propose a solution that uses model transformation to create a traceable infrastructure of test-related artifacts. We use this infrastructure to support model-based selective regression testing.
96.url:http://doi.acm.org/10.1145/1321631.1321744
96.opinion:exclude

97.title:Concolic testing
97.abstract:Concolic testing automates test input generation by combining the concrete and symbolic (concolic) execution of the code under test. Traditional test input generation techniques use either (1) concrete execution or (2) symbolic execution that builds constraints and is followed by a generation of concrete test inputs from these constraints. In contrast, concolic testing tightly couples both concrete and symbolic executions: they run simultaneously, and each gets feedback from the other. We have implemented concolic testing in tools for testing both C and Java programs. We have used the tools to find bugs in several real-world software systems including SGLIB, a popular C data structure library used in a commercial tool, a third-party implementation of the Needham-Schroeder protocol and the TMN protocol, the scheduler of Honeywell's DEOS real-time operating system, and the Sun Microsystems' JDK 1.4 collection framework. In this tutorial, we will describe concolic testing and some of its recent extensions
97.url:http://doi.acm.org/10.1145/1321631.1321746
97.opinion:exclude

98.title:Tutorial on JML, the java modeling language
98.abstract:The Java Modeling Language (JML) is widely used in academic research as a common language for formal methods tools that work with Java. JML is a design by contract language that can be used to specify detailed designs of Java programs, frameworks, and class libraries. Over twenty research groups worldwide have built several tools for checking code and finding bugs (see jmlspecs.org). This tutorial will give background for researchers and practitioners interested in doing formal methods research and in using JML for specifying the sequential behavior of Java classes and interfaces. Attendees will write JML specifications for a data type, including pre- and postconditions for methods and object invariants. They will also learn how to use the most important JML tools. In addition, they will learn how to use model fields to hide the actual field declarations in classes, and how JML supports modular reasoning about subtypes with behavioral subtyping
98.url:http://doi.acm.org/10.1145/1321631.1321747
98.opinion:exclude

99.title:Empirical research methods for software engineering
99.abstract:This full day tutorial introduces the use of empirical methods appropriate to research in automated software engineering. Using a blend of lecture and discussion, it aims to provide ASE researchers and practitioners with a foundation for conducting and critiquing empirical studies. The tutorial covers of the principal methods applicable to ASE: controlled experiments, quasi-experiments, case studies, survey research, ethnographies, and action research and relates these methods to relevant metatheories in the philosophy and sociology of science. The tutorial presents techniques applicable to each of the steps of a research project, including formulating research questions, theory building, data analysis (using both qualitative and quantitative methods), building evidence, assessing validity, and publishing. The tutorial is relevant for researchers, who will be able to conduct and write more credible empirical studies; for reviewers who will be able to provide qualified judgments of papers; and for practitioners, who will be able to more effectively interpret published results within the context of their own organizations
99.url:http://doi.acm.org/10.1145/1321631.1321749
99.opinion:exclude

100.title:Software reuse and evolution with generative techniques
100.abstract:Generative software development aims at modeling and implementing product lines in such a way that all or a substantial part of the desired system can be automatically generated from a specification written in one or more domain-specific languages (DSLs). The tutorial will explore several techniques of generative software development and show how they can help address software evolution and reuse challenges.
100.url:http://doi.acm.org/10.1145/1321631.1321750
100.opinion:exclude

101.title:Meta tools for implementing domain specific visual languages
101.abstract:In this tutorial we present an overview of Domain-specific visual languages (DSVLs), DSVL-based software engineering tools and meta-tools for constructing DSVL-based tools. We present a motivation for DSVL usage in software engineering; examine several DSVL exemplar tools; illustrate the design of DSVLs including meta-model, notation and view type design; and compare and contrast several tools to realizing DSVLs. Included are three short group exercises in DSVL design.
101.url:http://doi.acm.org/10.1145/1321631.1321751
101.opinion:exclude

102.title:Rosetta: language support for system-level design
102.abstract:
102.url:http://doi.acm.org/10.1145/1321631.1321752
102.opinion:exclude

